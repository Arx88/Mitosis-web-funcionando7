"""
Herramienta de investigaci√≥n comprehensiva - Multi-sitio con im√°genes
Combina Tavily para contenido y DuckDuckGo para im√°genes
"""

import os
import requests
from typing import Dict, Any, List
from tavily import TavilyClient
try:
    from duckduckgo_search import DDGS
except ImportError:
    print("Warning: duckduckgo_search not available, using fallback")
    DDGS = None
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, urlparse

class ComprehensiveResearchTool:
    def __init__(self):
        self.name = "comprehensive_research"
        self.description = "Realiza investigaci√≥n profunda multi-sitio con contenido e im√°genes, generando un informe consolidado"
        self.tavily_api_key = os.getenv('TAVILY_API_KEY')
        self.tavily_client = TavilyClient(api_key=self.tavily_api_key) if self.tavily_api_key else None
        # DDGS ELIMINADO - Solo Bing soportado
        self.parameters = [
            {
                "name": "query",
                "type": "string",
                "required": True,
                "description": "Tema principal para investigar"
            },
            {
                "name": "include_images",
                "type": "boolean",
                "required": False,
                "description": "Incluir b√∫squeda de im√°genes relacionadas",
                "default": True
            },
            {
                "name": "max_sources",
                "type": "integer",
                "required": False,
                "description": "N√∫mero m√°ximo de fuentes a analizar",
                "default": 8
            },
            {
                "name": "max_images",
                "type": "integer",
                "required": False,
                "description": "N√∫mero m√°ximo de im√°genes a incluir",
                "default": 6
            },
            {
                "name": "research_depth",
                "type": "string",
                "required": False,
                "description": "Profundidad de investigaci√≥n: 'standard', 'comprehensive', 'expert'",
                "default": "comprehensive"
            },
            {
                "name": "content_extraction",
                "type": "boolean",
                "required": False,
                "description": "Extraer contenido completo de las p√°ginas principales",
                "default": True
            }
        ]
    
    def get_description(self) -> str:
        return self.description
    
    def get_parameters(self) -> List[Dict[str, Any]]:
        return self.parameters
    
    def validate_parameters(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Validar par√°metros de entrada"""
        if not isinstance(parameters, dict):
            return {'valid': False, 'error': 'Parameters must be a dictionary'}
        
        if 'query' not in parameters:
            return {'valid': False, 'error': 'query parameter is required'}
        
        if not isinstance(parameters['query'], str) or not parameters['query'].strip():
            return {'valid': False, 'error': 'query must be a non-empty string'}
        
        if not self.tavily_api_key:
            return {'valid': False, 'error': 'Tavily API key not configured'}
        
        return {'valid': True}
    
    def execute(self, parameters: Dict[str, Any], config: Dict[str, Any] = None) -> Dict[str, Any]:
        """Ejecutar investigaci√≥n comprehensiva"""
        if config is None:
            config = {}
        
        # Validar par√°metros
        validation = self.validate_parameters(parameters)
        if not validation['valid']:
            return {'error': validation['error'], 'success': False}
        
        query = parameters['query'].strip()
        include_images = parameters.get('include_images', True)
        max_sources = min(parameters.get('max_sources', 8), 15)
        max_images = min(parameters.get('max_images', 6), 20)
        research_depth = parameters.get('research_depth', 'comprehensive')
        content_extraction = parameters.get('content_extraction', True)
        
        try:
            # Realizar investigaci√≥n multi-fase
            research_results = self._conduct_comprehensive_research(
                query, include_images, max_sources, max_images, 
                research_depth, content_extraction
            )
            
            return {
                'query': query,
                'research_depth': research_depth,
                'report': research_results['report'],
                'sources': research_results['sources'],
                'images': research_results['images'] if include_images else [],
                'extracted_content': research_results['extracted_content'],
                'key_findings': research_results['key_findings'],
                'recommendations': research_results['recommendations'],
                'source_count': len(research_results['sources']),
                'image_count': len(research_results['images']) if include_images else 0,
                'success': True
            }
            
        except Exception as e:
            return {
                'query': query,
                'error': str(e),
                'success': False
            }
    
    def _conduct_comprehensive_research(self, query: str, include_images: bool, 
                                      max_sources: int, max_images: int,
                                      research_depth: str, content_extraction: bool) -> Dict[str, Any]:
        """Realizar investigaci√≥n comprehensiva multi-sitio"""
        
        # FASE 1: B√∫squeda inicial con Tavily
        print(f"üîç FASE 1: B√∫squeda inicial sobre '{query}'")
        initial_search = self.tavily_client.search(
            query=query,
            search_depth="advanced",
            max_results=max_sources,
            include_answer=True
        )
        
        # FASE 2: B√∫squedas espec√≠ficas para mayor profundidad
        print(f"üîç FASE 2: B√∫squedas espec√≠ficas")
        specific_queries = [
            f"{query} definici√≥n concepto",
            f"{query} caracter√≠sticas principales",
            f"{query} ventajas beneficios",
            f"{query} desventajas riesgos",
            f"{query} tendencias futuro 2025"
        ]
        
        all_sources = []
        all_content = []
        
        # Procesar b√∫squeda inicial
        for result in initial_search.get('results', []):
            all_sources.append({
                'title': result.get('title', ''),
                'url': result.get('url', ''),
                'snippet': result.get('content', ''),
                'score': result.get('score', 0),
                'type': 'primary'
            })
            all_content.append(result.get('content', ''))
        
        # B√∫squedas espec√≠ficas adicionales
        for specific_query in specific_queries[:3]:  # Limitar para evitar sobrecarga
            try:
                specific_results = self.tavily_client.search(
                    query=specific_query,
                    search_depth="basic",
                    max_results=3,
                    include_answer=True
                )
                
                for result in specific_results.get('results', []):
                    all_sources.append({
                        'title': result.get('title', ''),
                        'url': result.get('url', ''),
                        'snippet': result.get('content', ''),
                        'score': result.get('score', 0),
                        'type': 'specific'
                    })
                    all_content.append(result.get('content', ''))
                    
            except Exception as e:
                print(f"Error en b√∫squeda espec√≠fica: {e}")
                continue
        
        # FASE 3: Extracci√≥n de contenido completo de p√°ginas principales
        print(f"üîç FASE 3: Extracci√≥n de contenido completo")
        extracted_content = []
        if content_extraction:
            top_sources = all_sources[:5]  # Extraer de las 5 mejores fuentes
            for source in top_sources:
                try:
                    content = self._extract_page_content(source['url'])
                    if content:
                        extracted_content.append({
                            'title': source['title'],
                            'url': source['url'],
                            'full_content': content,
                            'word_count': len(content.split())
                        })
                except Exception as e:
                    print(f"Error extrayendo contenido de {source['url']}: {e}")
                    continue
        
        # FASE 4: B√∫squeda de im√°genes - DESHABILITADA (DuckDuckGo eliminado)
        print(f"üîç FASE 4: B√∫squeda de im√°genes - DESHABILITADA")
        images = []
        if include_images:
            # DUCKDUCKGO ELIMINADO - Solo Bing soportado en el futuro
            print("‚ö†Ô∏è  B√∫squeda de im√°genes deshabilitada - DuckDuckGo eliminado")
            images = []
        
        # FASE 5: An√°lisis y generaci√≥n del informe
        print(f"üîç FASE 5: Generaci√≥n del informe")
        combined_content = "\n".join(all_content)
        
        # Extraer informaci√≥n de contenido completo
        full_content_text = ""
        for content in extracted_content:
            full_content_text += f"\n\n{content['full_content']}"
        
        # Generar informe consolidado
        report = self._generate_consolidated_report(
            query, initial_search.get('answer', ''), 
            combined_content, full_content_text, 
            research_depth, len(all_sources), len(images)
        )
        
        # Extraer hallazgos clave
        key_findings = self._extract_comprehensive_findings(
            combined_content, full_content_text, extracted_content
        )
        
        # Generar recomendaciones
        recommendations = self._generate_actionable_recommendations(
            query, key_findings, research_depth
        )
        
        return {
            'report': report,
            'sources': all_sources[:max_sources],
            'images': images,
            'extracted_content': extracted_content,
            'key_findings': key_findings,
            'recommendations': recommendations
        }
    
    def _extract_page_content(self, url: str) -> str:
        """Extraer contenido completo de una p√°gina web"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Remover elementos no deseados
            for element in soup(["script", "style", "nav", "footer", "header", "aside", "advertisement"]):
                element.decompose()
            
            # Buscar contenido principal
            main_content = (soup.find('main') or 
                          soup.find('article') or 
                          soup.find('div', class_='content') or
                          soup.find('div', class_='main-content'))
            
            if main_content:
                text = main_content.get_text(separator='\n', strip=True)
            else:
                text = soup.get_text(separator='\n', strip=True)
            
            # Limpiar y limitar contenido
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            clean_text = '\n'.join(lines)
            
            # Limitar a 3000 caracteres para evitar sobrecarga
            if len(clean_text) > 3000:
                clean_text = clean_text[:3000] + '...'
            
            return clean_text
            
        except Exception as e:
            print(f"Error extrayendo contenido: {e}")
            return ""
    
    def _generate_consolidated_report(self, query: str, initial_answer: str, 
                                    combined_content: str, full_content: str,
                                    research_depth: str, source_count: int, 
                                    image_count: int) -> str:
        """Generar informe consolidado"""
        
        report = f"""
# INFORME CONSOLIDADO: {query.upper()}

## üìä RESUMEN EJECUTIVO
{initial_answer}

## üîç METODOLOG√çA DE INVESTIGACI√ìN
- **Fuentes analizadas**: {source_count} sitios web
- **Im√°genes incluidas**: {image_count} im√°genes
- **Profundidad**: {research_depth}
- **Extracci√≥n de contenido**: {"S√≠" if full_content else "No"}

## üìã AN√ÅLISIS DETALLADO
{self._analyze_content(combined_content + full_content, research_depth)}

## üéØ PUNTOS CLAVE IDENTIFICADOS
{self._extract_key_points(combined_content + full_content, 8)}

## üìà TENDENCIAS Y PATRONES
{self._identify_trends(combined_content + full_content)}

## üí° IMPLICACIONES
{self._analyze_implications(combined_content + full_content)}

## üéØ CONCLUSIONES
{self._generate_conclusions(combined_content + full_content)}

---
*Informe generado autom√°ticamente mediante an√°lisis multi-sitio*
"""
        
        return report
    
    def _analyze_content(self, content: str, depth: str) -> str:
        """Analizar contenido seg√∫n profundidad"""
        sentences = content.split('.')
        important_sentences = []
        
        keywords = ['importante', 'clave', 'fundamental', 'esencial', 'cr√≠tico', 'significativo']
        
        for sentence in sentences:
            if any(keyword in sentence.lower() for keyword in keywords):
                if len(sentence.strip()) > 30:
                    important_sentences.append(sentence.strip())
        
        if depth == 'expert':
            return '\n'.join([f"‚Ä¢ {sent}" for sent in important_sentences[:10]])
        elif depth == 'comprehensive':
            return '\n'.join([f"‚Ä¢ {sent}" for sent in important_sentences[:7]])
        else:
            return '\n'.join([f"‚Ä¢ {sent}" for sent in important_sentences[:5]])
    
    def _extract_key_points(self, content: str, max_points: int) -> str:
        """Extraer puntos clave"""
        sentences = content.split('.')
        key_points = []
        
        for sentence in sentences:
            if len(sentence.strip()) > 40:
                key_points.append(f"‚Ä¢ {sentence.strip()}")
            if len(key_points) >= max_points:
                break
        
        return '\n'.join(key_points)
    
    def _identify_trends(self, content: str) -> str:
        """Identificar tendencias"""
        trend_keywords = ['tendencia', 'crecimiento', 'aumento', 'evoluci√≥n', 'futuro', '2025', 'pr√≥ximo']
        trends = []
        
        sentences = content.split('.')
        for sentence in sentences:
            if any(keyword in sentence.lower() for keyword in trend_keywords):
                if len(sentence.strip()) > 30:
                    trends.append(f"‚Ä¢ {sentence.strip()}")
        
        return '\n'.join(trends[:5]) if trends else "‚Ä¢ No se identificaron tendencias espec√≠ficas en el contenido analizado"
    
    def _analyze_implications(self, content: str) -> str:
        """Analizar implicaciones"""
        impl_keywords = ['implica', 'significa', 'consecuencia', 'resultado', 'impacto', 'efecto']
        implications = []
        
        sentences = content.split('.')
        for sentence in sentences:
            if any(keyword in sentence.lower() for keyword in impl_keywords):
                if len(sentence.strip()) > 30:
                    implications.append(f"‚Ä¢ {sentence.strip()}")
        
        return '\n'.join(implications[:5]) if implications else "‚Ä¢ Se requiere an√°lisis adicional para determinar implicaciones espec√≠ficas"
    
    def _generate_conclusions(self, content: str) -> str:
        """Generar conclusiones"""
        return f"""
‚Ä¢ Basado en el an√°lisis de m√∫ltiples fuentes, se identifica informaci√≥n consistente sobre el tema
‚Ä¢ La investigaci√≥n revela aspectos clave que requieren consideraci√≥n detallada
‚Ä¢ Se recomienda validaci√≥n adicional con fuentes especializadas seg√∫n el contexto espec√≠fico
‚Ä¢ Los hallazgos proporcionan una base s√≥lida para la toma de decisiones informadas
"""
    
    def _extract_comprehensive_findings(self, content: str, full_content: str, extracted_content: List[Dict]) -> List[str]:
        """Extraer hallazgos comprehensivos"""
        findings = []
        
        # Analizar contenido combinado
        all_text = content + " " + full_content
        sentences = all_text.split('.')
        
        # Palabras clave para identificar hallazgos importantes
        important_keywords = ['descubrimiento', 'hallazgo', 'resultado', 'concluye', 'determina', 'revela']
        
        for sentence in sentences:
            if any(keyword in sentence.lower() for keyword in important_keywords):
                if len(sentence.strip()) > 25:
                    findings.append(sentence.strip())
        
        # Agregar hallazgos espec√≠ficos del contenido extra√≠do
        for content_item in extracted_content:
            if content_item['word_count'] > 100:
                findings.append(f"An√°lisis detallado disponible en: {content_item['title']}")
        
        return findings[:8]
    
    def _generate_actionable_recommendations(self, query: str, findings: List[str], depth: str) -> List[str]:
        """Generar recomendaciones accionables"""
        recommendations = []
        
        # Recomendaciones espec√≠ficas por tema
        if any(word in query.lower() for word in ['negocio', 'empresa', 'marketing']):
            recommendations.extend([
                "Realizar an√°lisis de competencia espec√≠fico",
                "Evaluar oportunidades de mercado identificadas",
                "Desarrollar estrategia basada en hallazgos clave"
            ])
        
        if any(word in query.lower() for word in ['tecnolog√≠a', 'digital', 'software']):
            recommendations.extend([
                "Evaluar impacto tecnol√≥gico en el sector",
                "Considerar implementaci√≥n gradual de soluciones",
                "Monitorear tendencias de adopci√≥n"
            ])
        
        # Recomendaciones generales
        recommendations.extend([
            "Continuar monitoreando fuentes especializadas",
            "Validar informaci√≥n con expertos del √°rea",
            "Implementar sistema de seguimiento de tendencias",
            "Considerar factores contextuales espec√≠ficos"
        ])
        
        return recommendations[:6]