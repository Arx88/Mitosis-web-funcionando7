"""
üß† GENERADOR INTELIGENTE DE KEYWORDS - SOLUCI√ìN COMPLETA V2.0
Reemplaza la l√≥gica destructiva de _extract_clean_keywords_static()
con un sistema inteligente que preserva el contexto esencial

RESUELVE: 
- Keywords in√∫tiles como "REALIZA INFORME"
- P√©rdida de contexto importante
- B√∫squedas gen√©ricas sin resultados

IMPLEMENTA:
- Preservaci√≥n de entidades nombradas (personas, lugares)
- Extracci√≥n inteligente de conceptos principales
- M√∫ltiples variantes de b√∫squeda espec√≠ficas
- Contexto temporal y geogr√°fico autom√°tico
"""

import re
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)

class IntelligentKeywordGenerator:
    """üéØ Generador inteligente de keywords y t√©rminos de b√∫squeda"""
    
    def __init__(self):
        # Entidades importantes que SIEMPRE deben preservarse
        self.preserve_entities = {
            'personas': ['milei', 'javier', 'biden', 'musk', 'elon', 'trump', 'cristina', 'massa', 
                        'alberto', 'fern√°ndez', 'macri', 'mauricio', 'cristiano', 'messi', 'lionel'],
            'lugares': ['argentina', 'buenos', 'aires', 'c√≥rdoba', 'mendoza', 'espa√±a', 'madrid', 
                       'barcelona', 'valencia', 'm√©xico', 'colombia', 'chile', 'usa', 'eeuu'],
            'organizaciones': ['fifa', 'onu', 'oea', 'mercosur', 'gobierno', 'congress', 'senate'],
            'tecnologia': ['inteligencia', 'artificial', 'blockchain', 'bitcoin', 'crypto', 'tesla', 
                          'apple', 'google', 'microsoft', 'meta', 'openai', 'chatgpt'],
            'conceptos': ['econom√≠a', 'pol√≠tica', 'inflaci√≥n', 'd√≥lar', 'peso', 'elecciones', 
                         'democracia', 'libertad', 'socialismo', 'capitalismo', 'crisis'],
            'temporal': ['2024', '2025', 'enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio',
                        'actual', 'reciente', 'nuevo', '√∫ltima', '√∫ltimo'],
            'musica': ['arctic', 'monkeys', 'banda', 'm√∫sica', 'rock', 'discograf√≠a', '√°lbum', 
                      'canci√≥n', 'concierto', 'gira', 'festival'],
            'anime_manga': ['attack', 'titan', 'shingeki', 'kyojin', 'eren', 'mikasa', 'armin', 'levi', 
                           'anime', 'manga', 'naruto', 'one', 'piece', 'dragon', 'ball', 'studio', 
                           'ghibli', 'miyazaki', 'otaku', 'cosplay'],
            'entretenimiento': ['netflix', 'disney', 'marvel', 'dc', 'comics', 'superhero', 'movie', 
                               'film', 'serie', 'temporada', 'episodio', 'actor', 'actress', 'director']
        }
        
        # Palabras meta que deben eliminarse COMPLETAMENTE
        self.meta_words = {
            'instrucciones': ['buscar', 'informaci√≥n', 'sobre', 'acerca', 'utilizar', 'herramienta', 
                             'web_search', 'realizar', 'generar', 'crear', 'obtener', 'necesario',
                             'completar', 'espec√≠fico', 'datos', 'an√°lisis', 'informe', 'recopilar',
                             'desarrollar', 'estrategia', 'b√∫squeda', 'actual', 'actuales', 'web'],
            'conectores': ['para', 'con', 'por', 'desde', 'hasta', 'durante', 'mediante', 'seg√∫n',
                          'ante', 'bajo', 'contra', 'entre', 'hacia', 'seg√∫n', 'sin', 'tras'],
            'articulos': ['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas'],
            'pronombres': ['que', 'cual', 'quien', 'donde', 'cuando', 'como', 'este', 'esta', 'ese', 'esa']
        }
        
        # Patrones problem√°ticos que indican queries mal formados
        self.problematic_patterns = [
            r'\brealiza\s+informe\b',
            r'\butilizar\s+herramienta\b', 
            r'\bweb_search\s+para\b',
            r'\binformaci√≥n\s+espec√≠fica\s+sobre\b',
            r'\bgenera\s+(un|una)\s+(an√°lisis|informe|reporte)\b',
            r'\brecopilar\s+datos\s+de\s+mercado\b',
            r'\brealizar\s+una\s+b√∫squeda\s+web\b',
            r'\bdesarrollar\s+una\s+estrategia\b',
            r'\bobtener\s+datos\s+actuales\b'
        ]
    
    def get_intelligent_keywords(self, query_text: str) -> str:
        """üéØ FUNCI√ìN PRINCIPAL: Generar keywords inteligentes CON VALIDACI√ìN Y APROBACI√ìN"""
        
        if not query_text or len(query_text.strip()) < 3:
            return "informaci√≥n actualizada"
        
        print(f"üß† INTELLIGENT GENERATOR INPUT: '{query_text}'")
        
        # üîÑ SISTEMA DE VALIDACI√ìN Y APROBACI√ìN - M√ÅXIMO 5 INTENTOS
        max_attempts = 5
        for attempt in range(1, max_attempts + 1):
            print(f"üîÑ INTENTO {attempt}/{max_attempts}: Generando query...")
            
            # 1. Generar query candidata
            candidate_query = self._generate_candidate_query(query_text, attempt)
            print(f"üìù QUERY CANDIDATA: '{candidate_query}'")
            
            # 2. VALIDAR Y APROBAR la query candidata
            validation_result = self._validate_and_approve_query(candidate_query, query_text)
            
            if validation_result['approved']:
                print(f"‚úÖ QUERY APROBADA en intento {attempt}: '{candidate_query}'")
                print(f"‚úÖ RAZONES DE APROBACI√ìN: {', '.join(validation_result['approval_reasons'])}")
                return candidate_query
            else:
                print(f"‚ùå QUERY RECHAZADA en intento {attempt}: '{candidate_query}'")
                print(f"‚ùå RAZONES DE RECHAZO: {', '.join(validation_result['rejection_reasons'])}")
                if attempt < max_attempts:
                    print(f"üîÑ Generando nueva query (intento {attempt + 1})...")
                continue
        
        # Si todos los intentos fallaron, usar query de emergencia espec√≠fica
        emergency_query = self._generate_emergency_query(query_text)
        print(f"üö® EMERGENCY QUERY USADA: '{emergency_query}'")
        return emergency_query
    
    def _generate_candidate_query(self, query_text: str, attempt_number: int) -> str:
        """üéØ Generar query candidata usando diferentes estrategias seg√∫n el intento"""
        
        # Estrategia 1 (Intento 1): L√≥gica original mejorada
        if attempt_number == 1:
            if self._is_problematic_query(query_text):
                return self._fix_problematic_query(query_text)
            else:
                entities = self._extract_important_entities(query_text)
                concepts = self._extract_main_concepts(query_text)
                return self._combine_and_optimize(entities, concepts, query_text)
        
        # Estrategia 2 (Intento 2): Enfoque conservador - solo entidades principales
        elif attempt_number == 2:
            entities = self._extract_important_entities(query_text)
            return ' '.join(entities[:3]) if entities else self._extract_main_subject_from_text(query_text)
        
        # Estrategia 3 (Intento 3): Enfoque espec√≠fico - extraer tema exacto
        elif attempt_number == 3:
            main_subject = self._extract_main_subject_from_text(query_text)
            if main_subject:
                return main_subject
            else:
                return self._extract_key_terms_only(query_text)
        
        # Estrategia 4 (Intento 4): Enfoque minimalista - t√©rminos clave √∫nicos
        elif attempt_number == 4:
            return self._extract_key_terms_only(query_text)
        
        # Estrategia 5 (Intento 5): √öltimo recurso - tema + contexto b√°sico
        else:
            base_topic = self._extract_main_subject_from_text(query_text) or "informaci√≥n"
            return f"{base_topic} informaci√≥n actualizada"
    
    def _validate_and_approve_query(self, candidate_query: str, original_text: str) -> dict:
        """üõ°Ô∏è SISTEMA DE VALIDACI√ìN Y APROBACI√ìN DE QUERIES"""
        
        approval_reasons = []
        rejection_reasons = []
        
        # VALIDACI√ìN 1: Longitud apropiada (2-6 palabras)
        words = candidate_query.strip().split()
        if 2 <= len(words) <= 6:
            approval_reasons.append("Longitud apropiada")
        else:
            rejection_reasons.append(f"Longitud incorrecta: {len(words)} palabras")
        
        # VALIDACI√ìN 2: Sin duplicaciones de palabras
        unique_words = list(dict.fromkeys(words))  # Mantener orden, remover duplicados
        
        if len(unique_words) == len(words):
            approval_reasons.append("Sin duplicaciones")
        else:
            rejection_reasons.append(f"Contiene palabras duplicadas: {words} ‚Üí {unique_words}")
        
        # VALIDACI√ìN 3: Preserva tema principal del contexto original
        main_subjects = self._extract_known_subjects(original_text)
        if main_subjects:
            query_lower = candidate_query.lower()
            subject_preserved = any(subject.lower() in query_lower for subject in main_subjects)
            if subject_preserved:
                approval_reasons.append("Preserva tema principal")
            else:
                rejection_reasons.append("No preserva tema principal identificado")
        
        # VALIDACI√ìN 4: No contiene palabras meta prohibidas
        meta_words = ['investigar', 'informaci√≥n', 'buscar', 'datos', 'an√°lisis', 'realizar', 'web', 'search']
        query_lower = candidate_query.lower()
        meta_found = [word for word in meta_words if word in query_lower]
        if not meta_found:
            approval_reasons.append("Sin palabras meta")
        else:
            rejection_reasons.append(f"Contiene palabras meta: {', '.join(meta_found)}")
        
        # VALIDACI√ìN 5: Contiene t√©rminos espec√≠ficos reconocibles
        recognizable_terms = self._count_recognizable_terms(candidate_query)
        if recognizable_terms >= 2:
            approval_reasons.append(f"T√©rminos espec√≠ficos: {recognizable_terms}")
        else:
            rejection_reasons.append("Pocos t√©rminos espec√≠ficos reconocibles")
        
        # VALIDACI√ìN 6: No es gen√©rica
        generic_patterns = ['noticias actualidad', 'informaci√≥n completa', 'datos actuales', 'informaci√≥n actualizada']
        is_generic = any(pattern in query_lower for pattern in generic_patterns)
        if not is_generic:
            approval_reasons.append("Query espec√≠fica")
        else:
            rejection_reasons.append("Query demasiado gen√©rica")
        
        # DECISI√ìN DE APROBACI√ìN: 
        # 1. RECHAZOS AUTOM√ÅTICOS: Ciertas validaciones son OBLIGATORIAS
        automatic_rejections = [
            'Contiene palabras duplicadas',
            'No preserva tema principal identificado',
            'Longitud incorrecta'
        ]
        
        has_automatic_rejection = any(
            any(auto_reject in reason for auto_reject in automatic_rejections) 
            for reason in rejection_reasons
        )
        
        if has_automatic_rejection:
            approved = False  # Rechazo autom√°tico
        else:
            # 2. APROBACI√ìN NORMAL: Debe pasar al menos 4 de las 6 validaciones  
            approved = len(approval_reasons) >= 4 and len(rejection_reasons) <= 2
        
        return {
            'approved': approved,
            'approval_reasons': approval_reasons,
            'rejection_reasons': rejection_reasons,
            'score': len(approval_reasons) - len(rejection_reasons)
        }
    
    def _extract_known_subjects(self, text: str) -> list:
        """üéØ Extraer sujetos/temas conocidos del texto"""
        known_subjects = []
        text_lower = text.lower()
        
        # Temas espec√≠ficos conocidos
        specific_subjects = [
            ('arctic monkeys', 'Arctic Monkeys'),
            ('attack on titan', 'Attack on Titan'), 
            ('attack titan', 'Attack on Titan'),
            ('shingeki no kyojin', 'Shingeki no Kyojin'),
            ('javier milei', 'Javier Milei'),
            ('inteligencia artificial', 'Inteligencia Artificial'),
            ('machine learning', 'Machine Learning'),
            ('cambio clim√°tico', 'Cambio Clim√°tico')
        ]
        
        for search_term, subject_name in specific_subjects:
            if search_term in text_lower:
                known_subjects.append(subject_name)
        
        # Buscar nombres propios
        import re
        proper_nouns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
        for noun in proper_nouns:
            if len(noun.split()) <= 3:  # M√°ximo 3 palabras
                known_subjects.append(noun)
        
        return list(set(known_subjects))  # Remover duplicados
    
    def _count_recognizable_terms(self, query: str) -> int:
        """üìä Contar t√©rminos espec√≠ficos reconocibles en la query"""
        words = query.lower().split()
        recognizable = 0
        
        # T√©rminos de todas las categor√≠as de entidades conocidas
        all_entities = []
        for category in self.preserve_entities.values():
            all_entities.extend(category)
        
        for word in words:
            if (word in all_entities or 
                len(word) >= 5 or  # Palabras largas suelen ser espec√≠ficas
                word.istitle()):   # Nombres propios
                recognizable += 1
        
        return recognizable
    
    def _extract_main_subject_from_text(self, text: str) -> str:
        """üìù Extraer el tema principal del texto de manera simple y directa"""
        # Primero buscar temas conocidos
        text_lower = text.lower()
        
        known_mappings = {
            'arctic monkeys': 'Arctic Monkeys',
            'attack on titan': 'Attack on Titan',
            'attack titan': 'Attack on Titan', 
            'shingeki no kyojin': 'Shingeki no Kyojin',
            'javier milei': 'Javier Milei',
            'inteligencia artificial': 'inteligencia artificial',
        }
        
        for search_term, subject in known_mappings.items():
            if search_term in text_lower:
                return subject
        
        # Buscar nombres propios
        import re
        proper_nouns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)?\b', text)
        if proper_nouns:
            return proper_nouns[0]
        
        # Extraer palabras significativas (fallback)
        words = re.findall(r'\b[a-zA-Z]{4,}\b', text)
        significant_words = []
        
        meta_filter = {'investigar', 'informaci√≥n', 'buscar', 'datos', 'an√°lisis', 'sobre', 'acerca'}
        for word in words:
            if word.lower() not in meta_filter:
                significant_words.append(word.lower())
        
        return ' '.join(significant_words[:2]) if significant_words else "informaci√≥n"
    
    def _extract_key_terms_only(self, text: str) -> str:
        """üîë Extraer solo t√©rminos clave √∫nicos y espec√≠ficos"""
        import re
        
        # Buscar t√©rminos espec√≠ficos √∫nicos
        words = re.findall(r'\b[a-zA-Z]{3,}\b', text)
        
        # Filtrar palabras meta y comunes
        meta_filter = {
            'investigar', 'informaci√≥n', 'buscar', 'datos', 'an√°lisis', 'sobre', 'acerca',
            'realizar', 'generar', 'crear', 'obtener', 'utilizar', 'espec√≠fica', 'completa',
            'actualizada', 'relevante', 'importante', 'necesaria', 'web', 'search', 'para',
            'con', 'del', 'las', 'los', 'una', 'mediante'
        }
        
        key_terms = []
        seen = set()
        
        for word in words:
            word_lower = word.lower()
            if (word_lower not in meta_filter and 
                word_lower not in seen and 
                len(word) >= 3):
                key_terms.append(word_lower)
                seen.add(word_lower)
        
        return ' '.join(key_terms[:4]) if key_terms else "informaci√≥n espec√≠fica"
    
    def _generate_emergency_query(self, original_text: str) -> str:
        """üö® Generar query de emergencia cuando todos los intentos fallan"""
        # Extraer tema principal o usar t√©rminos b√°sicos
        main_subject = self._extract_main_subject_from_text(original_text)
        if main_subject and main_subject != "informaci√≥n":
            return main_subject
        
        # Fallback final basado en contexto detectado
        text_lower = original_text.lower()
        if 'arctic' in text_lower:
            return "Arctic Monkeys"
        elif 'attack' in text_lower and 'titan' in text_lower:
            return "Attack on Titan"
        elif 'milei' in text_lower:
            return "Javier Milei"
        elif 'inteligencia' in text_lower:
            return "inteligencia artificial"
        else:
            return "informaci√≥n noticias actualidad"
    
    def get_multiple_search_variants(self, query_text: str, count: int = 3) -> List[str]:
        """üîÑ Generar m√∫ltiples variantes de b√∫squeda para diversidad"""
        
        base_keywords = self.get_intelligent_keywords(query_text)
        variants = [base_keywords]
        
        # Variante 1: Con contexto temporal
        if '2025' not in base_keywords and '2024' not in base_keywords:
            variants.append(f"{base_keywords} 2025 actualidad")
        
        # Variante 2: Con especificidad geogr√°fica si aplicable
        if any(lugar in query_text.lower() for lugar in self.preserve_entities['lugares']):
            variants.append(f"{base_keywords} noticias recientes")
        else:
            variants.append(f"{base_keywords} informaci√≥n completa")
        
        # Variante 3: Con enfoque espec√≠fico
        if any(persona in query_text.lower() for persona in self.preserve_entities['personas']):
            variants.append(f"{base_keywords} biograf√≠a trayectoria")
        elif any(tech in query_text.lower() for tech in self.preserve_entities['tecnologia']):
            variants.append(f"{base_keywords} definici√≥n caracter√≠sticas")
        else:
            variants.append(f"{base_keywords} gu√≠a completa")
        
        return variants[:count]
    
    def _is_problematic_query(self, query_text: str) -> bool:
        """üö® Detectar queries problem√°ticos que generan keywords in√∫tiles"""
        
        query_lower = query_text.lower()
        
        # üéØ FIRST CHECK: Si contiene temas espec√≠ficos conocidos, NUNCA es problem√°tico
        specific_topics = [
            'attack on titan', 'shingeki no kyojin', 'eren jaeger', 'mikasa ackerman',
            'arctic monkeys', 'alex turner', 'the strokes', 'coldplay',
            'inteligencia artificial', 'machine learning', 'chatgpt', 'openai',
            'javier milei', 'argentina presidente', 'elecciones argentina',
            'netflix series', 'disney plus', 'marvel movies', 'dc comics',
            'bitcoin price', 'cryptocurrency', 'blockchain technology',
            'climate change', 'global warming', 'renewable energy'
        ]
        
        for topic in specific_topics:
            if topic in query_lower:
                print(f"‚úÖ SPECIFIC TOPIC DETECTED: '{topic}' - NOT problematic")
                return False
        
        # üéØ SECOND CHECK: Si tiene nombres propios claros, probablemente no es problem√°tico
        proper_nouns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', query_text)
        if len(proper_nouns) >= 2:  # Dos o m√°s nombres propios = tema espec√≠fico
            print(f"‚úÖ PROPER NOUNS DETECTED: {proper_nouns} - NOT problematic")
            return False
        
        # Verificar patrones problem√°ticos conocidos
        for pattern in self.problematic_patterns:
            if re.search(pattern, query_lower):
                return True
        
        # Verificar si tiene muchas palabras meta vs. pocas entidades
        meta_count = sum(1 for category in self.meta_words.values() 
                        for word in category if word in query_lower)
        
        entity_count = sum(1 for category in self.preserve_entities.values()
                          for entity in category if entity in query_lower)
        
        # üéØ AJUSTE CR√çTICO: Solo es problem√°tico si hay MUCHAS m√°s palabras meta que entidades
        # Y no contiene temas espec√≠ficos obvios
        if meta_count > 5 and entity_count < 1:
            # DOUBLE CHECK: Buscar entidades que no est√©n en la lista pero que sean obvias
            potential_entities = re.findall(r'\b[a-zA-Z]{4,}\b', query_text)
            non_meta_entities = []
            
            for word in potential_entities:
                word_lower = word.lower()
                if not any(word_lower in metas for metas in self.meta_words.values()):
                    non_meta_entities.append(word_lower)
            
            # Si hay entidades potenciales no-meta, NO es problem√°tico
            if len(non_meta_entities) >= 2:
                print(f"‚úÖ POTENTIAL ENTITIES FOUND: {non_meta_entities} - NOT problematic")
                return False
                
            return True
        
        return False
    
    def _fix_problematic_query(self, query_text: str) -> str:
        """üîß Reparar queries problem√°ticos extrayendo el tema real"""
        
        query_lower = query_text.lower()
        
        # Buscar patrones espec√≠ficos de tema
        theme_patterns = [
            r'sobre\s+"([^"]+)"',  # sobre "tema"
            r'sobre\s+([a-z√°√©√≠√≥√∫√±\s]+?)(?:\s+en\s|\s+con\s|$)',  # sobre tema
            r'informaci√≥n.*?([a-z√°√©√≠√≥√∫√±]{4,}(?:\s+[a-z√°√©√≠√≥√∫√±]{4,})*)',  # informaci√≥n tema
            r'an√°lisis.*?([a-z√°√©√≠√≥√∫√±]{4,}(?:\s+[a-z√°√©√≠√≥√∫√±]{4,})*)',  # an√°lisis tema
            r'mercado\s+objetivo.*?(redes\s+sociales|social\s+media|marketing)',  # mercado + marketing
            r'tendencias\s+de\s+(redes\s+sociales|social\s+media|marketing)',  # tendencias marketing
            r'estrategia\s+de\s+(marketing|redes\s+sociales|social\s+media)',  # estrategia marketing
            r'comportamiento\s+de\s+la\s+audiencia',  # comportamiento audiencia
        ]
        
        for pattern in theme_patterns:
            match = re.search(pattern, query_text, re.IGNORECASE)
            if match:
                theme = match.group(1).strip()
                print(f"üîß TEMA EXTRA√çDO DE QUERY PROBLEM√ÅTICO: '{theme}'")
                return self._clean_and_enhance_theme(theme)
        
        # Buscar conceptos espec√≠ficos del contexto de marketing
        marketing_concepts = ['marketing', 'redes sociales', 'social media', 'audiencia', 
                             'mercado', 'tendencias', 'comportamiento', 'digital', 'contenido']
        
        found_concepts = []
        for concept in marketing_concepts:
            if concept in query_lower:
                found_concepts.append(concept)
        
        if found_concepts:
            result = ' '.join(found_concepts[:3]) + ' 2025'
            print(f"üîß CONCEPTOS MARKETING EXTRA√çDOS: '{result}'")
            return result
        
        # Si no se encuentra patr√≥n, usar extracci√≥n de entidades de emergencia
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√±A-Z√Å√â√ç√ì√ö√ë]{4,}\b', query_text)
        significant_words = []
        
        for word in words:
            word_lower = word.lower()
            # Incluir si es entidad importante o no es palabra meta
            if (any(word_lower in entities for entities in self.preserve_entities.values()) or
                not any(word_lower in metas for metas in self.meta_words.values())):
                significant_words.append(word_lower)
        
        if significant_words:
            result = ' '.join(significant_words[:4])
            print(f"üîß EMERGENCIA - Palabras significativas extra√≠das: '{result}'")
            return result
        
        # √öltimo recurso
        return "marketing digital redes sociales 2025"
    
    def _clean_and_enhance_theme(self, theme: str) -> str:
        """‚ú® Limpiar y mejorar tema extra√≠do"""
        
        # Remover comillas y espacios extra
        clean_theme = re.sub(r'["\']', '', theme).strip()
        
        # Si el tema es muy corto, agregarlo contexto relevante
        if len(clean_theme.split()) < 2:
            if any(keyword in clean_theme.lower() for keyword in ['marketing', 'social', 'redes']):
                return f"{clean_theme} estrategias marketing digital 2025"
            elif 'inteligencia' in clean_theme.lower():
                return f"{clean_theme} tendencias aplicaciones 2025"
            else:
                return f"{clean_theme} informaci√≥n completa actualizada 2025"
        
        # Si ya es un buen tema, agregar contexto temporal
        if '2024' not in clean_theme and '2025' not in clean_theme:
            return f"{clean_theme} 2025"
        
        return clean_theme
    
    def _extract_important_entities(self, query_text: str) -> List[str]:
        """üè∑Ô∏è Extraer entidades importantes (nombres propios, conceptos clave) - CORREGIDO FILTRADO META"""
        
        entities = []
        query_lower = query_text.lower()
        
        # üî• FILTRO CR√çTICO: Lista completa de palabras meta que nunca deben ser entidades
        meta_filter_entities = {
            'investigar', 'informaci√≥n', 'espec√≠fica', 'buscar', 'datos', 'sobre', 'acerca',
            'realizar', 'generar', 'crear', 'an√°lisis', 'informe', 'completo', 'completa',
            'recopilar', 'obtener', 'utilizar', 'herramienta', 'web', 'search', 'incluyendo',
            'mediante', 'para', 'con', 'del', 'las', 'los', 'una', 'actualizada', 'actuales',
            'relevante', 'relevantes', 'importante', 'importantes', 'necesario', 'necesaria',
            'completar', 'desarrollo', 'espec√≠ficos', 'general', 'generales'
        }
        
        # Buscar todas las entidades de alta prioridad (filtradas)
        for category, entity_list in self.preserve_entities.items():
            for entity in entity_list:
                if entity in query_lower and entity not in meta_filter_entities:
                    entities.append(entity)
        
        # Buscar nombres propios adicionales (Capitalizados) y filtrar meta words
        proper_nouns = re.findall(r'\b[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]{2,}(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+)*\b', query_text)
        for noun in proper_nouns:
            noun_lower = noun.lower()
            if (len(noun) > 3 and 
                noun_lower not in meta_filter_entities and
                not any(meta in noun_lower for meta in ['investig', 'informac', 'buscar', 'datos'])):
                entities.append(noun_lower)
        
        # Remover duplicados manteniendo orden
        seen = set()
        unique_entities = []
        for entity in entities:
            if entity not in seen:
                seen.add(entity)
                unique_entities.append(entity)
        
        return unique_entities[:6]  # M√°ximo 6 entidades m√°s importantes
    
    def _extract_main_concepts(self, query_text: str) -> List[str]:
        """üí° Extraer conceptos principales del texto - CORREGIDO PARA FILTRAR MEJOR"""
        
        concepts = []
        
        # üî• LISTA AMPLIADA DE PALABRAS META QUE NUNCA DEBEN INCLUIRSE
        extended_meta_filter = {
            'investigar', 'informaci√≥n', 'espec√≠fica', 'buscar', 'datos', 'sobre', 'acerca',
            'realizar', 'generar', 'crear', 'an√°lisis', 'informe', 'completo', 'completa',
            'recopilar', 'obtener', 'utilizar', 'herramienta', 'web', 'search', 'incluyendo',
            'mediante', 'para', 'con', 'del', 'las', 'los', 'una', 'actualizada', 'actuales',
            'relevante', 'relevantes', 'importante', 'importantes', 'necesario', 'necesaria',
            'completar', 'desarrollo', 'espec√≠ficos', 'general', 'generales', 'relacionados',
            'relacionadas', 'particular', 'particulares', 'diversos', 'diversas'
        }
        
        # Extraer palabras significativas (4+ caracteres)
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√±A-Z√Å√â√ç√ì√ö√ë]{4,}\b', query_text)
        
        for word in words:
            word_lower = word.lower()
            
            # DOBLE FILTRO: palabras meta del objeto + lista extendida
            is_meta_original = any(word_lower in meta_category for meta_category in self.meta_words.values())
            is_meta_extended = word_lower in extended_meta_filter
            
            # Solo incluir si NO es palabra meta en ninguna de las listas
            if not is_meta_original and not is_meta_extended and len(word_lower) >= 4:
                concepts.append(word_lower)
        
        return concepts[:8]  # M√°ximo 8 conceptos
    
    def _combine_and_optimize(self, entities: List[str], concepts: List[str], original_query: str) -> str:
        """‚ö° Combinar y optimizar entidades y conceptos - CORREGIDO PARA FILTRAR PALABRAS META"""
        
        # üî• FILTRO CR√çTICO: Remover palabras meta de TODAS las listas
        meta_filter = {
            'investigar', 'informaci√≥n', 'espec√≠fica', 'buscar', 'datos', 'sobre', 'acerca',
            'realizar', 'generar', 'crear', 'an√°lisis', 'informe', 'completo', 'completa',
            'recopilar', 'obtener', 'utilizar', 'herramienta', 'web', 'search', 'incluyendo',
            'mediante', 'para', 'con', 'del', 'las', 'los', 'una', 'actualizada', 'actuales',
            'relevante', 'relevantes', 'importante', 'importantes', 'necesario', 'necesaria'
        }
        
        # 1. FILTRAR ENTIDADES - Remover palabras meta
        filtered_entities = []
        for entity in entities:
            if entity.lower() not in meta_filter and len(entity) >= 3:
                filtered_entities.append(entity)
        
        # 2. FILTRAR CONCEPTOS - Remover palabras meta  
        filtered_concepts = []
        for concept in concepts:
            if concept.lower() not in meta_filter and len(concept) >= 3:
                filtered_concepts.append(concept)
        
        # 3. Priorizar entidades limpias (son m√°s importantes)
        important_terms = filtered_entities[:4]  # Top 4 entidades sin meta words
        
        # 4. Agregar conceptos limpios que no sean repetitivos
        for concept in filtered_concepts:
            if (concept not in important_terms and 
                len(important_terms) < 5 and
                not any(concept in term for term in important_terms)):  # Evitar redundancia
                important_terms.append(concept)
        
        # 5. Si a√∫n muy pocos t√©rminos despu√©s del filtrado, agregar contexto espec√≠fico
        if len(important_terms) < 2:
            # Buscar t√©rminos espec√≠ficos que NO sean meta
            query_lower = original_query.lower()
            
            # Para anime/manga
            if any(term in query_lower for term in ['attack on titan', 'shingeki', 'anime', 'manga']):
                if 'attack' not in important_terms and 'titan' not in important_terms:
                    important_terms.extend(['attack', 'titan'])
                important_terms.append('anime')
            # Para m√∫sica
            elif any(term in query_lower for term in ['arctic monkeys', 'banda', 'm√∫sica', 'discograf√≠a']):
                important_terms.append('m√∫sica')
                important_terms.append('banda')
            # Para personas
            elif any(persona in query_lower for persona in self.preserve_entities['personas']):
                important_terms.append('biograf√≠a')
            # Para tecnolog√≠a  
            elif any(tech in query_lower for tech in self.preserve_entities['tecnologia']):
                important_terms.append('tecnolog√≠a')
            # Fallback gen√©rico
            else:
                important_terms.append('noticias')
        
        # 6. Construir resultado final SIN palabras meta
        result = ' '.join(important_terms[:5])  # M√°ximo 5 t√©rminos
        
        # 7. VALIDACI√ìN FINAL: Si el resultado contiene palabras meta, limpiar
        for meta_word in meta_filter:
            result = result.replace(meta_word, '').strip()
        
        # Limpiar espacios m√∫ltiples
        result = ' '.join(result.split())
        
        # 8. Si el resultado es muy corto despu√©s de la limpieza, agregar contexto
        if len(result.split()) < 2:
            if any(term in original_query.lower() for term in ['attack on titan', 'titan']):
                result = 'attack titan anime manga'
            elif any(term in original_query.lower() for term in ['arctic monkeys']):
                result = 'arctic monkeys m√∫sica banda'
            else:
                result += ' noticias actualidad'
        
        # 9. Agregar a√±o solo si es muy corto y no tiene contexto temporal
        if ('2024' not in result and '2025' not in result and len(result.split()) < 3):
            result += ' 2025'
        
        return result.strip()

    def detect_granular_search_needs(self, query_text: str) -> List[Dict[str, str]]:
        """
        üéØ DETECTOR GRANULAR MEJORADO - Detecta autom√°ticamente necesidad de b√∫squedas m√∫ltiples
        
        CORRECCI√ìN CR√çTICA: Ahora detecta temas espec√≠ficos que siempre necesitan granularidad,
        sin requerir indicadores de "informaci√≥n completa" expl√≠citos
        """
        query_lower = query_text.lower()
        searches = []
        
        print(f"üîç ANALYZING QUERY FOR GRANULAR NEEDS: '{query_text}'")
        
        # üéØ STEP 1: EXTRAER EL TEMA/SUJETO PRINCIPAL PRIMERO
        main_subject = self._extract_main_subject_generic(query_text)
        
        if not main_subject:
            print("‚ùå No se pudo extraer tema principal")
            return []
            
        print(f"‚úÖ TEMA PRINCIPAL DETECTADO: '{main_subject}'")
        
        # üéØ STEP 2: CLASIFICAR TIPO DE TEMA
        subject_type = self._classify_subject_type_generic(main_subject, query_text)
        print(f"üìä TIPO DE TEMA CLASIFICADO: {subject_type}")
        
        # üéØ STEP 3: NUEVO CRITERIO - DETECTAR SI EL TEMA INHERENTEMENTE NECESITA GRANULARIDAD
        needs_granularity = self._subject_needs_granular_search(main_subject, subject_type, query_text)
        print(f"üîç NECESITA GRANULARIDAD: {needs_granularity}")
        
        if not needs_granularity:
            # Solo retornar si NO es un tema que inherentemente necesita granularidad
            print("‚ùå Tema no requiere b√∫squedas granulares - b√∫squeda simple")
            return []
        
        # üéØ STEP 4: DETECTAR ASPECTOS ESPEC√çFICOS MENCIONADOS
        mentioned_aspects = self._extract_mentioned_aspects(query_text)
        print(f"üéØ ASPECTOS MENCIONADOS: {mentioned_aspects}")
        
        # üéØ STEP 5: GENERAR B√öSQUEDAS GRANULARES BASADAS EN TIPO DE TEMA
        searches = self._generate_searches_by_type_generic(main_subject, subject_type, mentioned_aspects)
        
        print(f"‚úÖ B√öSQUEDAS GRANULARES GENERADAS: {len(searches)}")
        for search in searches:
            print(f"   üéØ {search['category']}: {search['query']}")
        
        return searches if len(searches) > 1 else []
    
    def _subject_needs_granular_search(self, subject: str, subject_type: str, query_text: str) -> bool:
        """
        üéØ NUEVO M√âTODO - Determinar si un tema espec√≠fico necesita b√∫squedas granulares autom√°ticamente
        
        CRITERIOS:
        1. Personas p√∫blicas conocidas (pol√≠ticos, artistas, etc.)
        2. Temas complejos (tecnolog√≠a, econom√≠a, etc.) 
        3. Obras de entretenimiento (series, pel√≠culas, libros)
        4. VIDEOJUEGOS (Age of Empires, etc.) - NUEVO CRITERIO CR√çTICO
        5. Eventos o fen√≥menos importantes
        6. Solicitudes expl√≠citas de informaci√≥n comprehensiva
        """
        subject_lower = subject.lower()
        query_lower = query_text.lower()
        
        print(f"üîç EVALUATING GRANULAR NEED FOR: '{subject}' (type: {subject_type})")
        
        # üéÆ CRITERIO CR√çTICO NUEVO: VIDEOJUEGOS SIEMPRE NECESITAN GRANULARIDAD
        videogame_indicators = [
            # T√©rminos espec√≠ficos de videojuegos
            'age of empires', 'age empires', 'aoe', 'civilization', 'civ', 'total war',
            'counter strike', 'cs:go', 'cs2', 'valorant', 'league of legends', 'lol',
            'dota', 'fortnite', 'minecraft', 'world of warcraft', 'wow', 'overwatch',
            'call of duty', 'cod', 'battlefield', 'apex legends', 'fifa', 'pes',
            'grand theft auto', 'gta', 'red dead redemption', 'the witcher',
            # Contexto de videojuegos
            'mec√°nicas de juego', 'mec√°nicas juego', 'gameplay', 'jugabilidad',
            'expansiones', 'dlc', 'actualizaciones', 'parches', 'patches',
            'estad√≠sticas de jugadores', 'stats jugadores', 'ranking', 'competitivo',
            'impacto cultural gaming', 'comunidad gaming', 'esports', 'e-sports',
            'meta del juego', 'meta game', 'builds', 'estrategias juego'
        ]
        
        # Detectar videojuegos o contexto gaming
        is_videogame = (any(indicator in subject_lower for indicator in videogame_indicators) or
                       any(indicator in query_lower for indicator in videogame_indicators))
        
        if is_videogame:
            print(f"üéÆ VIDEOJUEGO DETECTADO - GRANULARIDAD AUTOM√ÅTICA REQUERIDA")
            print(f"üéØ Contexto gaming identificado en: '{subject}' o '{query_text[:50]}...'")
            return True
        
        # CRITERIO 1: PERSONAS P√öBLICAS CONOCIDAS - SIEMPRE necesitan granularidad
        known_public_figures = [
            # Pol√≠ticos argentinos
            'javier milei', 'milei', 'cristina fern√°ndez', 'cristina kirchner', 'alberto fern√°ndez',
            'mauricio macri', 'sergio massa', 'patricia bullrich', 'horacio rodr√≠guez larreta',
            # Pol√≠ticos internacionales  
            'joe biden', 'donald trump', 'vladimir putin', 'xi jinping', 'emmanuel macron',
            'elon musk', 'mark zuckerberg', 'bill gates', 'jeff bezos',
            # Deportistas
            'lionel messi', 'cristiano ronaldo', 'neymar', 'kylian mbapp√©',
            # Artistas/m√∫sicos
            'taylor swift', 'ed sheeran', 'billie eilish', 'coldplay', 'u2'
        ]
        
        for figure in known_public_figures:
            if figure in subject_lower:
                print(f"‚úÖ FIGURA P√öBLICA DETECTADA: {figure} - GRANULARIDAD REQUERIDA")
                return True
        
        # CRITERIO 2: TIPOS DE TEMA QUE INHERENTEMENTE NECESITAN GRANULARIDAD
        granular_subject_types = ['person', 'entertainment', 'music', 'technology', 'economics', 'literature', 'art', 'videogames', 'gaming']
        
        if subject_type in granular_subject_types:
            print(f"‚úÖ TIPO DE TEMA COMPLEJO: {subject_type} - GRANULARIDAD REQUERIDA")
            return True
        
        # CRITERIO 3: TEMAS ESPEC√çFICOS CONOCIDOS QUE SIEMPRE NECESITAN GRANULARIDAD
        complex_topics = [
            # Entretenimiento
            'attack on titan', 'attack titan', 'shingeki no kyojin', 'game of thrones',
            'breaking bad', 'the office', 'friends', 'stranger things',
            # M√∫sica
            'arctic monkeys', 'the beatles', 'radiohead', 'pink floyd',
            # Tecnolog√≠a 
            'inteligencia artificial', 'machine learning', 'blockchain', 'bitcoin',
            'chatgpt', 'openai', 'tesla', 'spacex',
            # Conceptos complejos
            'cambio clim√°tico', 'calentamiento global', 'crisis econ√≥mica',
            'guerra en ucrania', 'pandemia covid',
            # üéÆ VIDEOJUEGOS ESPEC√çFICOS AGREGADOS
            'age of empires 2', 'age empires 2', 'civilization 6', 'total war warhammer',
            'counter strike 2', 'valorant competitive', 'league legends meta'
        ]
        
        for topic in complex_topics:
            if topic in subject_lower:
                print(f"‚úÖ TEMA COMPLEJO DETECTADO: {topic} - GRANULARIDAD REQUERIDA")
                return True
        
        # CRITERIO 4: INDICADORES EXPL√çCITOS DE SOLICITUD COMPREHENSIVA
        comprehensive_indicators = [
            'informaci√≥n completa', 'datos completos', 'informaci√≥n sobre',
            'investigar sobre', 'buscar informaci√≥n', 'an√°lisis completo',
            'estudiar', 'recopilar informaci√≥n', 'informaci√≥n relevante',
            'aspectos importantes', 'caracter√≠sticas principales',
            'informaci√≥n espec√≠fica sobre', 'investigar informaci√≥n sobre',
            'incluyendo', 'que incluya', 'abarcando', 'cubriendo',
            'informaci√≥n detallada', 'datos detallados', 'an√°lisis detallado',
            'informe sobre', 'reporte sobre', 'investigar', 'buscar datos',
            'biograf√≠a', 'trayectoria', 'historia', 'antecedentes',
            'caracter√≠sticas', 'propiedades', 'aspectos',
            # üéÆ INDICADORES GAMING ESPEC√çFICOS
            'mec√°nicas de juego', 'historia del juego', 'expansiones', 
            'estad√≠sticas de jugadores', 'impacto cultural', 'comunidad gaming'
        ]
        
        has_comprehensive_request = any(indicator in query_lower for indicator in comprehensive_indicators)
        if has_comprehensive_request:
            print(f"‚úÖ SOLICITUD COMPREHENSIVA DETECTADA - GRANULARIDAD REQUERIDA")
            return True
        
        # CRITERIO 5: NOMBRES PROPIOS DE 2+ PALABRAS (probables personas/obras importantes)
        if len(subject.split()) >= 2 and subject[0].isupper():
            print(f"‚úÖ NOMBRE PROPIO COMPUESTO: {subject} - GRANULARIDAD PROBABLE")
            return True
        
        print(f"‚ùå NO REQUIERE GRANULARIDAD - B√∫squeda simple suficiente")
        return False
    
    def _extract_main_subject_generic(self, query_text: str) -> str:
        """üéØ Extraer el tema/sujeto principal de CUALQUIER consulta - VERSI√ìN CORREGIDA"""
        import re
        
        print(f"üîç EXTRACTING SUBJECT FROM: '{query_text}'")
        
        # üéØ M√âTODO ESPECIAL: Detectar nombres compuestos espec√≠ficos conocidos PRIMERO
        known_subjects = [
            r'\battack\s+on\s+titan\b',
            r'\battack\s+titan\b',  # ‚úÖ A√ëADIDO: Detectar "Attack Titan" tambi√©n
            r'\bshingeki\s+no\s+kyojin\b', 
            r'\barctic\s+monkeys\b',
            r'\bcoldplay\b',
            r'\bthe\s+strokes\b',
            r'\binteligencia\s+artificial\b',
            r'\bmachine\s+learning\b',
            r'\bjavier\s+milei\b',
            r'\bcambio\s+clim√°tico\b',
            r'\bcalentamiento\s+global\b'
        ]
        
        for subject_pattern in known_subjects:
            match = re.search(subject_pattern, query_text, re.IGNORECASE)
            if match:
                subject = match.group(0)
                # ‚úÖ CORRECCI√ìN ESPECIAL: Normalizar "Attack Titan" a "Attack on Titan"
                if subject.lower() == "attack titan":
                    subject = "Attack on Titan"
                print(f"‚úÖ SUBJECT FOUND (Method 0 - Known Subjects): '{subject}'")
                return subject.title()
        
        # M√âTODO 1: Buscar t√©rminos espec√≠ficos despu√©s de palabras clave (M√ÅS PRECISO)
        subject_patterns = [
            # Patrones con delimitadores claros
            r'sobre\s+"([^"]+)"',  # sobre "tema exacto"
            r'sobre\s+([a-zA-Z][a-zA-Z\s]+?)\s*(?:\s+incluyendo|\s+con|\s+-|\s+y\s|\s+para|\.|$)',  # sobre tema ANTES de "incluyendo"
            r'informaci√≥n\s+(?:completa\s+|espec√≠fica\s+)?sobre\s+([a-zA-Z][a-zA-Z\s]+?)\s*(?:\s+incluyendo|\s+con|\s+-|\s+y\s|\s+para|\.|$)',
            r'datos\s+(?:completos\s+)?sobre\s+([a-zA-Z][a-zA-Z\s]+?)\s*(?:\s+incluyendo|\s+con|\s+-|\s+y\s|\s+para|\.|$)',
            r'investigar\s+(?:informaci√≥n\s+(?:espec√≠fica\s+)?sobre\s+)?([a-zA-Z][a-zA-Z\s]+?)\s*(?:\s+incluyendo|\s+con|\s+-|\s+y\s|\s+para|\.|$)',
            r'buscar\s+informaci√≥n\s+(?:completa\s+|espec√≠fica\s+)?sobre\s+([a-zA-Z][a-zA-Z\s]+?)\s*(?:\s+incluyendo|\s+con|\s+-|\s+y\s|\s+para|\.|$)'
        ]
        
        for pattern in subject_patterns:
            match = re.search(pattern, query_text, re.IGNORECASE)
            if match:
                subject = match.group(1).strip()
                subject = re.sub(r'\s+', ' ', subject)  # Limpiar espacios m√∫ltiples
                
                # Validar que no sean palabras meta y que sea razonable
                meta_words = ['informaci√≥n', 'datos', 'sobre', 'an√°lisis', 'buscar', 'completa', 'completar', 'espec√≠fica']
                if (len(subject) > 3 and 
                    not any(meta.lower() in subject.lower() for meta in meta_words) and
                    not subject.lower() in ['y', 'el', 'la', 'los', 'las', 'de', 'del', 'con'] and
                    len(subject.split()) <= 4):  # No m√°s de 4 palabras
                    print(f"‚úÖ SUBJECT FOUND (Method 1): '{subject}'")
                    return subject
        
        # M√âTODO 2: Buscar nombres propios (2+ palabras capitalizadas)
        proper_nouns = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)+\b', query_text)
        for noun in proper_nouns:
            if len(noun.split()) >= 2 and len(noun.split()) <= 3:  # Entre 2 y 3 palabras
                print(f"‚úÖ SUBJECT FOUND (Method 2 - Proper Nouns): '{noun}'")
                return noun
        
        # M√âTODO 3: Buscar t√©rminos compuestos comunes (sin capitalizaci√≥n)
        # T√©rminos compuestos importantes conocidos
        known_compounds = [
            r'\bcambio\s+clim√°tico\b',
            r'\bcalentamiento\s+global\b',
            r'\binteligencia\s+artificial\b',
            r'\bmachine\s+learning\b',
            r'\baprendizaje\s+autom√°tico\b',
            r'\beconom√≠a\s+argentina\b',
            r'\beconom√≠a\s+mundial\b',
            r'\bcriptomonedas\b',
            r'\benerg√≠a\s+renovable\b',
            r'\bmedio\s+ambiente\b',
            r'\brecursos\s+naturales\b',
            r'\bdesarrollo\s+sostenible\b'
        ]
        
        for compound_pattern in known_compounds:
            match = re.search(compound_pattern, query_text, re.IGNORECASE)
            if match:
                subject = match.group(0)
                print(f"‚úÖ SUBJECT FOUND (Method 3 - Known Compounds): '{subject}'")
                return subject.title()
        
        # M√âTODO 4: Buscar cualquier t√©rmino compuesto de 2+ palabras ANTES DE "incluyendo"
        compound_before_including = re.search(r'\b([a-zA-Z]{3,}\s+[a-zA-Z]{3,}(?:\s+[a-zA-Z]{3,})?)\s+incluyendo\b', query_text, re.IGNORECASE)
        if compound_before_including:
            subject = compound_before_including.group(1).strip()
            meta_phrases = {'informaci√≥n sobre', 'datos sobre', 'buscar informaci√≥n', 'investigar sobre'}
            if subject.lower() not in meta_phrases and len(subject) > 8:
                print(f"‚úÖ SUBJECT FOUND (Method 4 - Before Including): '{subject}'")
                return subject.title()
        
        # M√âTODO 5: Buscar nombres propios simples importantes
        single_nouns = re.findall(r'\b[A-Z][a-z]{4,}\b', query_text)
        skip_words = {'Investigar', 'Buscar', 'Datos', 'Informaci√≥n', 'An√°lisis', 'Sobre', 'Para', 'Con', 'Realizar', 'Incluyendo'}
        
        for noun in single_nouns:
            if noun not in skip_words:
                print(f"‚úÖ SUBJECT FOUND (Method 5 - Single Proper Noun): '{noun}'")
                return noun
        
        print("‚ùå NO SUBJECT EXTRACTED")
        return None
    
    def _extract_mentioned_aspects(self, query_text: str) -> List[str]:
        """üîç Extraer aspectos espec√≠ficos mencionados en la consulta"""
        query_lower = query_text.lower()
        aspects = []
        
        # Mapeo de palabras clave a aspectos
        aspect_keywords = {
            'trama': ['trama', 'historia', 'argumento', 'narrativa', 'plot'],
            'personajes': ['personajes', 'protagonistas', 'caracteres', 'characters'],
            'biograf√≠a': ['biograf√≠a', 'vida', 'personal', 'nacimiento', 'historia personal'],
            'historia': ['historia', 'or√≠genes', 'desarrollo', 'evoluci√≥n', 'pasado'],
            'contexto': ['contexto', 'ambiente', '√©poca', 'per√≠odo', 'marco'],
            'cr√≠tica': ['cr√≠tica', 'recepci√≥n', 'opiniones', 'rese√±as', 'evaluaci√≥n'],
            'impacto': ['impacto', 'influencia', 'legado', 'consecuencias', 'efectos'],
            'caracter√≠sticas': ['caracter√≠sticas', 'propiedades', 'atributos', 'rasgos'],
            'causas': ['causas', 'or√≠genes', 'razones', 'motivos'],
            'efectos': ['efectos', 'consecuencias', 'resultados', 'impactos'],
            'soluciones': ['soluciones', 'remedios', 'propuestas', 'alternativas'],
            'obras': ['obras', 'trabajos', 'creaciones', 'producci√≥n'],
            'carrera': ['carrera', 'trayectoria', 'profesional', 'trabajo'],
            'pol√≠tica': ['pol√≠tica', 'posiciones', 'ideolog√≠a', 'propuestas']
        }
        
        for aspect, keywords in aspect_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                aspects.append(aspect)
        
        return aspects
    
    def _classify_subject_type_generic(self, subject: str, query_text: str) -> str:
        """üìä Clasificar gen√©ricamente el tipo de tema basado en indicadores"""
        subject_lower = subject.lower()
        query_lower = query_text.lower()
        
        # üéÆ NUEVA CATEGOR√çA CR√çTICA: VIDEOJUEGOS
        videogame_indicators = [
            'age of empires', 'age empires', 'aoe', 'civilization', 'total war',
            'counter strike', 'valorant', 'league of legends', 'dota', 'fortnite',
            'minecraft', 'call of duty', 'fifa', 'pes', 'overwatch', 'apex legends',
            'mec√°nicas de juego', 'gameplay', 'jugabilidad', 'expansiones', 'dlc',
            'estad√≠sticas de jugadores', 'gaming', 'videojuego', 'video juego'
        ]
        
        # Detectar videojuegos PRIMERO (alta prioridad)
        if (any(indicator in subject_lower for indicator in videogame_indicators) or
            any(indicator in query_lower for indicator in videogame_indicators)):
            return 'videogames'
        
        # Indicadores de tipo de tema
        if (any(indicator in query_lower for indicator in ['anime', 'manga', 'serie', 'pel√≠cula', 'film']) or
            any(name in subject_lower for name in ['attack on titan', 'attack titan', 'shingeki', 'naruto', 'one piece', 'dragon ball'])):
            return 'entertainment'
        elif any(indicator in query_lower for indicator in ['banda', 'm√∫sica', 'cantante', 'artista musical']):
            return 'music'  
        elif (any(indicator in query_lower for indicator in ['presidente', 'pol√≠tico', 'l√≠der', 'personalidad']) or
              any(name in subject_lower for name in ['javier milei', 'milei', 'cristina fern√°ndez', 'alberto fern√°ndez', 
                                                    'mauricio macri', 'sergio massa', 'biden', 'trump', 'putin'])):
            return 'person'
        elif any(indicator in query_lower for indicator in ['tecnolog√≠a', 'ciencia', 'cient√≠fico', 't√©cnico']):
            return 'technology'
        elif any(indicator in query_lower for indicator in ['econom√≠a', 'econ√≥mico', 'mercado', 'financiero']):
            return 'economics'
        elif any(indicator in query_lower for indicator in ['equipo', 'deporte', 'f√∫tbol', 'selecci√≥n']):
            return 'sports'
        elif any(indicator in query_lower for indicator in ['libro', 'novela', 'autor', 'literatura']):
            return 'literature'
        elif any(indicator in query_lower for indicator in ['pintor', 'artista', 'arte', 'pintura']):
            return 'art'
        elif any(indicator in query_lower for indicator in ['hist√≥rico', 'historia', '√©poca', 'per√≠odo']):
            return 'history'
        elif any(indicator in query_lower for indicator in ['empresa', 'compa√±√≠a', 'corporaci√≥n', 'negocio']):
            return 'business'
        else:
            # Clasificaci√≥n por nombres propios conocidos o contexto
            if len(subject.split()) >= 2:  # Nombres compuestos = probablemente persona o obra
                return 'person_or_work'
            else:
                return 'general_topic'
    
    def _generate_searches_by_type_generic(self, subject: str, subject_type: str, mentioned_aspects: List[str]) -> List[Dict[str, str]]:
        """üéØ GENERAR B√öSQUEDAS INTELIGENTES USANDO LLM - SIN PLANTILLAS HARDCODEADAS"""
        searches = []
        
        try:
            # USAR LLM PARA GENERAR B√öSQUEDAS INTELIGENTES Y ADAPTATIVAS
            searches = self._generate_intelligent_searches_with_llm(subject, subject_type, mentioned_aspects)
            
            if searches and len(searches) >= 3:
                print(f"‚úÖ LLM gener√≥ {len(searches)} b√∫squedas inteligentes para '{subject}'")
                return searches
            else:
                print(f"‚ö†Ô∏è LLM gener√≥ pocas b√∫squedas ({len(searches)}), usando fallback")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error usando LLM para b√∫squedas: {e}, usando fallback")
        
        # FALLBACK INTELIGENTE - Solo cuando el LLM falla
        return self._generate_fallback_searches(subject, mentioned_aspects)
    
    def _generate_intelligent_searches_with_llm(self, subject: str, subject_type: str, mentioned_aspects: List[str]) -> List[Dict[str, str]]:
        """üß† USAR LLM PARA GENERAR B√öSQUEDAS GRANULARES INTELIGENTES"""
        
        # Construir prompt inteligente y conciso para el LLM - CORREGIDO PARA EVITAR ALUCINACIONES
        aspects_text = f" Aspectos mencionados: {', '.join(mentioned_aspects)}." if mentioned_aspects else ""
        
        prompt = f"""Para el tema "{subject}", genera 5 b√∫squedas web efectivas pero GENERALES.{aspects_text}

IMPORTANTE: No inventes eventos espec√≠ficos, fechas exactas, o entrevistas particulares. Usa t√©rminos generales que permitan encontrar informaci√≥n actual.

Responde SOLO con JSON:
{{"searches": [
  {{"category": "aspecto1", "query": "b√∫squeda general efectiva 1"}},
  {{"category": "aspecto2", "query": "b√∫squeda general efectiva 2"}},  
  {{"category": "aspecto3", "query": "b√∫squeda general efectiva 3"}},
  {{"category": "aspecto4", "query": "b√∫squeda general efectiva 4"}},
  {{"category": "aspecto5", "query": "b√∫squeda general efectiva 5"}}
]}}

Ejemplos de b√∫squedas GENERALES efectivas:
- "{subject} biograf√≠a historia personal"
- "{subject} √∫ltimas noticias 2025" 
- "{subject} posiciones pol√≠ticas ideolog√≠a"
- "{subject} controversias pol√©micas"
- "{subject} declaraciones recientes actualidad"

NO uses eventos espec√≠ficos inventados."""

        try:
            # Importar OllamaService para generar respuestas inteligentes
            import sys
            import os
            sys.path.append('/app/backend')
            
            from backend.src.services.ollama_service import OllamaService
            
            print(f"üß† Consultando LLM para generar b√∫squedas inteligentes sobre '{subject}'...")
            
            # Crear instancia del servicio Ollama
            ollama_service = OllamaService()
            
            # Generar respuesta usando el LLM
            response = ollama_service.generate_response(
                prompt=prompt,
                context={'max_tokens': 800, 'temperature': 0.7},
                use_tools=False,
                task_id="search_generation"
            )
            
            if response and isinstance(response, dict) and response.get('response'):
                content = response['response'].strip()
                print(f"üîç Respuesta LLM recibida: {content[:200]}...")
                
                # Parsear respuesta JSON del LLM
                searches = self._parse_llm_search_response(content)
                
                if searches:
                    print(f"‚úÖ LLM gener√≥ {len(searches)} b√∫squedas inteligentes exitosamente")
                    return searches
                else:
                    print("‚ö†Ô∏è Error parseando respuesta JSON del LLM")
            else:
                print("‚ö†Ô∏è LLM no gener√≥ respuesta v√°lida")
                
        except ImportError:
            print("‚ö†Ô∏è OllamaService no disponible")
        except Exception as e:
            print(f"‚ö†Ô∏è Error ejecutando LLM: {e}")
        
        return []
    
    def _parse_llm_search_response(self, llm_response: str) -> List[Dict[str, str]]:
        """üìù Parsear respuesta JSON del LLM"""
        import json
        import re
        
        try:
            # Intentar extraer JSON de la respuesta
            # Buscar bloque JSON
            json_match = re.search(r'\{.*"searches".*\}', llm_response, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(0)
                data = json.loads(json_str)
                
                if 'searches' in data and isinstance(data['searches'], list):
                    searches = []
                    
                    for search_item in data['searches']:
                        if isinstance(search_item, dict) and 'category' in search_item and 'query' in search_item:
                            searches.append({
                                'category': search_item['category'].strip(),
                                'query': search_item['query'].strip()
                            })
                    
                    return searches[:5]  # M√°ximo 5 b√∫squedas
            
            # Fallback: intentar parsear l√≠nea por l√≠nea si no hay JSON v√°lido
            lines = llm_response.split('\n')
            searches = []
            
            for line in lines:
                if ':' in line and len(searches) < 5:
                    parts = line.split(':', 1)
                    if len(parts) == 2:
                        category = parts[0].strip().replace('-', '').replace('*', '').strip()
                        query = parts[1].strip()
                        
                        if len(category) > 0 and len(query) > 10:
                            searches.append({
                                'category': category,
                                'query': query
                            })
            
            return searches
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error parseando respuesta LLM: {e}")
            return []
    
    def _generate_fallback_searches(self, subject: str, mentioned_aspects: List[str]) -> List[Dict[str, str]]:
        """üõ†Ô∏è FALLBACK INTELIGENTE cuando el LLM no funciona"""
        
        print(f"üîÑ Generando b√∫squedas fallback inteligentes para '{subject}'")
        
        # Aspectos universales que aplican a casi cualquier tema
        universal_aspects = [
            ('informaci√≥n_general', f'{subject} informaci√≥n general descripci√≥n'),
            ('historia_contexto', f'{subject} historia antecedentes contexto'),
            ('caracter√≠sticas', f'{subject} caracter√≠sticas principales aspectos importantes'),
            ('impacto_relevancia', f'{subject} importancia relevancia impacto significado'),
            ('actualidad', f'{subject} noticias recientes actualidad 2025')
        ]
        
        # Si hay aspectos mencionados espec√≠ficos, priorizarlos
        if mentioned_aspects:
            specific_searches = []
            for aspect in mentioned_aspects[:3]:  # M√°ximo 3 aspectos espec√≠ficos
                specific_searches.append((aspect, f'{subject} {aspect} informaci√≥n detallada'))
            
            # Combinar aspectos espec√≠ficos con universales
            all_aspects = specific_searches + universal_aspects[len(specific_searches):]
        else:
            all_aspects = universal_aspects
        
        return [
            {'category': category, 'query': query}
            for category, query in all_aspects[:5]
        ]

# Funciones p√∫blicas para usar desde unified_web_search_tool.py
def get_intelligent_keywords(query_text: str) -> str:
    """üéØ Funci√≥n principal para generar keywords inteligentes CON VALIDACI√ìN"""
    generator = IntelligentKeywordGenerator()
    return generator.get_intelligent_keywords(query_text)

def get_multiple_search_variants(query_text: str, count: int = 3) -> List[str]:
    """üîÑ Generar m√∫ltiples variantes de b√∫squeda para diversidad"""
    generator = IntelligentKeywordGenerator()
    return generator.get_multiple_search_variants(query_text, count)

def detect_granular_search_needs(query_text: str) -> List[Dict[str, str]]:
    """üéØ Detectar si una consulta necesita b√∫squedas granulares m√∫ltiples"""
    generator = IntelligentKeywordGenerator()
    return generator.detect_granular_search_needs(query_text)

# Testing directo si se ejecuta como script
if __name__ == "__main__":
    # Tests de casos problem√°ticos reportados por el usuario
    test_cases = [
        "Investigar informaci√≥n espec√≠fica sobre Arctic Monkeys",
        "Buscar informaci√≥n sobre 'Javier Milei' en bing y explorar los primeros resultados",
        "realizar an√°lisis de datos espec√≠ficos sobre inteligencia artificial",  
        "genera informe sobre Attack on Titan trama personajes",
        "utilizar herramienta web_search para obtener datos econ√≥micos Argentina"
    ]
    
    print("üß™ TESTING INTELLIGENT KEYWORD GENERATOR CON VALIDACI√ìN")
    print("=" * 70)
    
    for i, test in enumerate(test_cases, 1):
        print(f"\nüîç TEST {i}: {test}")
        result = get_intelligent_keywords(test)
        print(f"‚úÖ FINAL RESULT: {result}")
        print("-" * 50)