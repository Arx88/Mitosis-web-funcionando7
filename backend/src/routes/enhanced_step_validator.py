"""
ENHANCED STEP VALIDATION SYSTEM - CORRECCI√ìN CR√çTICA
Sistema mejorado para asegurar que Paso 1 recolecte informaci√≥n REAL de m√∫ltiples fuentes
"""

import re
import logging
from typing import Dict, List, Any, Set
from datetime import datetime
from urllib.parse import urlparse

logger = logging.getLogger(__name__)

class EnhancedStepValidator:
    """
    üî• VALIDADOR SUPER ESTRICTO PARA PASO 1 
    
    NO PERMITE AVANZAR hasta que se haya recolectado informaci√≥n REAL y SUSTANCIAL
    de M√öLTIPLES sitios web diferentes con contenido verificable
    """
    
    def __init__(self):
        self.required_sources_minimum = 3  # M√≠nimo 3 sitios diferentes
        self.minimum_content_per_source = 300  # M√≠nimo 300 chars por fuente
        self.total_content_minimum = 2000  # M√≠nimo 2000 chars total
        
        # Patrones espec√≠ficos reforzados para informaci√≥n pol√≠tica
        self.critical_patterns = {
            'biografia_personal': {
                'patterns': [
                    r'\bnaci√≥\s+(en|el)', r'\bfecha\s+de\s+nacimiento', r'\bnacimiento',
                    r'\bedad\s+de\s+\d+', r'\ba√±os', r'\bformaci√≥n\s+acad√©mica', 
                    r'\buniversidad', r'\bestudios', r'\bt√≠tulo', r'\bcarrera',
                    r'\bfamilia', r'\bpadres', r'\besposa', r'\bhijos',
                    r'\binfancia', r'\bor√≠genes', r'\bprimer\s+trabajo'
                ],
                'required_indicators': ['datos personales verificables', 'fechas espec√≠ficas', 'formaci√≥n'],
                'weight': 25
            },
            'trayectoria_politica_detallada': {
                'patterns': [
                    r'\bcargo\s+pol√≠tico', r'\bdiputado', r'\bsenador', r'\bministro',
                    r'\bpresidente', r'\bcandidato', r'\belecci√≥n\s+\d{4}',
                    r'\bpartido\s+pol√≠tico', r'\bmovimiento\s+pol√≠tico', r'\bfuerza\s+pol√≠tica',
                    r'\bcampa√±a\s+electoral', r'\bvotaci√≥n', r'\bpropuesta\s+pol√≠tica',
                    r'\bgobierno\s+de', r'\badministraci√≥n\s+de'
                ],
                'required_indicators': ['cargos espec√≠ficos', 'fechas de elecciones', 'partidos pol√≠ticos'],
                'weight': 25
            },
            'ideologia_especifica': {
                'patterns': [
                    r'\bliberal\s+econ√≥mico', r'\bconservador\s+social', r'\blibertario',
                    r'\bderecha\s+pol√≠tica', r'\bcentro\s+derecha', r'\bizquierda',
                    r'\bposici√≥n\s+ideol√≥gica', r'\bprincipios\s+pol√≠ticos', r'\bvisi√≥n\s+econ√≥mica',
                    r'\bpol√≠ticas\s+p√∫blicas', r'\bmodelo\s+econ√≥mico', r'\bestado\s+m√≠nimo'
                ],
                'required_indicators': ['posici√≥n ideol√≥gica clara', 'principios espec√≠ficos'],
                'weight': 20
            },
            'declaraciones_recientes': {
                'patterns': [
                    r'\bdeclar√≥\s+(que|ayer|hoy)', r'\bafirm√≥\s+en', r'\bmanifest√≥\s+que',
                    r'\bentrevista\s+(con|en)', r'\brueda\s+de\s+prensa', r'\bdiscurso\s+en',
                    r'\bconferencia\s+de\s+prensa', r'\bmedio\s+de\s+comunicaci√≥n',
                    r'\bperiodista', r'\bopini√≥n\s+sobre', r'\bpostura\s+ante'
                ],
                'required_indicators': ['declaraciones literales', 'contexto espec√≠fico'],
                'weight': 15
            },
            'cobertura_mediatica': {
                'patterns': [
                    r'\bnoticia\s+(sobre|de)', r'\bmedio\s+public√≥', r'\bdiario\s+[A-Z]',
                    r'\bcanal\s+de\s+televisi√≥n', r'\bperi√≥dico', r'\brevista',
                    r'\bcorrespondencia', r'\breportaje', r'\bcobertura\s+medi√°tica'
                ],
                'required_indicators': ['fuentes period√≠sticas', 'medios espec√≠ficos'],
                'weight': 15
            }
        }
        
        # Patrones que indican meta-contenido (PROHIBIDOS)
        self.forbidden_meta_patterns = [
            r'se\s+(realizar√°|proceder√°|analizar√°|evaluar√°|estudiar√°)',
            r'este\s+(an√°lisis|documento|informe)\s+(se|tiene)',
            r'los\s+objetivos\s+(son|del\s+an√°lisis)',
            r'la\s+metodolog√≠a\s+(ser√°|utilizada)',
            r'el\s+(siguiente\s+paso|proceso\s+de\s+an√°lisis)',
            r'marco\s+(te√≥rico|conceptual|de\s+referencia)',
            r'revisi√≥n\s+bibliogr√°fica',
            r'el\s+presente\s+(estudio|trabajo|documento)',
            r'informaci√≥n\s+general\s+sobre',
            r'datos\s+b√°sicos\s+(de|sobre)',
            r'introducci√≥n\s+al\s+tema'
        ]

    def validate_step_1_completion(self, step_description: str, step_title: str, 
                                   collected_results: List[Dict[str, Any]], 
                                   task_id: str) -> Dict[str, Any]:
        """
        üî• VALIDACI√ìN SUPER ESTRICTA PARA PASO 1
        
        NO permite avanzar hasta que haya informaci√≥n REAL de m√∫ltiples fuentes
        """
        try:
            logger.info(f"üîç INICIANDO VALIDACI√ìN SUPER ESTRICTA PARA PASO 1: {step_title}")
            
            # 1. AN√ÅLISIS DE FUENTES - Verificar m√∫ltiples sitios web
            sources_analysis = self._analyze_multiple_sources(collected_results)
            logger.info(f"üìä Fuentes analizadas: {sources_analysis['unique_sources']} sitios √∫nicos")
            
            # 2. AN√ÅLISIS DE CONTENIDO - Verificar informaci√≥n real vs metadata
            content_analysis = self._analyze_real_content_quality(collected_results)
            logger.info(f"üìù Contenido real: {content_analysis['total_real_content']} caracteres")
            
            # 3. VALIDACI√ìN DE PATRONES CR√çTICOS - Verificar elementos espec√≠ficos
            pattern_validation = self._validate_critical_patterns(collected_results)
            logger.info(f"üéØ Patrones encontrados: {len(pattern_validation['found_patterns'])} de {len(self.critical_patterns)}")
            
            # 4. DETECCI√ìN DE META-CONTENIDO - Rechazar contenido gen√©rico
            meta_content_check = self._detect_forbidden_meta_content(collected_results)
            
            # 5. C√ÅLCULO DE SCORE FINAL
            final_score = self._calculate_comprehensive_score(
                sources_analysis, content_analysis, pattern_validation, meta_content_check
            )
            
            # 6. DECISI√ìN ESTRICTA DE COMPLETITUD
            is_complete = self._make_strict_completion_decision(
                final_score, sources_analysis, content_analysis, pattern_validation, meta_content_check
            )
            
            # 7. GENERAR RECOMENDACIONES ESPEC√çFICAS
            specific_recommendations = self._generate_targeted_search_recommendations(
                pattern_validation, sources_analysis, content_analysis
            )
            
            result = {
                'meets_requirements': is_complete,
                'completeness_score': final_score,
                'sources_analysis': sources_analysis,
                'content_analysis': content_analysis,
                'pattern_validation': pattern_validation,
                'meta_content_detected': meta_content_check['has_meta_content'],
                'validation_summary': self._generate_detailed_summary(
                    is_complete, final_score, sources_analysis, pattern_validation
                ),
                'specific_recommendations': specific_recommendations,
                'strict_requirements_status': {
                    'minimum_sources': sources_analysis['unique_sources'] >= self.required_sources_minimum,
                    'minimum_content': content_analysis['total_real_content'] >= self.total_content_minimum,
                    'no_meta_content': not meta_content_check['has_meta_content'],
                    'critical_patterns_found': len(pattern_validation['found_patterns']) >= 3
                },
                'timestamp': datetime.now().isoformat()
            }
            
            if is_complete:
                logger.info(f"‚úÖ PASO 1 APROBADO - Score: {final_score}% - Fuentes: {sources_analysis['unique_sources']}")
            else:
                logger.warning(f"‚ùå PASO 1 RECHAZADO - Score: {final_score}% - Requisitos no cumplidos")
                logger.warning(f"‚ùå Necesita m√°s informaci√≥n espec√≠fica de m√∫ltiples fuentes")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error en validaci√≥n estricta: {str(e)}")
            return {
                'meets_requirements': False,
                'completeness_score': 0,
                'error': str(e),
                'validation_summary': f'Error en validaci√≥n: {str(e)}'
            }

    def _analyze_multiple_sources(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analiza si los resultados vienen de m√∫ltiples fuentes reales"""
        unique_domains = set()
        valid_sources = []
        
        for result in results:
            url = result.get('url', '')
            if url and url.startswith('http'):
                try:
                    domain = urlparse(url).netloc.lower()
                    if domain and 'bing.com' not in domain:  # Excluir p√°ginas de b√∫squeda
                        unique_domains.add(domain)
                        valid_sources.append({
                            'domain': domain,
                            'url': url,
                            'title': result.get('title', ''),
                            'content_length': len(result.get('snippet', '') or result.get('content', ''))
                        })
                except:
                    pass
        
        return {
            'unique_sources': len(unique_domains),
            'total_results': len(results),
            'valid_sources': valid_sources,
            'domains': list(unique_domains),
            'meets_minimum': len(unique_domains) >= self.required_sources_minimum,
            'sources_with_substantial_content': len([s for s in valid_sources if s['content_length'] > 200])
        }

    def _analyze_real_content_quality(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analiza la calidad del contenido real recolectado"""
        total_content = ""
        content_pieces = []
        
        for result in results:
            content = result.get('snippet', '') or result.get('content', '') or result.get('description', '')
            if content and len(content.strip()) > 50:
                total_content += f" {content}"
                content_pieces.append({
                    'length': len(content),
                    'preview': content[:100] + "..." if len(content) > 100 else content,
                    'source': result.get('url', 'unknown')
                })
        
        # An√°lisis de indicadores de contenido real
        real_data_indicators = [
            r'\d{4}',  # A√±os (2023, 2024, etc)
            r'\d+\s*a√±os',  # Edades
            r'naci√≥\s+en',  # Datos biogr√°ficos
            r'presidente\s+de',  # Cargos espec√≠ficos
            r'declar√≥\s+que',  # Declaraciones
            r'seg√∫n\s+fuentes',  # Referencias
            r'en\s+una\s+entrevista',  # Contexto espec√≠fico
            r'candidato\s+a',  # Posiciones pol√≠ticas
        ]
        
        real_indicators_found = []
        for pattern in real_data_indicators:
            matches = re.findall(pattern, total_content.lower())
            if matches:
                real_indicators_found.extend(matches[:3])  # M√°ximo 3 por patr√≥n
        
        return {
            'total_real_content': len(total_content),
            'content_pieces_count': len(content_pieces),
            'content_pieces': content_pieces[:5],  # Mostrar solo primeros 5
            'real_indicators_found': len(real_indicators_found),
            'real_indicators_examples': real_indicators_found[:10],
            'meets_minimum_content': len(total_content) >= self.total_content_minimum,
            'average_content_per_piece': len(total_content) // max(len(content_pieces), 1)
        }

    def _validate_critical_patterns(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Valida patrones cr√≠ticos espec√≠ficos para informaci√≥n pol√≠tica"""
        all_content = ""
        for result in results:
            content = result.get('snippet', '') or result.get('content', '') or result.get('description', '')
            if content:
                all_content += f" {content}"
        
        all_content_lower = all_content.lower()
        found_patterns = {}
        total_weight = 0
        found_weight = 0
        
        for pattern_name, config in self.critical_patterns.items():
            patterns = config['patterns']
            weight = config['weight']
            total_weight += weight
            
            matches = []
            for pattern in patterns:
                pattern_matches = re.findall(pattern, all_content_lower)
                matches.extend(pattern_matches)
            
            if matches:
                found_patterns[pattern_name] = {
                    'matches_count': len(matches),
                    'evidence': matches[:3],  # Primeros 3 matches como evidencia
                    'weight': weight,
                    'quality': 'high' if len(matches) >= 3 else 'medium' if len(matches) >= 2 else 'low'
                }
                found_weight += weight
            else:
                found_patterns[pattern_name] = {
                    'matches_count': 0,
                    'evidence': [],
                    'weight': weight,
                    'quality': 'none'
                }
        
        pattern_coverage = (found_weight / total_weight * 100) if total_weight > 0 else 0
        
        return {
            'found_patterns': {k: v for k, v in found_patterns.items() if v['matches_count'] > 0},
            'missing_patterns': {k: v for k, v in found_patterns.items() if v['matches_count'] == 0},
            'pattern_coverage_score': int(pattern_coverage),
            'total_patterns_available': len(self.critical_patterns),
            'patterns_found_count': len([p for p in found_patterns.values() if p['matches_count'] > 0]),
            'high_quality_patterns': len([p for p in found_patterns.values() if p['quality'] == 'high'])
        }

    def _detect_forbidden_meta_content(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Detecta contenido meta-gen√©rico que debe ser rechazado"""
        all_content = ""
        for result in results:
            content = result.get('snippet', '') or result.get('content', '') or result.get('description', '')
            if content:
                all_content += f" {content}"
        
        all_content_lower = all_content.lower()
        meta_patterns_found = []
        
        for pattern in self.forbidden_meta_patterns:
            matches = re.findall(pattern, all_content_lower)
            if matches:
                meta_patterns_found.extend([{
                    'pattern': pattern,
                    'matches': matches[:2]  # Primeros 2 matches
                }])
        
        has_meta_content = len(meta_patterns_found) > 0
        
        if has_meta_content:
            logger.warning(f"üö´ META-CONTENIDO DETECTADO: {len(meta_patterns_found)} patrones prohibidos encontrados")
        
        return {
            'has_meta_content': has_meta_content,
            'meta_patterns_found': meta_patterns_found,
            'meta_patterns_count': len(meta_patterns_found)
        }

    def _calculate_comprehensive_score(self, sources_analysis: Dict, content_analysis: Dict, 
                                     pattern_validation: Dict, meta_content_check: Dict) -> int:
        """Calcula score comprehensivo basado en m√∫ltiples factores"""
        score = 0
        
        # 1. Score por fuentes m√∫ltiples (25 puntos)
        sources_score = min(25, (sources_analysis['unique_sources'] / self.required_sources_minimum) * 25)
        score += sources_score
        
        # 2. Score por contenido real (25 puntos)
        content_score = min(25, (content_analysis['total_real_content'] / self.total_content_minimum) * 25)
        score += content_score
        
        # 3. Score por patrones cr√≠ticos encontrados (30 puntos)
        patterns_score = pattern_validation['pattern_coverage_score'] * 0.3
        score += patterns_score
        
        # 4. Score por indicadores de datos reales (20 puntos)
        real_data_score = min(20, content_analysis['real_indicators_found'] * 2)
        score += real_data_score
        
        # 5. Penalizaci√≥n severa por meta-contenido (-50 puntos)
        if meta_content_check['has_meta_content']:
            score -= 50
            logger.warning("üö´ PENALIZACI√ìN APLICADA: -50 puntos por meta-contenido")
        
        return max(0, min(100, int(score)))

    def _make_strict_completion_decision(self, final_score: int, sources_analysis: Dict, 
                                       content_analysis: Dict, pattern_validation: Dict, 
                                       meta_content_check: Dict) -> bool:
        """Decisi√≥n estricta de si el paso est√° realmente completo"""
        
        # Criterios OBLIGATORIOS (todos deben cumplirse)
        mandatory_criteria = {
            'minimum_score': final_score >= 75,
            'minimum_sources': sources_analysis['unique_sources'] >= self.required_sources_minimum,
            'minimum_content': content_analysis['total_real_content'] >= self.total_content_minimum,
            'no_meta_content': not meta_content_check['has_meta_content'],
            'minimum_patterns': pattern_validation['patterns_found_count'] >= 3
        }
        
        # Log de cada criterio
        for criterion, meets in mandatory_criteria.items():
            status = "‚úÖ" if meets else "‚ùå"
            logger.info(f"{status} {criterion}: {meets}")
        
        # TODOS los criterios deben cumplirse
        all_criteria_met = all(mandatory_criteria.values())
        
        if not all_criteria_met:
            failed_criteria = [criterion for criterion, meets in mandatory_criteria.items() if not meets]
            logger.warning(f"‚ùå Criterios no cumplidos: {failed_criteria}")
        
        return all_criteria_met

    def _generate_targeted_search_recommendations(self, pattern_validation: Dict, 
                                                sources_analysis: Dict, 
                                                content_analysis: Dict) -> List[str]:
        """Genera recomendaciones espec√≠ficas basadas en elementos faltantes"""
        recommendations = []
        
        missing_patterns = pattern_validation.get('missing_patterns', {})
        
        for pattern_name, info in missing_patterns.items():
            if pattern_name == 'biografia_personal':
                recommendations.append(
                    "Buscar biograf√≠a espec√≠fica: 'nombre completo fecha nacimiento lugar formaci√≥n acad√©mica universidad carrera familia'"
                )
            elif pattern_name == 'trayectoria_politica_detallada':
                recommendations.append(
                    "Buscar trayectoria pol√≠tica: 'cargos espec√≠ficos diputado senador ministro elecciones a√±os partidos pol√≠ticos gobierno'"
                )
            elif pattern_name == 'ideologia_especifica':
                recommendations.append(
                    "Buscar ideolog√≠a pol√≠tica: 'posici√≥n ideol√≥gica liberal conservador principios pol√≠ticos modelo econ√≥mico estado'"
                )
            elif pattern_name == 'declaraciones_recientes':
                recommendations.append(
                    "Buscar declaraciones: 'entrevistas recientes rueda prensa declaraciones opiniones postura ante'"
                )
            elif pattern_name == 'cobertura_mediatica':
                recommendations.append(
                    "Buscar cobertura medi√°tica: 'noticias diarios medios period√≠sticos reportajes cobertura prensa'"
                )
        
        # Si necesita m√°s fuentes
        if sources_analysis['unique_sources'] < self.required_sources_minimum:
            recommendations.append(
                f"Buscar en {self.required_sources_minimum - sources_analysis['unique_sources']} fuentes adicionales: Wikipedia, biograf√≠as oficiales, medios argentinos espec√≠ficos"
            )
        
        # Si necesita m√°s contenido sustancial
        if content_analysis['total_real_content'] < self.total_content_minimum:
            recommendations.append(
                "Buscar fuentes con informaci√≥n m√°s detallada: biograf√≠as completas, perfiles extensos, an√°lisis en profundidad"
            )
        
        return recommendations[:5]  # M√°ximo 5 recomendaciones

    def _generate_detailed_summary(self, is_complete: bool, final_score: int, 
                                 sources_analysis: Dict, pattern_validation: Dict) -> str:
        """Genera un resumen detallado de la validaci√≥n"""
        if is_complete:
            return (f"‚úÖ PASO 1 COMPLETAMENTE APROBADO - Score: {final_score}% | "
                   f"Fuentes: {sources_analysis['unique_sources']} sitios √∫nicos | "
                   f"Patrones: {pattern_validation['patterns_found_count']} elementos encontrados")
        else:
            return (f"‚ùå PASO 1 RECHAZADO - Score: {final_score}% | "
                   f"Fuentes: {sources_analysis['unique_sources']}/{self.required_sources_minimum} requeridas | "
                   f"Patrones: {pattern_validation['patterns_found_count']} de {len(self.critical_patterns)} elementos | "
                   f"NECESITA M√ÅS INFORMACI√ìN ESPEC√çFICA")

# Funci√≥n de wrapper para integraci√≥n
def validate_step_1_with_enhanced_validator(step_description: str, step_title: str, 
                                          collected_results: List[Dict[str, Any]], 
                                          task_id: str) -> Dict[str, Any]:
    """
    üî• FUNCI√ìN PRINCIPAL DE VALIDACI√ìN MEJORADA PARA PASO 1
    """
    validator = EnhancedStepValidator()
    return validator.validate_step_1_completion(step_description, step_title, collected_results, task_id)