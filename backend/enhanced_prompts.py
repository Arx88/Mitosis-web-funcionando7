"""
Sistema de Gesti√≥n de Prompts Mejorado para el agente Mitosis
Incluye auto-optimizaci√≥n y plantillas adaptativas
"""

import logging
import json
import time
import hashlib
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import re

from enhanced_prompts import PromptType

@dataclass
class PromptPerformance:
    """M√©tricas de rendimiento de un prompt"""
    prompt_id: str
    usage_count: int = 0
    success_count: int = 0
    average_response_time: float = 0.0
    average_quality_score: float = 0.0
    last_used: float = 0.0
    optimization_count: int = 0

class PromptOptimizationStrategy(Enum):
    """Estrategias de optimizaci√≥n de prompts"""
    CLARITY = "clarity"
    SPECIFICITY = "specificity"
    CONTEXT = "context"
    STRUCTURE = "structure"
    EXAMPLES = "examples"

class EnhancedPromptManager:
    """Gestor de prompts mejorado con capacidades de auto-optimizaci√≥n"""
    
    def __init__(self, memory_manager, task_manager):
        self.memory_manager = memory_manager
        self.task_manager = task_manager
        self.logger = logging.getLogger(__name__)
        
        # Configuraci√≥n
        self.max_context_tokens = 4000
        self.include_memory_context = True
        self.auto_optimization_enabled = True
        self.optimization_threshold = 0.6  # Umbral de √©xito para optimizaci√≥n
        
        # Almacenamiento de prompts y rendimiento
        self.prompt_templates: Dict[str, str] = {}
        self.prompt_performance: Dict[str, PromptPerformance] = {}
        self.optimization_history: List[Dict[str, Any]] = []
        
        # Patrones de optimizaci√≥n
        self.optimization_patterns = {
            PromptOptimizationStrategy.CLARITY: [
                "S√© m√°s espec√≠fico y claro en las instrucciones",
                "Usa lenguaje directo y evita ambig√ºedades",
                "Define t√©rminos t√©cnicos cuando sea necesario"
            ],
            PromptOptimizationStrategy.SPECIFICITY: [
                "Proporciona ejemplos concretos",
                "Especifica el formato de salida deseado",
                "Incluye criterios de √©xito espec√≠ficos"
            ],
            PromptOptimizationStrategy.CONTEXT: [
                "A√±ade m√°s contexto relevante",
                "Incluye informaci√≥n de fondo necesaria",
                "Relaciona con experiencias previas"
            ],
            PromptOptimizationStrategy.STRUCTURE: [
                "Organiza las instrucciones en pasos numerados",
                "Usa vi√±etas para listar requisitos",
                "Separa claramente las secciones"
            ],
            PromptOptimizationStrategy.EXAMPLES: [
                "Incluye ejemplos de entrada y salida",
                "Muestra casos de uso espec√≠ficos",
                "Proporciona contraejemplos cuando sea √∫til"
            ]
        }
        
        # Inicializar plantillas base
        self._initialize_base_templates()
        
        self.logger.info("Enhanced Prompt Manager inicializado")
    
    def _initialize_base_templates(self):
        """Inicializa plantillas base de prompts"""
        base_templates = {
            "system_prompt": """Eres un agente inteligente llamado Mitosis, dise√±ado para ayudar a los usuarios de manera eficiente y precisa.

Caracter√≠sticas principales:
- Anal√≠tico y reflexivo
- Capaz de aprender de la experiencia
- Orientado a resultados
- Transparente en tu proceso de pensamiento

Contexto actual: {context}
Modo cognitivo: {cognitive_mode}

Responde de manera √∫til, precisa y adaptada al contexto del usuario.""",

            "task_planning": """Como agente especializado en planificaci√≥n, necesito crear un plan detallado para la siguiente tarea:

**Objetivo:** {goal}
**Descripci√≥n:** {description}
**Contexto disponible:** {context}
**Recursos:** {resources}

Crea un plan estructurado que incluya:
1. **An√°lisis del objetivo** - Descomp√≥n el objetivo en componentes clave
2. **Fases de ejecuci√≥n** - Lista las fases en orden l√≥gico
3. **Dependencias** - Identifica qu√© fases dependen de otras
4. **Herramientas necesarias** - Especifica qu√© herramientas se requieren
5. **Criterios de √©xito** - Define c√≥mo medir el √©xito de cada fase
6. **Estimaci√≥n de tiempo** - Proporciona estimaciones realistas

Formato de respuesta en JSON:
```json
{
  "goal": "objetivo reformulado",
  "phases": [
    {
      "id": 1,
      "title": "nombre de la fase",
      "description": "descripci√≥n detallada",
      "required_capabilities": ["capacidad1", "capacidad2"],
      "dependencies": [],
      "estimated_duration": "tiempo estimado",
      "success_criteria": "criterios espec√≠ficos"
    }
  ]
}
```""",

            "phase_execution": """Ejecutando la fase actual de la tarea:

**Tarea:** {task_title}
**Fase actual:** {phase_title}
**Descripci√≥n:** {phase_description}
**Capacidades requeridas:** {required_capabilities}
**Contexto de la tarea:** {task_context}

**Progreso previo:**
{previous_results}

**Instrucciones espec√≠ficas:**
1. Analiza los requisitos de esta fase
2. Utiliza las capacidades disponibles de manera eficiente
3. Documenta tu proceso de pensamiento
4. Proporciona resultados claros y verificables
5. Identifica cualquier problema o limitaci√≥n

Ejecuta esta fase paso a paso y proporciona un resultado detallado.""",

            "reflection_prompt": """Reflexiona sobre la siguiente acci√≥n y su resultado:

**Acci√≥n realizada:** {action_taken}
**Resultado obtenido:** {result}
**Resultado esperado:** {expected_outcome}
**Contexto de la tarea:** {task_context}

**An√°lisis requerido:**
1. **Evaluaci√≥n de √©xito:** ¬øSe logr√≥ el objetivo? (S√≠/No y por qu√©)
2. **An√°lisis de proceso:** ¬øQu√© funcion√≥ bien y qu√© no?
3. **Patrones identificados:** ¬øQu√© patrones puedes observar?
4. **Lecciones aprendidas:** ¬øQu√© insights espec√≠ficos obtuviste?
5. **Mejoras futuras:** ¬øC√≥mo abordar√≠as esto diferente la pr√≥xima vez?

Proporciona una reflexi√≥n honesta y constructiva que ayude al aprendizaje futuro.""",

            "error_handling": """Se ha producido un error que requiere an√°lisis y estrategia de recuperaci√≥n:

**Error:** {error_message}
**Acci√≥n que fall√≥:** {failed_action}
**Contexto:** {context}
**Informaci√≥n adicional:** {additional_info}

**An√°lisis requerido:**
1. **Diagn√≥stico:** ¬øCu√°l es la causa ra√≠z del error?
2. **Impacto:** ¬øQu√© tan cr√≠tico es este error?
3. **Opciones de recuperaci√≥n:** ¬øQu√© alternativas existen?
4. **Estrategia recomendada:** ¬øCu√°l es el mejor curso de acci√≥n?
5. **Prevenci√≥n futura:** ¬øC√≥mo evitar este error en el futuro?

Proporciona una estrategia de recuperaci√≥n clara y accionable."""
        }
        
        for template_id, template in base_templates.items():
            self.prompt_templates[template_id] = template
            self.prompt_performance[template_id] = PromptPerformance(prompt_id=template_id)
    
    def generate_system_prompt(self, context: str = "", cognitive_mode: str = "adaptive") -> str:
        """Genera un prompt del sistema optimizado"""
        template = self.prompt_templates.get("system_prompt", "")
        
        # Aplicar optimizaciones si est√°n disponibles
        optimized_template = self._apply_optimizations("system_prompt", template)
        
        return optimized_template.format(
            context=context,
            cognitive_mode=cognitive_mode
        )
    
    def generate_task_planning_prompt(self, goal: str, description: str = "", 
                                    context: str = "", resources: str = "") -> str:
        """Genera un prompt optimizado para planificaci√≥n de tareas"""
        template = self.prompt_templates.get("task_planning", "")
        optimized_template = self._apply_optimizations("task_planning", template)
        
        return optimized_template.format(
            goal=goal,
            description=description,
            context=context,
            resources=resources
        )
    
    def generate_phase_execution_prompt(self, task, phase) -> str:
        """Genera un prompt optimizado para ejecuci√≥n de fases"""
        template = self.prompt_templates.get("phase_execution", "")
        optimized_template = self._apply_optimizations("phase_execution", template)
        
        # Obtener resultados previos
        previous_results = ""
        for prev_phase in task.phases:
            if prev_phase.id < phase.id and prev_phase.results:
                previous_results += f"Fase {prev_phase.id}: {prev_phase.results}\n"
        
        return optimized_template.format(
            task_title=task.title,
            phase_title=phase.title,
            phase_description=phase.description,
            required_capabilities=", ".join(phase.required_capabilities),
            task_context=json.dumps(task.context, indent=2),
            previous_results=previous_results or "No hay resultados previos"
        )
    
    def generate_reflection_prompt(self, action_taken: str, result: str, 
                                 expected_outcome: str, task_context: str = "") -> str:
        """Genera un prompt optimizado para reflexi√≥n"""
        template = self.prompt_templates.get("reflection_prompt", "")
        optimized_template = self._apply_optimizations("reflection_prompt", template)
        
        return optimized_template.format(
            action_taken=action_taken,
            result=result,
            expected_outcome=expected_outcome,
            task_context=task_context
        )
    
    def generate_error_handling_prompt(self, error_message: str, failed_action: str,
                                     context: str = "", additional_info: str = "") -> str:
        """Genera un prompt optimizado para manejo de errores"""
        template = self.prompt_templates.get("error_handling", "")
        optimized_template = self._apply_optimizations("error_handling", template)
        
        return optimized_template.format(
            error_message=error_message,
            failed_action=failed_action,
            context=context,
            additional_info=additional_info
        )
    
    def _apply_optimizations(self, prompt_id: str, template: str) -> str:
        """Aplica optimizaciones a una plantilla de prompt"""
        if not self.auto_optimization_enabled:
            return template
        
        performance = self.prompt_performance.get(prompt_id)
        if not performance:
            return template
        
        # Si el rendimiento es bajo, aplicar optimizaciones
        if (performance.usage_count > 5 and 
            performance.success_count / performance.usage_count < self.optimization_threshold):
            
            return self._optimize_prompt_template(prompt_id, template)
        
        return template
    
    def _optimize_prompt_template(self, prompt_id: str, template: str) -> str:
        """Optimiza una plantilla de prompt"""
        performance = self.prompt_performance[prompt_id]
        
        # Determinar estrategia de optimizaci√≥n basada en el rendimiento
        if performance.average_quality_score < 0.5:
            strategy = PromptOptimizationStrategy.CLARITY
        elif performance.average_response_time > 10.0:
            strategy = PromptOptimizationStrategy.SPECIFICITY
        else:
            strategy = PromptOptimizationStrategy.STRUCTURE
        
        # Aplicar optimizaci√≥n
        optimized_template = self._apply_optimization_strategy(template, strategy)
        
        # Registrar optimizaci√≥n
        optimization_record = {
            "prompt_id": prompt_id,
            "strategy": strategy.value,
            "timestamp": time.time(),
            "original_performance": asdict(performance),
            "optimization_count": performance.optimization_count + 1
        }
        
        self.optimization_history.append(optimization_record)
        performance.optimization_count += 1
        
        self.logger.info(f"Prompt {prompt_id} optimizado usando estrategia: {strategy.value}")
        
        return optimized_template
    
    def _apply_optimization_strategy(self, template: str, 
                                   strategy: PromptOptimizationStrategy) -> str:
        """Aplica una estrategia espec√≠fica de optimizaci√≥n"""
        patterns = self.optimization_patterns[strategy]
        
        if strategy == PromptOptimizationStrategy.CLARITY:
            # Hacer el prompt m√°s claro
            template = re.sub(r'(?i)\b(puede|podr√≠a|tal vez)\b', 'debe', template)
            template = template.replace('si es posible', 'espec√≠ficamente')
            
        elif strategy == PromptOptimizationStrategy.SPECIFICITY:
            # A√±adir m√°s especificidad
            if "Proporciona" in template and "espec√≠fico" not in template:
                template = template.replace("Proporciona", "Proporciona espec√≠ficamente")
            
        elif strategy == PromptOptimizationStrategy.STRUCTURE:
            # Mejorar estructura
            if "1." not in template and "Instrucciones" in template:
                # A√±adir numeraci√≥n si no existe
                lines = template.split('\n')
                for i, line in enumerate(lines):
                    if line.strip().startswith('-'):
                        lines[i] = line.replace('-', f"{i}.")
                template = '\n'.join(lines)
        
        elif strategy == PromptOptimizationStrategy.CONTEXT:
            # A√±adir m√°s contexto
            if "{context}" not in template:
                template = f"Contexto adicional: {{context}}\n\n{template}"
        
        elif strategy == PromptOptimizationStrategy.EXAMPLES:
            # A√±adir ejemplos
            if "Ejemplo:" not in template and "ejemplo" not in template.lower():
                template += "\n\nEjemplo de respuesta esperada: [Proporciona un ejemplo relevante]"
        
        return template
    
    def record_prompt_performance(self, prompt_id: str, success: bool, 
                                response_time: float = 0.0, quality_score: float = 0.5):
        """Registra el rendimiento de un prompt"""
        if prompt_id not in self.prompt_performance:
            self.prompt_performance[prompt_id] = PromptPerformance(prompt_id=prompt_id)
        
        performance = self.prompt_performance[prompt_id]
        performance.usage_count += 1
        performance.last_used = time.time()
        
        if success:
            performance.success_count += 1
        
        # Actualizar promedio de tiempo de respuesta
        if response_time > 0:
            if performance.usage_count == 1:
                performance.average_response_time = response_time
            else:
                alpha = 0.1  # Factor de aprendizaje
                performance.average_response_time = (
                    (1 - alpha) * performance.average_response_time + alpha * response_time
                )
        
        # Actualizar puntuaci√≥n de calidad
        if performance.usage_count == 1:
            performance.average_quality_score = quality_score
        else:
            alpha = 0.1
            performance.average_quality_score = (
                (1 - alpha) * performance.average_quality_score + alpha * quality_score
            )
    
    def get_prompt_analytics(self) -> Dict[str, Any]:
        """Obtiene anal√≠ticas de los prompts"""
        analytics = {
            "total_prompts": len(self.prompt_templates),
            "total_optimizations": len(self.optimization_history),
            "prompt_performance": {}
        }
        
        for prompt_id, performance in self.prompt_performance.items():
            if performance.usage_count > 0:
                success_rate = performance.success_count / performance.usage_count
                analytics["prompt_performance"][prompt_id] = {
                    "usage_count": performance.usage_count,
                    "success_rate": success_rate,
                    "average_response_time": performance.average_response_time,
                    "average_quality_score": performance.average_quality_score,
                    "optimization_count": performance.optimization_count
                }
        
        return analytics
    
    def export_optimized_prompts(self) -> Dict[str, str]:
        """Exporta las plantillas de prompts optimizadas"""
        return self.prompt_templates.copy()
    
    def import_prompt_templates(self, templates: Dict[str, str]):
        """Importa plantillas de prompts"""
        for template_id, template in templates.items():
            self.prompt_templates[template_id] = template
            if template_id not in self.prompt_performance:
                self.prompt_performance[template_id] = PromptPerformance(prompt_id=template_id)
        
        self.logger.info(f"Importadas {len(templates)} plantillas de prompts")

# Ejemplo de uso
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # Crear gestor de prompts mejorado (mock de dependencias)
    class MockMemoryManager:
        pass
    
    class MockTaskManager:
        pass
    
    enhanced_prompt_manager = EnhancedPromptManager(
        MockMemoryManager(), MockTaskManager()
    )
    
    print("üìù Probando Enhanced Prompt Manager...")
    
    # Generar prompt del sistema
    system_prompt = enhanced_prompt_manager.generate_system_prompt(
        context="Usuario trabajando en an√°lisis de datos",
        cognitive_mode="analytical"
    )
    print(f"ü§ñ System prompt generado: {len(system_prompt)} caracteres")
    
    # Generar prompt de planificaci√≥n
    planning_prompt = enhanced_prompt_manager.generate_task_planning_prompt(
        goal="Crear un dashboard de ventas",
        description="Dashboard interactivo para visualizar m√©tricas de ventas",
        context="Empresa de e-commerce con datos en SQL",
        resources="Python, Plotly, base de datos SQL"
    )
    print(f"üìã Planning prompt generado: {len(planning_prompt)} caracteres")
    
    # Simular uso y rendimiento
    enhanced_prompt_manager.record_prompt_performance(
        "task_planning", success=True, response_time=5.2, quality_score=0.8
    )
    enhanced_prompt_manager.record_prompt_performance(
        "task_planning", success=False, response_time=8.1, quality_score=0.4
    )
    
    # Obtener anal√≠ticas
    analytics = enhanced_prompt_manager.get_prompt_analytics()
    print(f"üìä Anal√≠ticas de prompts:")
    print(f"  Total de prompts: {analytics['total_prompts']}")
    print(f"  Optimizaciones realizadas: {analytics['total_optimizations']}")
    
    for prompt_id, perf in analytics["prompt_performance"].items():
        print(f"  {prompt_id}: {perf['success_rate']:.2f} √©xito, {perf['usage_count']} usos")
    
    print("‚úÖ Pruebas completadas")

