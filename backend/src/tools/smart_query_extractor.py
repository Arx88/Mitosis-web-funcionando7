"""
üß† EXTRACTOR INTELIGENTE DE QUERIES PARA B√öSQUEDA WEB
Convierte t√≠tulos y descripciones de pasos en queries de b√∫squeda inteligentes y espec√≠ficos

SOLUCIONA: El problema donde el agente busca "BUSCAR INFORMACI√ìN" en lugar de keywords relevantes
"""

import re
from typing import List, Set
from datetime import datetime

def extract_smart_search_query(title: str, description: str, original_message: str = "") -> str:
    """
    üéØ EXTRACTOR PRINCIPAL - Convierte pasos en queries de b√∫squeda inteligentes
    
    Args:
        title: T√≠tulo del paso (ej: "Buscar informaci√≥n sobre la selecci√≥n argentina 2025")
        description: Descripci√≥n detallada del paso
        original_message: Mensaje original del usuario para contexto adicional
        
    Returns:
        Query optimizado para b√∫squeda web (ej: "selecci√≥n argentina futbol 2025 jugadores")
    """
    
    # Combinar todo el texto disponible
    full_text = f"{title} {description} {original_message}".strip()
    
    # PASO 1: Limpiar texto de instrucciones gen√©ricas
    cleaned_text = _remove_generic_instructions(full_text)
    
    # PASO 2: Extraer entidades importantes (nombres, lugares, fechas)
    entities = _extract_named_entities(cleaned_text)
    
    # PASO 3: Extraer keywords tem√°ticos espec√≠ficos
    thematic_keywords = _extract_thematic_keywords(cleaned_text)
    
    # PASO 4: Combinar y priorizar t√©rminos
    final_query = _build_optimized_query(entities, thematic_keywords, cleaned_text)
    
    # PASO 5: Validar y ajustar longitud
    final_query = _validate_and_optimize_length(final_query)
    
    return final_query

def _remove_generic_instructions(text: str) -> str:
    """üßπ Remover frases gen√©ricas de instrucciones"""
    
    # Patrones a eliminar
    generic_patterns = [
        r'buscar informaci√≥n sobre\s*',
        r'investigar sobre\s*',
        r'analizar\s*',
        r'obtener datos sobre\s*',
        r'recopilar informaci√≥n de\s*',
        r'utilizar.*herramienta.*para\s*',
        r'web_search para\s*',
        r'b√∫squeda.*web.*sobre\s*',
        r'informaci√≥n actualizada sobre\s*',
        r'informaci√≥n espec√≠fica sobre\s*',
        r'datos espec√≠ficos de\s*',
        r'realizar.*b√∫squeda.*sobre\s*',
        r'encontrar informaci√≥n sobre\s*',
        r'conseguir datos de\s*'
    ]
    
    cleaned = text.lower()
    
    for pattern in generic_patterns:
        cleaned = re.sub(pattern, ' ', cleaned, flags=re.IGNORECASE)
    
    # Limpiar espacios m√∫ltiples
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned

def _extract_named_entities(text: str) -> Set[str]:
    """üè∑Ô∏è Extraer entidades importantes (nombres, lugares, organizaciones)"""
    
    entities = set()
    
    # PA√çSES Y NACIONALIDADES
    countries_and_nationalities = [
        'argentina', 'argentino', 'argentinos', 'argentina', 
        'brasil', 'brasile√±o', 'brazil',
        'espa√±a', 'espa√±ol', 'espa√±oles', 'spain',
        'italia', 'italiano', 'italy',
        'francia', 'franc√©s', 'france',
        'alemania', 'alem√°n', 'germany',
        'inglaterra', 'ingl√©s', 'england',
        'portugal', 'portugu√©s',
        'chile', 'chileno',
        'uruguay', 'uruguayo',
        'colombia', 'colombiano',
        'm√©xico', 'mexicano', 'mexico',
        'per√∫', 'peruano',
        'ecuador', 'ecuatoriano'
    ]
    
    for country in countries_and_nationalities:
        if country in text.lower():
            entities.add(country)
    
    # DEPORTES Y T√âRMINOS DEPORTIVOS
    sports_terms = [
        'f√∫tbol', 'futbol', 'soccer', 'football',
        'selecci√≥n', 'seleccion', 'nacional', 'equipo',
        'mundial', 'copa', 'am√©rica', 'champions',
        'liga', 'torneo', 'campeonato',
        'jugador', 'jugadores', 'player', 'players',
        'entrenador', 't√©cnico', 'coach',
        'estadio', 'cancha'
    ]
    
    for term in sports_terms:
        if term in text.lower():
            entities.add(term)
    
    # A√ëOS Y FECHAS
    years = re.findall(r'\b(20\d{2})\b', text)
    for year in years:
        entities.add(year)
    
    # NOMBRES PROPIOS (empiezan con may√∫scula)
    # Buscar secuencias de palabras que empiezan con may√∫scula
    proper_nouns = re.findall(r'\b[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+)*\b', text)
    for noun in proper_nouns:
        if len(noun) > 3 and not noun.lower().startswith(('buscar', 'informaci√≥n', 'sobre')):
            entities.add(noun.lower())
    
    # T√âRMINOS T√âCNICOS ESPEC√çFICOS
    technical_terms = [
        'inteligencia artificial', 'ia', 'ai', 'machine learning',
        'blockchain', 'bitcoin', 'ethereum', 'crypto',
        'covid', 'coronavirus', 'pandemic', 'vaccine',
        'climate', 'clima', 'environment', 'sostenible',
        'economy', 'econom√≠a', 'inflation', 'inflaci√≥n',
        'politics', 'pol√≠tica', 'election', 'elecci√≥n',
        'technology', 'tecnolog√≠a', 'innovation', 'innovaci√≥n'
    ]
    
    for term in technical_terms:
        if term in text.lower():
            entities.add(term)
    
    return entities

def _extract_thematic_keywords(text: str) -> Set[str]:
    """üéØ Extraer keywords tem√°ticos espec√≠ficos del dominio"""
    
    keywords = set()
    text_lower = text.lower()
    
    # PATRONES TEM√ÅTICOS ESPEC√çFICOS
    thematic_patterns = {
        # Deportes
        'deportes': ['gol', 'goles', 'partido', 'partidos', 'resultado', 'resultados',
                    'clasificaci√≥n', 'tabla', 'posiciones', 'fixture',
                    'transferencia', 'fichaje', 'lesi√≥n', 'suspendido'],
        
        # Pol√≠tica y gobierno
        'politica': ['gobierno', 'presidente', 'ministro', 'congreso', 'senado',
                    'diputado', 'ley', 'decreto', 'reforma', 'pol√≠tica',
                    'elecci√≥n', 'candidato', 'partido pol√≠tico'],
        
        # Econom√≠a
        'economia': ['precio', 'precios', 'inflaci√≥n', 'd√≥lar', 'peso',
                    'mercado', 'inversi√≥n', 'empresa', 'negocio',
                    'trabajo', 'empleo', 'salario', 'sueldo'],
        
        # Tecnolog√≠a
        'tecnologia': ['software', 'aplicaci√≥n', 'app', 'sistema',
                      'internet', 'digital', 'online', 'plataforma',
                      'algoritmo', 'datos', 'base de datos'],
        
        # Salud
        'salud': ['m√©dico', 'hospital', 'tratamiento', 'medicina',
                 's√≠ntoma', 'enfermedad', 'vacuna', 'virus'],
        
        # Educaci√≥n
        'educacion': ['universidad', 'estudiante', 'profesor', 'curso',
                     'carrera', 't√≠tulo', 'educaci√≥n', 'aprendizaje']
    }
    
    # Buscar keywords tem√°ticos en el texto
    for theme, theme_keywords in thematic_patterns.items():
        for keyword in theme_keywords:
            if keyword in text_lower:
                keywords.add(keyword)
    
    # BUSCAR VERBOS DE ACCI√ìN RELEVANTES
    action_verbs = ['comprar', 'vender', 'analizar', 'comparar', 'estudiar', 
                   'desarrollar', 'implementar', 'mejorar', 'optimizar',
                   'crear', 'dise√±ar', 'construir', 'establecer']
    
    for verb in action_verbs:
        if verb in text_lower:
            keywords.add(verb)
    
    return keywords

def _build_optimized_query(entities: Set[str], keywords: Set[str], original_text: str) -> str:
    """üî® Construir query optimizado combinando entidades y keywords"""
    
    # Combinar todos los t√©rminos
    all_terms = list(entities) + list(keywords)
    
    # Eliminar stop words
    stop_words = {
        'de', 'del', 'la', 'el', 'en', 'con', 'para', 'por', 'sobre',
        'un', 'una', 'y', 'o', 'que', 'se', 'es', 'son', 'est√°', 'est√°n',
        'fue', 'ser', 'han', 'ha', 'su', 'sus', 'le', 'les', 'lo', 'los',
        'las', 'al', 'pero', 'como', 'm√°s', 'muy', 'puede', 'pueden',
        'desde', 'hasta', 'entre', 'durante', 'antes', 'despu√©s',
        'informaci√≥n', 'datos', 'buscar', 'obtener', 'encontrar'
    }
    
    filtered_terms = []
    for term in all_terms:
        if term and len(term) > 2 and term.lower() not in stop_words:
            filtered_terms.append(term)
    
    # Priorizar t√©rminos m√°s espec√≠ficos
    prioritized_terms = []
    
    # PRIORIDAD 1: Nombres propios y entidades espec√≠ficas
    for term in filtered_terms:
        if any(char.isupper() for char in term) or re.match(r'20\d{2}', term):
            prioritized_terms.append(term)
    
    # PRIORIDAD 2: T√©rminos tem√°ticos relevantes
    for term in filtered_terms:
        if term not in prioritized_terms and len(term) > 4:
            prioritized_terms.append(term)
    
    # PRIORIDAD 3: T√©rminos generales pero √∫tiles
    for term in filtered_terms:
        if term not in prioritized_terms:
            prioritized_terms.append(term)
    
    # Construir query final
    if prioritized_terms:
        # Tomar los t√©rminos m√°s importantes (m√°ximo 6)
        final_terms = prioritized_terms[:6]
        return ' '.join(final_terms)
    else:
        # Fallback: extraer palabras significativas del texto original
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√±A-Z√Å√â√ç√ì√ö√ë]{4,}\b', original_text)
        significant_words = [w for w in words if w.lower() not in stop_words][:4]
        return ' '.join(significant_words) if significant_words else "noticias actualidad 2025"

def _validate_and_optimize_length(query: str) -> str:
    """‚úÖ Validar y optimizar la longitud del query final"""
    
    # Limpiar espacios m√∫ltiples
    query = re.sub(r'\s+', ' ', query).strip()
    
    # Si el query es muy corto, agregar contexto temporal
    if len(query) < 10:
        current_year = datetime.now().year
        query = f"{query} {current_year}"
    
    # Si es muy largo, tomar solo las primeras palabras m√°s importantes
    words = query.split()
    if len(words) > 6:
        query = ' '.join(words[:6])
    
    # Si est√° vac√≠o o muy gen√©rico, usar fallback
    generic_terms = ['informaci√≥n', 'datos', 'buscar', 'sobre']
    if not query or all(term in generic_terms for term in query.split()):
        query = f"noticias actualidad {datetime.now().year}"
    
    return query

# FUNCI√ìN DE UTILIDAD PARA TESTING
def test_query_extraction():
    """üß™ Funci√≥n de testing para verificar la extracci√≥n de queries"""
    
    test_cases = [
        {
            'title': 'Buscar informaci√≥n sobre la selecci√≥n argentina 2025',
            'description': 'Necesito datos actualizados sobre los jugadores convocados',
            'expected_keywords': ['selecci√≥n', 'argentina', '2025', 'jugadores']
        },
        {
            'title': 'Investigar sobre inteligencia artificial',
            'description': 'Obtener informaci√≥n sobre los √∫ltimos avances en IA',
            'expected_keywords': ['inteligencia', 'artificial', 'ia', 'avances']
        },
        {
            'title': 'Analizar la inflaci√≥n en Argentina 2024',
            'description': 'Datos econ√≥micos sobre el impacto de la inflaci√≥n',
            'expected_keywords': ['inflaci√≥n', 'argentina', '2024', 'econ√≥micos']
        }
    ]
    
    print("üß™ TESTING SMART QUERY EXTRACTION")
    print("=" * 50)
    
    for i, test_case in enumerate(test_cases, 1):
        title = test_case['title']
        description = test_case['description']
        
        # Extraer query
        smart_query = extract_smart_search_query(title, description)
        
        print(f"\nTest {i}:")
        print(f"  Input Title: {title}")
        print(f"  Input Description: {description}")
        print(f"  Smart Query: '{smart_query}'")
        print(f"  Expected Keywords: {test_case['expected_keywords']}")
        
        # Verificar si contiene keywords esperados
        found_keywords = []
        for keyword in test_case['expected_keywords']:
            if keyword.lower() in smart_query.lower():
                found_keywords.append(keyword)
        
        print(f"  Found Keywords: {found_keywords}")
        print(f"  Match Score: {len(found_keywords)}/{len(test_case['expected_keywords'])}")

if __name__ == "__main__":
    test_query_extraction()