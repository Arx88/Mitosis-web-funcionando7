"""
Herramientas de Ollama para an치lisis y procesamiento inteligente
Estas herramientas utilizan Ollama para generar contenido relevante
en lugar de hacer b칰squedas web irrelevantes.
"""

import logging
import sys
import os
from typing import Dict, Any, List

# Add the backend src directory to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from tools.base_tool import BaseTool, ParameterDefinition, ToolExecutionResult, register_tool
from services.ollama_service import OllamaService

logger = logging.getLogger(__name__)

@register_tool
class OllamaAnalysisTool(BaseTool):
    """
    Herramienta de an치lisis inteligente usando Ollama
    Genera an치lisis detallados basados en datos previos y contexto
    """
    
    def __init__(self):
        super().__init__(
            name="ollama_analysis",
            description="Realiza an치lisis inteligentes usando Ollama basado en contexto previo"
        )
    
    def _define_parameters(self) -> List[ParameterDefinition]:
        """Definir par치metros espec칤ficos del an치lisis Ollama"""
        return [
            ParameterDefinition(
                name="prompt",
                param_type="string",
                required=True,
                description="Prompt para el an치lisis inteligente",
                min_value=10  # M칤nimo 10 caracteres
            ),
            ParameterDefinition(
                name="max_tokens",
                param_type="integer",
                required=False,
                description="M치ximo n칰mero de tokens para la respuesta",
                default=1000,
                min_value=100,
                max_value=4000
            )
        ]
    
    def _execute_tool(self, parameters: Dict[str, Any], config: Dict[str, Any]) -> ToolExecutionResult:
        """
        Ejecutar an치lisis usando Ollama
        """
        try:
            prompt = parameters.get('prompt', '')
            max_tokens = parameters.get('max_tokens', 1000)
            
            # Crear instancia de OllamaService
            ollama_service = OllamaService()
            
            # Configurar contexto para generar respuesta
            context = {
                'system_prompt': "Eres un asistente experto en an치lisis. Proporciona an치lisis detallados, estructurados y 칰tiles basados en la informaci칩n proporcionada.",
                'max_tokens': max_tokens,
                'temperature': 0.7
            }
            
            # Generar respuesta usando Ollama
            logger.info(f"游 Iniciando an치lisis con Ollama - Prompt: {prompt[:100]}...")
            
            response = ollama_service.generate_response(
                prompt=prompt,
                context=context,
                use_tools=False,
                task_id=self.current_task_id or "analysis",
                step_id="analysis_step"
            )
            
            if response and 'response' in response:
                analysis_content = response['response']
                
                result_data = {
                    'type': 'analysis',
                    'content': analysis_content,
                    'summary': f"An치lisis completado: {len(analysis_content)} caracteres generados",
                    'tool_used': 'ollama_analysis',
                    'analysis_result': analysis_content,
                    'prompt_length': len(prompt),
                    'response_length': len(analysis_content)
                }
                
                return self._create_success_result(result_data)
            else:
                return self._create_error_result(f'No se pudo generar an치lisis con Ollama: {response}')
                
        except Exception as e:
            logger.error(f"Error en an치lisis Ollama: {e}")
            return self._create_error_result(f'Error interno en an치lisis: {str(e)}')