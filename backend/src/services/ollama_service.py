"""
Servicio de integraci√≥n con Ollama - Versi√≥n con Cola de Concurrencia
Conecta directamente con Ollama para generar respuestas, usando un sistema
de cola para gestionar llamadas concurrentes y evitar saturaci√≥n.

üîÑ NUEVA FUNCIONALIDAD: Sistema de Cola de Concurrencia
- Integraci√≥n transparente con OllamaQueueManager
- Control autom√°tico de llamadas concurrentes
- Priorizaci√≥n inteligente de requests
- M√©tricas de rendimiento y monitoreo
"""

import json
import time
import os
import logging
import asyncio
import concurrent.futures
import threading
from typing import Dict, List, Optional, Any
import requests
from requests.exceptions import RequestException, Timeout

# Importar configuraci√≥n centralizada
from ..config.ollama_config import get_ollama_config, get_ollama_endpoint, get_ollama_model

# üö¶ IMPORTACI√ìN DEL GESTOR DE COLA
from .ollama_queue_manager import (
    OllamaQueueManager, 
    OllamaRequest, 
    RequestPriority,
    get_ollama_queue_manager
)

class OllamaService:
    def __init__(self, base_url: str = None):
        # Usar configuraci√≥n centralizada
        self.ollama_config = get_ollama_config()
        self.base_url = base_url or self.ollama_config.endpoint
        self.default_model = self.ollama_config.model
        self.current_model = None
        self.conversation_history = []
        self.request_timeout = self.ollama_config.timeout
        
        # üÜï PROBLEMA 3: Configuraci√≥n de par√°metros por modelo
        self.model_configs = self._load_model_configs()
        
        # üö¶ INTEGRACI√ìN CON GESTOR DE COLA
        self.use_queue = os.getenv('OLLAMA_USE_QUEUE', 'true').lower() == 'true'
        self._queue_manager = None  # Se inicializar√° lazy
        
        # Logging espec√≠fico para cola
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        if self.use_queue:
            self.logger.info("üö¶ OllamaService configurado con sistema de cola activado")
        else:
            self.logger.warning("‚ö†Ô∏è OllamaService configurado SIN sistema de cola - pueden ocurrir problemas de concurrencia")
    
    def _get_queue_manager(self) -> Optional[OllamaQueueManager]:
        """
        üö¶ OBTENER GESTOR DE COLA (LAZY LOADING)
        
        Inicializa el gestor de cola solo cuando es necesario
        """
        if not self.use_queue:
            return None
            
        if self._queue_manager is None:
            try:
                self._queue_manager = get_ollama_queue_manager()
                self.logger.info("üö¶ Gestor de cola Ollama obtenido exitosamente")
            except Exception as e:
                self.logger.error(f"‚ùå Error obteniendo gestor de cola: {str(e)}")
                return None
                
        return self._queue_manager
    
    async def _execute_with_queue(self, 
                                 prompt: str, 
                                 model: str, 
                                 options: Dict[str, Any],
                                 priority: RequestPriority = RequestPriority.NORMAL,
                                 task_id: str = "",
                                 step_id: str = "") -> Dict[str, Any]:
        """
        üö¶ EJECUTAR LLAMADA A OLLAMA A TRAV√âS DE LA COLA
        
        Encapsula la llamada a Ollama dentro del sistema de cola para
        controlar la concurrencia autom√°ticamente.
        
        Args:
            prompt: Prompt para Ollama
            model: Modelo a utilizar
            options: Opciones de generaci√≥n
            priority: Prioridad del request
            task_id: ID de la tarea (para tracking)
            step_id: ID del paso (para tracking)
            
        Returns:
            Resultado de Ollama o error
        """
        queue_manager = self._get_queue_manager()
        if not queue_manager:
            self.logger.warning("‚ö†Ô∏è Gestor de cola no disponible, ejecutando llamada directa")
            return await self._execute_direct_call(prompt, model, options)
        
        # Crear request para la cola
        ollama_request = OllamaRequest(
            task_id=task_id,
            step_id=step_id,
            prompt=prompt,
            model=model,
            options=options,
            priority=priority,
            timeout=self._get_model_config(model).get("request_timeout", 180)
        )
        
        self.logger.info(f"üö¶ Encolando request para modelo {model} (tarea: {task_id}, prioridad: {priority.name})")
        
        # Funci√≥n callback que ejecuta la llamada real
        async def execution_callback(request: OllamaRequest) -> Dict[str, Any]:
            return await self._execute_direct_call(request.prompt, request.model, request.options)
        
        # Ejecutar a trav√©s de la cola
        try:
            result = await queue_manager.enqueue_request(ollama_request, execution_callback)
            
            # Log resultado
            if 'error' in result:
                self.logger.error(f"‚ùå Request {ollama_request.request_id} fall√≥: {result.get('error')}")
            else:
                self.logger.info(f"‚úÖ Request {ollama_request.request_id} completado exitosamente")
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå Error en cola de Ollama: {str(e)}")
            return {'error': str(e), 'error_type': 'queue_system_error'}
    
    async def _execute_direct_call(self, 
                                  prompt: str, 
                                  model: str, 
                                  options: Dict[str, Any]) -> Dict[str, Any]:
        """
        üîß EJECUTAR LLAMADA DIRECTA A OLLAMA (SIN COLA)
        
        Versi√≥n async de _call_ollama_api para uso dentro del sistema de cola.
        Mantiene la misma l√≥gica pero adaptada para async/await.
        """
        try:
            # Usar la l√≥gica existente pero adaptada para async
            loop = asyncio.get_event_loop()
            
            # Ejecutar la llamada en un thread pool para no bloquear
            result = await loop.run_in_executor(
                None, 
                self._call_ollama_api_sync, 
                prompt, 
                model, 
                options
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå Error en llamada directa a Ollama: {str(e)}")
            return {'error': str(e), 'error_type': 'direct_call_error'}
    
    def _call_ollama_api_sync(self, prompt: str, model: str, options: Dict[str, Any]) -> Dict[str, Any]:
        """
        üîß VERSI√ìN SINCR√ìNICA DE LA LLAMADA A OLLAMA
        
        Esta es la implementaci√≥n original adaptada para ser llamada desde async
        """
        try:
            model_config = self._get_model_config(model)
            request_timeout = model_config.get("request_timeout", self.request_timeout)
            
            # Detectar si es una solicitud JSON y ajustar par√°metros espec√≠ficamente
            is_json_request = any(keyword in prompt.lower() for keyword in ['json', '"steps"', 'genera un plan', 'plan de acci√≥n'])
            
            final_options = options.copy()
            if is_json_request:
                # Para solicitudes JSON, usar par√°metros m√°s estrictos
                final_options['temperature'] = min(final_options.get('temperature', 0.7) * 0.5, 0.1)
                final_options['top_p'] = min(final_options.get('top_p', 0.9) * 0.8, 0.7)
                
                # Agregar stops espec√≠ficos para JSON si no est√°n
                current_stops = final_options.get('stop', [])
                json_stops = ['```', '---', '}```', '}\n```']
                final_options['stop'] = list(set(current_stops + json_stops))
            
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": final_options
            }
            
            # Logging detallado para debug
            self.logger.debug(f"ü§ñ Ollama Request - Model: {model}")
            self.logger.debug(f"‚öôÔ∏è Options: temp={final_options.get('temperature')}, timeout={request_timeout}s")
            if is_json_request:
                self.logger.debug(f"üìã JSON mode detected, using strict parameters")
            
            # Headers necesarios para ngrok
            headers = {
                'Content-Type': 'application/json'
            }
            # Agregar header ngrok si el endpoint es ngrok
            if 'ngrok' in self.base_url:
                headers['ngrok-skip-browser-warning'] = 'true'
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                headers=headers,
                timeout=min(request_timeout, 180)  # M√°ximo 3 minutos para evitar cuelgues
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"‚ùå Ollama API returned error for model {model}: HTTP {response.status_code}")
                return {
                    'error': f"HTTP {response.status_code}: {response.text}"
                }
                
        except Timeout:
            self.logger.error(f"‚è±Ô∏è Ollama API request timed out after {request_timeout} seconds for model {model}.")
            return {
                'error': f"Timeout despu√©s de {request_timeout} segundos para el modelo {model}. El modelo puede necesitar m√°s tiempo para respuestas complejas."
            }
        except RequestException as e:
            self.logger.error(f"üîå Connection error to Ollama API for model {model}: {str(e)}")
            return {
                'error': f"Error de conexi√≥n: {str(e)}"
            }
        except Exception as e:
            self.logger.error(f"üí• Unexpected error in Ollama API call for model {model}: {str(e)}")
            return {
                'error': f"Error inesperado: {str(e)}"
            }
    
    def update_endpoint(self, new_endpoint: str) -> bool:
        """
        Actualiza el endpoint de Ollama din√°micamente
        """
        try:
            old_endpoint = self.base_url
            self.base_url = new_endpoint
            
            # Verificar que el nuevo endpoint funciona
            if self.is_healthy():
                logging.getLogger(__name__).info(f"‚úÖ Ollama endpoint updated: {old_endpoint} ‚Üí {new_endpoint}")
                return True
            else:
                # Revertir si no funciona
                self.base_url = old_endpoint
                logging.getLogger(__name__).error(f"‚ùå Failed to update Ollama endpoint to {new_endpoint}, reverted to {old_endpoint}")
                return False
                
        except Exception as e:
            logging.getLogger(__name__).error(f"‚ùå Error updating Ollama endpoint: {str(e)}")
            return False
    
    def get_endpoint_info(self) -> dict:
        """Obtiene informaci√≥n completa del endpoint actual"""
        try:
            return {
                'endpoint': self.base_url,
                'current_model': self.get_current_model(),
                'is_healthy': self.is_healthy(),
                'available_models': self.get_available_models(),
                'connection_info': self.check_connection()
            }
        except Exception as e:
            return {
                'endpoint': self.base_url,
                'error': str(e),
                'is_healthy': False
            }
        
    def _load_model_configs(self) -> dict:
        """Carga las configuraciones de los modelos desde un archivo o define valores por defecto."""
        config_path = os.getenv("OLLAMA_MODEL_CONFIGS_PATH", "/app/backend/config/ollama_model_configs.json")
        
        try:
            if os.path.exists(config_path):
                with open(config_path, "r", encoding="utf-8") as f:
                    configs = json.load(f)
                    
                    # Validar estructura b√°sica de las configuraciones cargadas
                    for model, params in configs.items():
                        if model.startswith('_'):  # Ignorar metadatos
                            continue
                        if not isinstance(params, dict) or "options" not in params:
                            logging.getLogger(__name__).warning(f"‚ö†Ô∏è Configuraci√≥n inv√°lida para el modelo {model}. Usando valores por defecto.")
                            configs[model] = self._get_default_model_config()
                    
                    logging.getLogger(__name__).info(f"‚úÖ Configuraciones de modelos Ollama cargadas desde {config_path}")
                    return configs
            else:
                logging.getLogger(__name__).info(f"‚ÑπÔ∏è Archivo de configuraci√≥n no encontrado en {config_path}. Usando valores por defecto.")
                return self._get_default_model_configs()
                
        except Exception as e:
            logging.getLogger(__name__).error(f"‚ùå Error al cargar configuraciones de modelos desde {config_path}: {e}. Usando valores por defecto.")
            return self._get_default_model_configs()
    
    def _get_default_model_configs(self) -> dict:
        """Define las configuraciones por defecto para los modelos conocidos."""
        return {
            "llama3.1:8b": {
                "options": {
                    "temperature": 0.15,  # M√°s creativo que 0.1 pero preciso
                    "top_p": 0.7,         # Reduce probabilidad de palabras menos probables
                    "top_k": 20,          # Limita muestreo a las 20 palabras m√°s probables
                    "repeat_penalty": 1.1,# Penaliza repetici√≥n de tokens
                    "stop": ["```", "---", "<|eot_id|>", "<|end_of_text|>"]  # Tokens de parada espec√≠ficos
                },
                "request_timeout": 180    # 3 minutos para Llama3.1:8b
            },
            "qwen3:32b": {
                "options": {
                    "temperature": 0.1,   # Muy bajo para m√°xima precisi√≥n
                    "top_p": 0.6,         # M√°s restrictivo para evitar divagaciones
                    "top_k": 15,          # Muestreo muy limitado
                    "repeat_penalty": 1.05,# Penalizaci√≥n ligera
                    "stop": ["```json", "</tool_code>", "<|im_end|>", "<|endoftext|>"]  # Tokens espec√≠ficos Qwen
                },
                "request_timeout": 480    # 8 minutos para Qwen3:32b (modelo grande)
            },
            "deepseek-r1:32b": {
                "options": {
                    "temperature": 0.12,
                    "top_p": 0.65,
                    "top_k": 18,
                    "repeat_penalty": 1.08,
                    "stop": ["```", "---", "<|endofthought|>", "<|end|>"]
                },
                "request_timeout": 420    # 7 minutos para DeepSeek R1
            },
            "default": {
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 40,
                    "repeat_penalty": 1.1,
                    "stop": []
                },
                "request_timeout": 120    # 2 minutos por defecto
            }
        }
    
    def _get_model_config(self, model_name: str) -> dict:
        """Obtiene la configuraci√≥n para un modelo dado, con fallback a la configuraci√≥n por defecto."""
        if model_name in self.model_configs:
            return self.model_configs[model_name]
        else:
            logging.getLogger(__name__).warning(f"‚ö†Ô∏è No hay configuraci√≥n espec√≠fica para el modelo '{model_name}', usando configuraci√≥n por defecto.")
            return self.model_configs.get("default", self.model_configs["default"])
    
    def get_model_info(self, model_name: str = None) -> dict:
        """
        Obtiene informaci√≥n detallada sobre la configuraci√≥n de un modelo.
        
        Args:
            model_name: Nombre del modelo (si no se especifica, usa el modelo actual)
        
        Returns:
            Dict con informaci√≥n de configuraci√≥n del modelo
        """
        target_model = model_name or self.get_current_model()
        config = self._get_model_config(target_model)
        
        return {
            'model_name': target_model,
            'config': config,
            'is_optimized': target_model in self.model_configs and target_model != 'default',
            'timeout': config.get('request_timeout', 120),
            'temperature': config.get('options', {}).get('temperature', 0.7),
            'description': config.get('description', 'Sin descripci√≥n disponible')
        }
        
        
    def is_healthy(self) -> bool:
        """Verificar si Ollama est√° disponible"""
        try:
            headers = {}
            if 'ngrok' in self.base_url:
                headers['ngrok-skip-browser-warning'] = 'true'
            
            response = requests.get(f"{self.base_url}/api/tags", timeout=5, headers=headers)
            return response.status_code == 200
        except:
            return False
    
    def is_available(self) -> bool:
        """Verificar si Ollama est√° disponible y no est√° ocupado procesando otra tarea"""
        if not self.is_healthy():
            return False
        
        # Verificar si hay requests activos en el queue manager
        queue_manager = self._get_queue_manager()
        if queue_manager:
            try:
                # Si hay tareas en proceso o en cola, no est√° disponible
                active_requests = queue_manager.get_active_requests_count()
                queued_requests = queue_manager.get_queue_size()
                
                self.logger.info(f"üö¶ Ollama availability check: {active_requests} active, {queued_requests} queued")
                
                # Disponible solo si no hay tareas activas y pocas en cola
                return active_requests == 0 and queued_requests <= 1
                
            except Exception as e:
                self.logger.error(f"‚ùå Error checking Ollama availability: {e}")
                # En caso de error, asumir no disponible por seguridad
                return False
        else:
            # Sin gestor de cola, usar check simple
            return True
    
    def check_connection(self) -> Dict[str, Any]:
        """Verificar conexi√≥n con Ollama y retornar informaci√≥n detallada"""
        try:
            headers = {}
            if 'ngrok' in self.base_url:
                headers['ngrok-skip-browser-warning'] = 'true'
            
            response = requests.get(f"{self.base_url}/api/tags", timeout=5, headers=headers)
            if response.status_code == 200:
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                return {
                    'status': 'connected',
                    'url': self.base_url,
                    'models_available': len(models),
                    'current_model': self.current_model or self.default_model,
                    'healthy': True
                }
        except Exception as e:
            return {
                'status': 'error',
                'url': self.base_url,
                'error': str(e),
                'healthy': False
            }
        
        return {
            'status': 'disconnected',
            'url': self.base_url,
            'healthy': False
        }
    
    def get_available_models(self) -> List[str]:
        """Obtener lista de modelos disponibles desde Ollama"""
        try:
            headers = {}
            if 'ngrok' in self.base_url:
                headers['ngrok-skip-browser-warning'] = 'true'
            
            response = requests.get(f"{self.base_url}/api/tags", timeout=5, headers=headers)
            if response.status_code == 200:
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                return models
        except:
            pass
        
        # Fallback a modelos conocidos si no se puede conectar
        return [
            "llama3.2",
            "llama3.1", 
            "mistral",
            "codellama",
            "phi3"
        ]
    
    def set_model(self, model_name: str) -> bool:
        """Establecer el modelo a usar - FORZAR sin validaci√≥n de disponibilidad"""
        # üöÄ FIX CR√çTICO: Permitir cambio de modelo sin validar disponibilidad
        # porque el frontend puede enviar modelos v√°lidos que no aparecen en la lista
        self.current_model = model_name
        logger = logging.getLogger(__name__)
        logger.info(f"üîÑ Modelo forzado a: {model_name}")
        return True
    
    def set_model_with_validation(self, model_name: str) -> bool:
        """Establecer el modelo a usar CON validaci√≥n de disponibilidad"""
        available_models = self.get_available_models()
        if model_name in available_models:
            self.current_model = model_name
            return True
        return False
    
    def get_current_model(self) -> str:
        """Obtener el modelo actual"""
        return self.current_model or self.default_model
    
    def generate_casual_response(self, prompt: str, context: Dict = None) -> Dict[str, Any]:
        """
        üîÑ GENERAR RESPUESTA CASUAL CON COLA OPCIONAL
        
        Genera respuesta casual usando Ollama (sin planes ni herramientas),
        utilizando el sistema de cola si est√° habilitado.
        
        Args:
            prompt: Mensaje del usuario
            context: Contexto adicional (historial, etc.)
        
        Returns:
            Dict con respuesta casual y metadatos
        """
        if not self.is_healthy():
            return {
                'response': "‚ö†Ô∏è Ollama no est√° disponible en este momento. Verifica la configuraci√≥n del endpoint de Ollama.",
                'tool_calls': [],
                'raw_response': "",
                'model': self.get_current_model(),
                'timestamp': time.time(),
                'error': 'Ollama no disponible'
            }
        
        try:
            # Construir el prompt con system prompt para conversaci√≥n casual
            system_prompt = self._build_system_prompt(use_tools=False, conversation_mode=True)
            full_prompt = self._build_full_prompt(prompt, context, system_prompt)
            
            # Determinar si usar cola o llamada directa
            if self.use_queue:
                # Detectar si estamos en eventlet y usar llamada directa como fallback
                try:
                    import threading
                    current_thread = threading.current_thread()
                    if hasattr(current_thread, 'name') and 'GreenThread' in current_thread.name:
                        # En eventlet, usar llamada directa
                        self.logger.warning("‚ö†Ô∏è Detectado GreenThread, usando llamada directa")
                        response = self._call_ollama_api(full_prompt)
                    else:
                        # Usar cola con prioridad normal para conversaciones casuales
                        def run_async():
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            try:
                                return new_loop.run_until_complete(self._execute_with_queue(
                                    prompt=full_prompt,
                                    model=self.get_current_model(),
                                    options=self._get_model_config(self.get_current_model()).get("options", {}),
                                    priority=RequestPriority.LOW,
                                    task_id="casual_conversation",
                                    step_id="casual_response"
                                ))
                            finally:
                                new_loop.close()
                        
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(run_async)
                            response = future.result(timeout=30)
                except Exception as e:
                    self.logger.error(f"‚ùå Error en execute con cola: {e}")
                    # Fallback a llamada directa
                    response = self._call_ollama_api(full_prompt)
            else:
                # Llamada directa tradicional
                response = self._call_ollama_api(full_prompt)
            
            if response.get('error'):
                return {
                    'response': f"‚ùå Error al generar respuesta: {response['error']}",
                    'tool_calls': [],
                    'raw_response': "",
                    'model': self.get_current_model(),
                    'timestamp': time.time(),
                    'error': response['error']
                }
            
            # Para conversaci√≥n casual, no parseamos tool calls, solo devolvemos el texto
            response_text = response.get('response', '').strip()
            
            return {
                'response': response_text,
                'tool_calls': [],
                'raw_response': response.get('response', ''),
                'model': self.get_current_model(),
                'timestamp': time.time(),
                'used_queue': self.use_queue
            }
            
        except Exception as e:
            return {
                'response': f"‚ùå Error interno: {str(e)}",
                'tool_calls': [],
                'raw_response': "",
                'model': self.get_current_model(),
                'timestamp': time.time(),
                'error': str(e)
            }

    def generate_response(self, prompt: str, context: Dict = None, use_tools: bool = True, task_id: str = "", step_id: str = "") -> Dict[str, Any]:
        """
        üîÑ GENERAR RESPUESTA CON COLA Y PRIORIZACI√ìN INTELIGENTE
        
        Genera respuesta usando Ollama, con control autom√°tico de cola
        y priorizaci√≥n basada en el tipo de contenido.
        
        Args:
            prompt: Mensaje del usuario
            context: Contexto adicional (historial, herramientas, etc.)
            use_tools: Si debe considerar el uso de herramientas
            task_id: ID de la tarea (para tracking y priorizaci√≥n)
            step_id: ID del paso (para tracking)
        
        Returns:
            Dict con respuesta, tool_calls, y metadatos incluyendo info de cola
        """
        if not self.is_healthy():
            return {
                'response': "‚ö†Ô∏è Ollama no est√° disponible en este momento. Verifica la configuraci√≥n del endpoint de Ollama.",
                'tool_calls': [],
                'raw_response': "",
                'model': self.get_current_model(),
                'timestamp': time.time(),
                'error': 'Ollama no disponible'
            }
        
        try:
            # Construir el prompt completo
            system_prompt = self._build_system_prompt(use_tools, conversation_mode=False)
            full_prompt = self._build_full_prompt(prompt, context, system_prompt)
            
            # üîç DETERMINAR PRIORIDAD AUTOM√ÅTICAMENTE
            priority = self._determine_request_priority(prompt, context, task_id, step_id)
            
            # Determinar si usar cola o llamada directa
            if self.use_queue:
                # Detectar si estamos en eventlet y usar llamada directa como fallback
                try:
                    import threading
                    current_thread = threading.current_thread()
                    if hasattr(current_thread, 'name') and 'GreenThread' in current_thread.name:
                        # En eventlet, usar llamada directa
                        self.logger.warning("‚ö†Ô∏è Detectado GreenThread, usando llamada directa")
                        response = self._call_ollama_api(full_prompt)
                    else:
                        # Usar cola con prioridad determinada autom√°ticamente
                        def run_async():
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            try:
                                return new_loop.run_until_complete(self._execute_with_queue(
                                    prompt=full_prompt,
                                    model=self.get_current_model(),
                                    options=self._get_model_config(self.get_current_model()).get("options", {}),
                                    priority=priority,
                                    task_id=task_id or "unknown_task",
                                    step_id=step_id or "unknown_step"
                                ))
                            finally:
                                new_loop.close()
                        
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(run_async)
                            response = future.result(timeout=60)
                except Exception as e:
                    self.logger.error(f"‚ùå Error en execute con cola: {e}")
                    # Fallback a llamada directa
                    response = self._call_ollama_api(full_prompt)
                
                self.logger.info(f"üö¶ Request procesado a trav√©s de cola (prioridad: {priority.name})")
                
            else:
                # Llamada directa tradicional (sin cola)
                response = self._call_ollama_api(full_prompt)
                self.logger.warning("‚ö†Ô∏è Request procesado SIN cola - riesgo de problemas de concurrencia")
            
            if response.get('error'):
                return {
                    'response': f"‚ùå Error al generar respuesta: {response['error']}",
                    'tool_calls': [],
                    'raw_response': "",
                    'model': self.get_current_model(),
                    'timestamp': time.time(),
                    'error': response['error'],
                    'used_queue': self.use_queue,
                    'priority': priority.name if self.use_queue else 'none'
                }
            
            # Parsear la respuesta
            parsed_response = self._parse_response(response.get('response', ''))
            
            return {
                'response': parsed_response['text'],
                'tool_calls': parsed_response['tool_calls'],
                'raw_response': response.get('response', ''),
                'model': self.get_current_model(),
                'timestamp': time.time(),
                'used_queue': self.use_queue,
                'priority': priority.name if self.use_queue else 'none'
            }
            
        except Exception as e:
            return {
                'response': f"‚ùå Error interno: {str(e)}",
                'tool_calls': [],
                'raw_response': "",
                'model': self.get_current_model(),
                'timestamp': time.time(),
                'error': str(e),
                'used_queue': self.use_queue
            }
    
    def _determine_request_priority(self, prompt: str, context: Dict, task_id: str, step_id: str) -> RequestPriority:
        """
        üîç DETERMINAR PRIORIDAD DE REQUEST AUTOM√ÅTICAMENTE
        
        Analiza el contenido del prompt y contexto para asignar
        la prioridad apropiada al request.
        
        Args:
            prompt: Contenido del prompt
            context: Contexto adicional
            task_id: ID de la tarea
            step_id: ID del paso
            
        Returns:
            Prioridad apropiada para el request
        """
        prompt_lower = prompt.lower()
        
        # üö® CR√çTICO: Generaci√≥n de planes iniciales
        if any(keyword in prompt_lower for keyword in ['genera un plan', 'plan de acci√≥n', 'create_dynamic_plan']):
            return RequestPriority.CRITICAL
        
        # üî¥ ALTA: Recuperaci√≥n de errores o reintentos
        if any(keyword in prompt_lower for keyword in ['error', 'fall√≥', 'reintento', 'retry', 'recuperaci√≥n']):
            return RequestPriority.HIGH
        
        # üî¥ ALTA: An√°lisis o decisiones importantes
        if any(keyword in prompt_lower for keyword in ['analizar', 'decidir', 'evaluar', 'importante', 'cr√≠tico']):
            return RequestPriority.HIGH
        
        # üü† NORMAL: Ejecuci√≥n de pasos de plan
        if step_id and step_id != "unknown_step":
            return RequestPriority.NORMAL
        
        # üü° BAJA: Conversaciones casuales o consultas generales
        if any(keyword in prompt_lower for keyword in ['hola', 'saludo', 'informaci√≥n', 'explicar']):
            return RequestPriority.LOW
        
        # Por defecto: NORMAL
        return RequestPriority.NORMAL
    
    def _call_ollama_api(self, prompt: str, custom_options: Optional[Dict] = None) -> Dict[str, Any]:
        """
        üîß HACER LLAMADA A API DE OLLAMA (COMPATIBILIDAD)
        
        M√©todo de compatibilidad que redirige a la implementaci√≥n
        sincr√≥nica para mantener el c√≥digo existente funcionando.
        
        NOTA: Este m√©todo se mantiene para compatibilidad, pero se
        recomienda usar generate_response() que incluye el sistema de cola.
        """
        try:
            current_model_name = self.get_current_model()
            model_config = self._get_model_config(current_model_name)
            
            # Obtener opciones base del modelo
            model_options = model_config.get("options", {}).copy()
            
            # Fusionar con opciones personalizadas si se proporcionan
            if custom_options:
                model_options.update(custom_options)
            
            # Usar la implementaci√≥n sincr√≥nica
            return self._call_ollama_api_sync(prompt, current_model_name, model_options)
            
        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.error(f"üí• Error en _call_ollama_api: {str(e)}")
            return {
                'error': f"Error inesperado: {str(e)}"
            }
    
    def _parse_response(self, response_text: str) -> Dict[str, Any]:
        """
        Parsear respuesta para extraer texto y tool calls con estrategias robustas
        Mejora implementada seg√∫n UPGRADE.md Secci√≥n 4: Servicio Ollama y Extracci√≥n de Query
        """
        import re
        import json
        import logging
        
        logger = logging.getLogger(__name__)
        
        if not response_text or not isinstance(response_text, str):
            return {'text': '', 'tool_calls': []}
        
        tool_calls = []
        clean_text = response_text
        
        # Estrategia 1: Buscar bloques JSON cl√°sicos con ``` 
        json_pattern_1 = r'```json\s*(\{.*?\})\s*```'
        matches_1 = re.findall(json_pattern_1, response_text, re.DOTALL)
        
        for match in matches_1:
            try:
                data = json.loads(match)
                if 'tool_call' in data:
                    tool_calls.append(data['tool_call'])
                    # Remover el bloque JSON del texto
                    clean_text = clean_text.replace(f'```json\n{match}\n```', '')
                    logger.debug(f"‚úÖ JSON parsing strategy 1 successful: {match[:50]}...")
            except json.JSONDecodeError as e:
                logger.debug(f"‚ö†Ô∏è JSON parsing strategy 1 failed for match: {str(e)}")
                continue
        
        # Estrategia 2: Buscar JSON sin marcadores de bloque
        if not tool_calls:
            json_pattern_2 = r'\{[^{}]*"tool_call"[^{}]*\{[^{}]*\}[^{}]*\}'
            matches_2 = re.findall(json_pattern_2, response_text)
            
            for match in matches_2:
                try:
                    data = json.loads(match)
                    if 'tool_call' in data:
                        tool_calls.append(data['tool_call'])
                        clean_text = clean_text.replace(match, '')
                        logger.debug(f"‚úÖ JSON parsing strategy 2 successful: {match[:50]}...")
                except json.JSONDecodeError as e:
                    logger.debug(f"‚ö†Ô∏è JSON parsing strategy 2 failed: {str(e)}")
                    continue
        
        # Estrategia 3: Buscar cualquier JSON v√°lido y verificar si contiene tool_call
        if not tool_calls:
            json_pattern_3 = r'\{[^}]*\}'
            potential_jsons = re.findall(json_pattern_3, response_text)
            
            for potential_json in potential_jsons:
                try:
                    # Intentar corregir JSON mal formateado
                    corrected_json = potential_json.replace("'", '"')  # Comillas simples por dobles
                    data = json.loads(corrected_json)
                    
                    if isinstance(data, dict) and 'tool_call' in data:
                        tool_calls.append(data['tool_call'])
                        clean_text = clean_text.replace(potential_json, '')
                        logger.debug(f"‚úÖ JSON parsing strategy 3 successful: {potential_json[:50]}...")
                        break
                except (json.JSONDecodeError, ValueError) as e:
                    logger.debug(f"‚ö†Ô∏è JSON parsing strategy 3 failed for '{potential_json[:30]}...': {str(e)}")
                    continue
        
        # Estrategia 4: Extracci√≥n por regex espec√≠fico de tool_call
        if not tool_calls:
            try:
                tool_pattern = r'"tool_call"\s*:\s*\{[^}]*"tool"\s*:\s*"([^"]+)"[^}]*"parameters"\s*:\s*\{[^}]*\}'
                tool_matches = re.finditer(tool_pattern, response_text)
                
                for tool_match in tool_matches:
                    try:
                        # Intentar construir tool_call b√°sico desde regex
                        full_match = tool_match.group()
                        tool_name_match = re.search(r'"tool"\s*:\s*"([^"]+)"', full_match)
                        params_match = re.search(r'"parameters"\s*:\s*(\{[^}]*\})', full_match)
                        
                        if tool_name_match:
                            tool_call = {
                                "tool": tool_name_match.group(1),
                                "parameters": json.loads(params_match.group(1)) if params_match else {}
                            }
                            tool_calls.append(tool_call)
                            clean_text = clean_text.replace(tool_match.group(), '')
                            logger.debug(f"‚úÖ JSON parsing strategy 4 successful for tool: {tool_call['tool']}")
                            
                    except (json.JSONDecodeError, AttributeError) as e:
                        logger.debug(f"‚ö†Ô∏è JSON parsing strategy 4 failed for tool extraction: {str(e)}")
                        continue
                        
            except Exception as e:
                logger.debug(f"‚ö†Ô∏è JSON parsing strategy 4 overall failed: {str(e)}")
        
        # Limpiar texto final
        clean_text = re.sub(r'```\w*\n?', '', clean_text)  # Remover marcadores de c√≥digo
        clean_text = re.sub(r'\n\s*\n', '\n', clean_text)  # Remover l√≠neas vac√≠as m√∫ltiples
        clean_text = clean_text.strip()
        
        if tool_calls:
            logger.info(f"üîß Successfully extracted {len(tool_calls)} tool calls from Ollama response")
        
        return {
            'text': clean_text,
            'tool_calls': tool_calls
        }
    
    def _build_system_prompt(self, use_tools: bool, conversation_mode: bool = False) -> str:
        """Construir prompt del sistema"""
        
        # Sistema prompt para conversaci√≥n casual (sin planes ni herramientas)
        if conversation_mode:
            return """Eres un asistente de IA general llamado 'Agente General' que puede ayudar con una amplia variedad de tareas. 
Eres inteligente, √∫til y amigable.

IMPORTANTE: Est√°s en modo conversaci√≥n casual. Responde de manera natural y amigable sin generar planes de acci√≥n ni mencionar herramientas.

NUNCA generes planes de acci√≥n en este modo.
NUNCA menciones herramientas disponibles.
NUNCA uses formatos estructurados como "**PLAN DE ACCI√ìN:**"

Para conversaciones casuales:
- Saludos: Responde amigablemente y pregunta c√≥mo puedes ayudar
- Preguntas sobre ti: Explica que eres un asistente de IA general
- Traducciones simples: Proporciona la traducci√≥n directamente
- Explicaciones: Da respuestas claras y concisas
- Preguntas de conocimiento: Responde con la informaci√≥n que tienes

Mant√©n las respuestas naturales, conversacionales y √∫tiles.
Responde en espa√±ol de manera clara y amigable."""
        
        # Sistema prompt para tareas (con planes y herramientas)
        base_prompt = """Eres un asistente de IA general llamado 'Agente General' que puede ayudar con una amplia variedad de tareas. 
Eres inteligente, √∫til y puedes usar herramientas para realizar acciones concretas.

IMPORTANTE: Est√°s en modo agente. Para tareas complejas, genera un PLAN DE ACCI√ìN ESPEC√çFICO y DETALLADO paso a paso.

El plan debe ser:
1. ESPEC√çFICO para la tarea solicitada (no gen√©rico)
2. DETALLADO con pasos concretos
3. ESTRUCTURADO en orden l√≥gico
4. PR√ÅCTICO y realizable

Formato del plan:
**PLAN DE ACCI√ìN:**
1. [Paso espec√≠fico 1]
2. [Paso espec√≠fico 2]
3. [Paso espec√≠fico 3]
4. [Paso espec√≠fico 4]
5. [Paso espec√≠fico 5]

Despu√©s del plan, explica brevemente qu√© vas a hacer y qu√© herramientas utilizar√°s.

Responde en espa√±ol de manera clara y concisa."""
        
        if use_tools:
            tools_prompt = """
HERRAMIENTAS DISPONIBLES:
Para usar una herramienta, incluye en tu respuesta un bloque JSON con el siguiente formato:
```json
{
  "tool_call": {
    "tool": "nombre_herramienta",
    "parameters": {
      "parametro": "valor"
    }
  }
}
```

Herramientas disponibles:
1. **web_search** - Buscar informaci√≥n en internet
2. **analysis** - Realizar an√°lisis de datos e informaci√≥n
3. **creation** - Crear contenido, documentos o c√≥digo
4. **planning** - Planificaci√≥n y organizaci√≥n de tareas
5. **delivery** - Entrega y presentaci√≥n de resultados
6. **processing** - Procesamiento general de informaci√≥n
7. **synthesis** - S√≠ntesis y resumen de informaci√≥n
8. **research** - Investigaci√≥n detallada
9. **investigation** - Investigaci√≥n espec√≠fica
10. **shell** - Ejecutar comandos de terminal
11. **search_definition** - B√∫squeda de definiciones
12. **data_analysis** - An√°lisis espec√≠fico de datos
"""
            return base_prompt + tools_prompt
        
        return base_prompt
    
    def _build_full_prompt(self, prompt: str, context: Dict, system_prompt: str) -> str:
        """Construir prompt completo con contexto"""
        full_prompt = f"{system_prompt}\n\n"
        
        # A√±adir contexto si est√° disponible
        if context:
            if 'task_id' in context:
                full_prompt += f"ID de tarea: {context['task_id']}\n"
            
            if 'previous_messages' in context and context['previous_messages']:
                full_prompt += "Conversaci√≥n anterior:\n"
                for msg in context['previous_messages'][-3:]:  # √öltimos 3 mensajes
                    sender = "Usuario" if msg.get('sender') == 'user' else "Asistente"
                    full_prompt += f"{sender}: {msg.get('content', '')}\n"
                full_prompt += "\n"
        
        full_prompt += f"Usuario: {prompt}\nAsistente: "
        
        return full_prompt
    
    def chat_streaming(self, prompt: str, context: Dict = None, use_tools: bool = True):
        """
        Generar respuesta streaming usando Ollama
        Devuelve un generator que produce chunks de respuesta
        """
        if not self.is_healthy():
            yield {
                'response': "‚ö†Ô∏è Ollama no est√° disponible en este momento.",
                'done': True,
                'error': 'Ollama no disponible'
            }
            return
        
        try:
            system_prompt = self._build_system_prompt(use_tools, conversation_mode=False)
            full_prompt = self._build_full_prompt(prompt, context, system_prompt)
            
            payload = {
                "model": self.get_current_model(),
                "prompt": full_prompt,
                "stream": True,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 40
                }
            }
            
            # Headers necesarios para ngrok
            headers = {
                'Content-Type': 'application/json'
            }
            if 'ngrok' in self.base_url:
                headers['ngrok-skip-browser-warning'] = 'true'
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                headers=headers,
                timeout=self.request_timeout,
                stream=True
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        try:
                            chunk = json.loads(line.decode('utf-8'))
                            yield chunk
                        except json.JSONDecodeError:
                            continue
            else:
                yield {
                    'response': f"‚ùå Error HTTP {response.status_code}: {response.text}",
                    'done': True,
                    'error': f"HTTP {response.status_code}"
                }
                
        except Exception as e:
            yield {
                'response': f"‚ùå Error: {str(e)}",
                'done': True,
                'error': str(e)
            }