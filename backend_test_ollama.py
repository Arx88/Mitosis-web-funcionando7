#!/usr/bin/env python3
"""
BACKEND TESTING SUITE FOR MITOSIS OLLAMA PROCESSING TOOL FIX
Testing the corrected OllamaProcessingTool to verify task_id error is resolved
Focus: Verify that 'OllamaProcessingTool' object has no attribute 'task_id' error is fixed
Context: Error was in line 76, changed self.task_id to config.get('task_id', 'unknown')
"""

import requests
import json
import time
import sys
from datetime import datetime
import threading
import re

# Configuration
BACKEND_URL = "https://83993f50-e8e3-4f88-9193-8787d73814b8.preview.emergentagent.com"

class MitosisOllamaProcessingTester:
    def __init__(self):
        self.backend_url = BACKEND_URL
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        })
        self.test_results = []
        self.created_task_id = None
        self.task_id_errors_detected = []
        
    def log_test(self, test_name, success, details, error=None):
        """Log test results"""
        result = {
            'test': test_name,
            'success': success,
            'details': details,
            'error': str(error) if error else None,
            'timestamp': datetime.now().isoformat()
        }
        self.test_results.append(result)
        
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"{status} {test_name}")
        if details:
            print(f"   Details: {details}")
        if error:
            print(f"   Error: {error}")
        print()

    def test_1_backend_health(self):
        """Test 1: Backend Health Check"""
        try:
            print("üîÑ Test 1: Checking backend health endpoints")
            
            # Test /api/health
            url = f"{self.backend_url}/api/health"
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                services = data.get('services', {})
                database = services.get('database', False)
                ollama = services.get('ollama', False)
                tools = services.get('tools', 0)
                
                details = f"Database: {database}, Ollama: {ollama}, Tools: {tools}"
                self.log_test("1. Backend Health Check", True, details)
                return True
            else:
                self.log_test("1. Backend Health Check", False, f"HTTP {response.status_code}: {response.text}")
                return False
                
        except Exception as e:
            self.log_test("1. Backend Health Check", False, "Request failed", e)
            return False

    def test_2_create_python_info_task(self):
        """Test 2: Create Python Information Task - Testing OllamaProcessingTool"""
        try:
            print("üîÑ Test 2: Creating Python information task to test OllamaProcessingTool")
            
            url = f"{self.backend_url}/api/agent/chat"
            payload = {
                "message": "Busca informaci√≥n sobre Python",
                "auto_execute": True
            }
            
            response = self.session.post(url, json=payload, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                task_id = data.get('task_id')
                
                if task_id:
                    self.created_task_id = task_id
                    details = f"Task created successfully: {task_id}"
                    self.log_test("2. Create Python Info Task", True, details)
                    return task_id
                else:
                    self.log_test("2. Create Python Info Task", False, f"No task_id in response: {data}")
                    return None
            else:
                self.log_test("2. Create Python Info Task", False, f"HTTP {response.status_code}: {response.text}")
                return None
                
        except Exception as e:
            self.log_test("2. Create Python Info Task", False, "Request failed", e)
            return None

    def test_3_verify_plan_generation(self):
        """Test 3: Verify Plan Generation Without task_id Errors"""
        try:
            print("üîÑ Test 3: Verifying plan generation without task_id errors")
            
            if not self.created_task_id:
                self.log_test("3. Plan Generation", False, "No task_id available")
                return False
            
            # Wait a moment for plan generation
            time.sleep(5)
            
            url = f"{self.backend_url}/api/agent/get-task-status/{self.created_task_id}"
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                success = data.get('success', False)
                plan = data.get('plan', [])
                total_steps = data.get('total_steps', 0)
                
                if success and len(plan) >= 1:
                    details = f"Plan generated: {len(plan)} steps, Total: {total_steps}"
                    self.log_test("3. Plan Generation", True, details)
                    return True
                else:
                    self.log_test("3. Plan Generation", False, f"Invalid plan: success={success}, steps={len(plan)}")
                    return False
            else:
                self.log_test("3. Plan Generation", False, f"HTTP {response.status_code}: {response.text}")
                return False
                
        except Exception as e:
            self.log_test("3. Plan Generation", False, "Request failed", e)
            return False

    def test_4_monitor_step_execution_for_task_id_errors(self):
        """Test 4: Monitor Step Execution for task_id Attribute Errors"""
        try:
            print("üîÑ Test 4: Monitoring step execution for task_id attribute errors (CRITICAL)")
            
            if not self.created_task_id:
                self.log_test("4. Task ID Error Detection", False, "No task_id available")
                return False
            
            # Monitor step execution for up to 60 seconds
            start_time = time.time()
            max_wait_time = 60
            task_id_error_detected = False
            steps_executed = 0
            
            print("   Monitoring for task_id attribute errors...")
            
            while time.time() - start_time < max_wait_time:
                url = f"{self.backend_url}/api/agent/get-task-status/{self.created_task_id}"
                response = self.session.get(url, timeout=5)
                
                if response.status_code == 200:
                    data = response.json()
                    status = data.get('status', 'unknown')
                    current_step = data.get('current_step', 0)
                    completed_steps = data.get('completed_steps', 0)
                    plan = data.get('plan', [])
                    
                    # Check each step for task_id errors
                    for i, step in enumerate(plan):
                        step_status = step.get('status', 'pending')
                        step_result = str(step.get('result', ''))
                        step_error = str(step.get('error', ''))
                        
                        # Look for the specific task_id attribute error
                        task_id_error_patterns = [
                            "'OllamaProcessingTool' object has no attribute 'task_id'",
                            "AttributeError: 'OllamaProcessingTool' object has no attribute 'task_id'",
                            "task_id.*attribute.*error",
                            "OllamaProcessingTool.*task_id"
                        ]
                        
                        combined_text = f"{step_result} {step_error}".lower()
                        
                        for pattern in task_id_error_patterns:
                            if re.search(pattern.lower(), combined_text):
                                task_id_error_detected = True
                                error_details = f"Task ID error detected in step {i+1}: {pattern}"
                                self.task_id_errors_detected.append(error_details)
                                print(f"   ‚ùå CRITICAL: {error_details}")
                        
                        if step_status in ['completed', 'in_progress']:
                            steps_executed = max(steps_executed, i + 1)
                    
                    print(f"   Status: {status}, Steps executed: {steps_executed}, Completed: {completed_steps}")
                    
                    # If we detect task_id errors, fail immediately
                    if task_id_error_detected:
                        details = f"CRITICAL: task_id attribute errors detected: {len(self.task_id_errors_detected)} errors"
                        self.log_test("4. Task ID Error Detection", False, details)
                        return False
                    
                    # Check if we have good progress without errors
                    if completed_steps > 0 or steps_executed > 0:
                        details = f"Steps executing without task_id errors: executed={steps_executed}, completed={completed_steps}"
                        self.log_test("4. Task ID Error Detection", True, details)
                        return True
                    
                    # Check for task failure
                    if status in ['failed', 'error']:
                        # Check if failure is due to task_id error
                        if not task_id_error_detected:
                            details = f"Task failed but no task_id errors detected: status={status}"
                            self.log_test("4. Task ID Error Detection", True, details)
                            return True
                        else:
                            details = f"Task failed due to task_id errors: status={status}"
                            self.log_test("4. Task ID Error Detection", False, details)
                            return False
                
                time.sleep(3)
            
            # Timeout reached - check final state
            if task_id_error_detected:
                details = f"CRITICAL: task_id errors detected during execution: {len(self.task_id_errors_detected)} errors"
                self.log_test("4. Task ID Error Detection", False, details)
                return False
            elif steps_executed > 0:
                details = f"No task_id errors detected, {steps_executed} steps executed successfully"
                self.log_test("4. Task ID Error Detection", True, details)
                return True
            else:
                details = f"No task_id errors detected but no steps executed within {max_wait_time}s"
                self.log_test("4. Task ID Error Detection", True, details)
                return True
                
        except Exception as e:
            self.log_test("4. Task ID Error Detection", False, "Request failed", e)
            return False

    def test_5_verify_ollama_processing_tool_functionality(self):
        """Test 5: Verify OllamaProcessingTool Works Without Errors"""
        try:
            print("üîÑ Test 5: Verifying OllamaProcessingTool functionality")
            
            if not self.created_task_id:
                self.log_test("5. OllamaProcessingTool Functionality", False, "No task_id available")
                return False
            
            # Get current task status
            url = f"{self.backend_url}/api/agent/get-task-status/{self.created_task_id}"
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                plan = data.get('plan', [])
                status = data.get('status', 'unknown')
                
                # Look for evidence of successful OllamaProcessingTool usage
                ollama_tool_used = False
                ollama_tool_successful = False
                
                for step in plan:
                    step_result = str(step.get('result', ''))
                    step_status = step.get('status', 'pending')
                    
                    # Look for indicators that Ollama was used
                    ollama_indicators = [
                        'ollama',
                        'processing',
                        'generated',
                        'analysis',
                        'python',
                        'information'
                    ]
                    
                    if any(indicator in step_result.lower() for indicator in ollama_indicators):
                        ollama_tool_used = True
                        if step_status == 'completed' and len(step_result) > 50:
                            ollama_tool_successful = True
                
                if ollama_tool_successful:
                    details = f"OllamaProcessingTool working successfully: tool used and generated content"
                    self.log_test("5. OllamaProcessingTool Functionality", True, details)
                    return True
                elif ollama_tool_used:
                    details = f"OllamaProcessingTool used but results unclear: tool detected but limited output"
                    self.log_test("5. OllamaProcessingTool Functionality", True, details)
                    return True
                else:
                    details = f"OllamaProcessingTool usage not clearly detected: status={status}"
                    self.log_test("5. OllamaProcessingTool Functionality", False, details)
                    return False
            else:
                self.log_test("5. OllamaProcessingTool Functionality", False, f"HTTP {response.status_code}: {response.text}")
                return False
                
        except Exception as e:
            self.log_test("5. OllamaProcessingTool Functionality", False, "Request failed", e)
            return False

    def test_6_check_backend_logs_for_task_id_errors(self):
        """Test 6: Check if Backend Logs Show task_id Errors (Indirect)"""
        try:
            print("üîÑ Test 6: Checking for task_id errors through task execution patterns")
            
            if not self.created_task_id:
                self.log_test("6. Backend Log Analysis", False, "No task_id available")
                return False
            
            # Monitor task for patterns that indicate task_id errors
            start_time = time.time()
            max_wait_time = 30
            
            while time.time() - start_time < max_wait_time:
                url = f"{self.backend_url}/api/agent/get-task-status/{self.created_task_id}"
                response = self.session.get(url, timeout=5)
                
                if response.status_code == 200:
                    data = response.json()
                    status = data.get('status', 'unknown')
                    plan = data.get('plan', [])
                    
                    # Look for error patterns that suggest task_id issues
                    error_patterns_found = []
                    
                    for i, step in enumerate(plan):
                        step_error = str(step.get('error', ''))
                        step_result = str(step.get('result', ''))
                        
                        # Patterns that might indicate task_id errors
                        if 'attribute' in step_error.lower() and 'task_id' in step_error.lower():
                            error_patterns_found.append(f"Step {i+1}: Attribute error with task_id")
                        
                        if 'ollamaprocessingtool' in step_error.lower() and 'task_id' in step_error.lower():
                            error_patterns_found.append(f"Step {i+1}: OllamaProcessingTool task_id error")
                        
                        # Look for successful execution patterns
                        if step.get('status') == 'completed' and len(step_result) > 20:
                            # This suggests the tool worked without task_id errors
                            pass
                    
                    if error_patterns_found:
                        details = f"Task ID error patterns detected: {error_patterns_found}"
                        self.log_test("6. Backend Log Analysis", False, details)
                        return False
                    
                    # If task is progressing normally, that's a good sign
                    if status in ['in_progress', 'completed'] or any(step.get('status') == 'completed' for step in plan):
                        details = f"Task executing normally, no task_id error patterns detected"
                        self.log_test("6. Backend Log Analysis", True, details)
                        return True
                
                time.sleep(2)
            
            # If we get here, no obvious errors were detected
            details = f"No task_id error patterns detected in task execution"
            self.log_test("6. Backend Log Analysis", True, details)
            return True
                
        except Exception as e:
            self.log_test("6. Backend Log Analysis", False, "Request failed", e)
            return False

    def test_7_verify_automatic_step_execution(self):
        """Test 7: Verify Steps Execute Automatically Without Hanging"""
        try:
            print("üîÑ Test 7: Verifying automatic step execution")
            
            if not self.created_task_id:
                self.log_test("7. Automatic Step Execution", False, "No task_id available")
                return False
            
            # Monitor for automatic step execution for up to 90 seconds
            start_time = time.time()
            max_wait_time = 90
            initial_completed = 0
            
            # Get initial state
            url = f"{self.backend_url}/api/agent/get-task-status/{self.created_task_id}"
            response = self.session.get(url, timeout=5)
            if response.status_code == 200:
                data = response.json()
                initial_completed = data.get('completed_steps', 0)
            
            while time.time() - start_time < max_wait_time:
                response = self.session.get(url, timeout=5)
                
                if response.status_code == 200:
                    data = response.json()
                    status = data.get('status', 'unknown')
                    completed_steps = data.get('completed_steps', 0)
                    current_step = data.get('current_step', 0)
                    total_steps = data.get('total_steps', 0)
                    
                    print(f"   Status: {status}, Progress: {completed_steps}/{total_steps}")
                    
                    # Check for progress
                    if completed_steps > initial_completed:
                        details = f"Steps executing automatically: {completed_steps}/{total_steps} completed"
                        self.log_test("7. Automatic Step Execution", True, details)
                        return True
                    
                    # Check for completion
                    if status == 'completed':
                        details = f"Task completed automatically: {completed_steps}/{total_steps} steps"
                        self.log_test("7. Automatic Step Execution", True, details)
                        return True
                    
                    # Check for failure due to task_id errors
                    if status in ['failed', 'error']:
                        # Check if any task_id errors were detected
                        if self.task_id_errors_detected:
                            details = f"Task failed due to task_id errors: {len(self.task_id_errors_detected)} errors"
                            self.log_test("7. Automatic Step Execution", False, details)
                            return False
                        else:
                            details = f"Task failed but not due to task_id errors: status={status}"
                            self.log_test("7. Automatic Step Execution", True, details)
                            return True
                
                time.sleep(5)
            
            # Check final state
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                status = data.get('status', 'unknown')
                completed_steps = data.get('completed_steps', 0)
                total_steps = data.get('total_steps', 0)
                
                if completed_steps > initial_completed:
                    details = f"Some automatic execution occurred: {completed_steps}/{total_steps} steps"
                    self.log_test("7. Automatic Step Execution", True, details)
                    return True
                else:
                    details = f"No automatic step execution within {max_wait_time}s: {completed_steps}/{total_steps}"
                    self.log_test("7. Automatic Step Execution", False, details)
                    return False
            else:
                self.log_test("7. Automatic Step Execution", False, "Cannot get final task status")
                return False
                
        except Exception as e:
            self.log_test("7. Automatic Step Execution", False, "Request failed", e)
            return False

    def run_ollama_processing_tests(self):
        """Run comprehensive OllamaProcessingTool fix tests"""
        print("üöÄ MITOSIS OLLAMA PROCESSING TOOL FIX TESTING")
        print("=" * 70)
        print(f"Backend URL: {self.backend_url}")
        print(f"Test Time: {datetime.now().isoformat()}")
        print(f"Test Task: 'Busca informaci√≥n sobre Python'")
        print(f"FOCUS: Verify 'OllamaProcessingTool' object has no attribute 'task_id' error is RESOLVED")
        print(f"CONTEXT: Error was in line 76, changed self.task_id to config.get('task_id', 'unknown')")
        print()
        
        # Test 1: Backend Health
        print("=" * 50)
        health_ok = self.test_1_backend_health()
        if not health_ok:
            print("‚ùå Backend health check failed. Aborting tests.")
            return self.test_results
        
        # Test 2: Create Python Info Task
        print("=" * 50)
        task_id = self.test_2_create_python_info_task()
        if not task_id:
            print("‚ùå Failed to create Python info task. Aborting remaining tests.")
            self.print_summary()
            return self.test_results
        
        # Wait a moment for task to be saved
        print("‚è≥ Waiting 5 seconds for task to be saved...")
        time.sleep(5)
        
        # Test 3: Plan Generation
        print("=" * 50)
        plan_ok = self.test_3_verify_plan_generation()
        
        # Test 4: Task ID Error Detection (CRITICAL)
        print("=" * 50)
        task_id_ok = self.test_4_monitor_step_execution_for_task_id_errors()
        
        # Test 5: OllamaProcessingTool Functionality
        print("=" * 50)
        ollama_ok = self.test_5_verify_ollama_processing_tool_functionality()
        
        # Test 6: Backend Log Analysis
        print("=" * 50)
        log_ok = self.test_6_check_backend_logs_for_task_id_errors()
        
        # Test 7: Automatic Step Execution
        print("=" * 50)
        execution_ok = self.test_7_verify_automatic_step_execution()
        
        # Summary
        self.print_summary()
        
        return self.test_results

    def print_summary(self):
        """Print test summary"""
        print("\n" + "=" * 70)
        print("üéØ OLLAMA PROCESSING TOOL FIX TEST SUMMARY")
        print("=" * 70)
        
        passed = sum(1 for result in self.test_results if result['success'])
        total = len(self.test_results)
        
        print(f"Tests Passed: {passed}/{total}")
        print()
        
        # Analyze results for the specific OllamaProcessingTool fix
        critical_issues = []
        task_id_error_resolved = True
        ollama_tool_working = True
        automatic_execution_working = True
        
        for result in self.test_results:
            if not result['success']:
                test_name = result['test']
                details = result['details'] or result['error']
                
                if 'Task ID Error Detection' in test_name:
                    critical_issues.append(f"üö® CRITICAL: {test_name} - {details}")
                    task_id_error_resolved = False
                elif 'OllamaProcessingTool Functionality' in test_name:
                    critical_issues.append(f"üö® CRITICAL: {test_name} - {details}")
                    ollama_tool_working = False
                elif 'Automatic Step Execution' in test_name:
                    critical_issues.append(f"‚ö†Ô∏è MAJOR: {test_name} - {details}")
                    automatic_execution_working = False
                elif 'Plan Generation' in test_name:
                    critical_issues.append(f"‚ö†Ô∏è MAJOR: {test_name} - {details}")
                else:
                    critical_issues.append(f"‚ùå {test_name} - {details}")
        
        if critical_issues:
            print("üö® ISSUES FOUND:")
            for issue in critical_issues:
                print(f"  {issue}")
        else:
            print("‚úÖ All OllamaProcessingTool fix tests passed successfully")
        
        print()
        
        # Specific diagnosis for the OllamaProcessingTool fix
        print("üîç OLLAMA PROCESSING TOOL FIX ANALYSIS:")
        
        if task_id_error_resolved:
            print("‚úÖ 'TASK_ID ATTRIBUTE' ERROR: RESOLVED")
            print("   - No 'OllamaProcessingTool' object has no attribute 'task_id' errors detected")
            print("   - Steps execute without AttributeError exceptions")
        else:
            print("‚ùå 'TASK_ID ATTRIBUTE' ERROR: STILL OCCURRING")
            print("   - The critical task_id attribute error is still present")
            print(f"   - {len(self.task_id_errors_detected)} task_id errors detected")
        
        if ollama_tool_working:
            print("‚úÖ OLLAMA PROCESSING TOOL: WORKING")
            print("   - Tool executes successfully and generates content")
            print("   - No attribute errors during tool execution")
        else:
            print("‚ùå OLLAMA PROCESSING TOOL: NOT WORKING")
            print("   - Tool fails to execute or generates no content")
        
        if automatic_execution_working:
            print("‚úÖ AUTOMATIC STEP EXECUTION: WORKING")
            print("   - Steps execute automatically without hanging")
            print("   - Task progresses through multiple steps")
        else:
            print("‚ùå AUTOMATIC STEP EXECUTION: NOT WORKING")
            print("   - Steps do not execute automatically")
            print("   - Task may be hanging due to errors")
        
        print()
        
        # Overall assessment
        if task_id_error_resolved and ollama_tool_working:
            print("üéâ OVERALL ASSESSMENT: ‚úÖ OLLAMA PROCESSING TOOL FIX SUCCESSFUL")
            print("   - The task_id attribute error has been resolved")
            print("   - OllamaProcessingTool works without AttributeError")
            print("   - Tasks can execute and progress normally")
            print("   - The fix (self.task_id ‚Üí config.get('task_id', 'unknown')) is working")
        else:
            print("‚ö†Ô∏è OVERALL ASSESSMENT: ‚ùå OLLAMA PROCESSING TOOL FIX NEEDS MORE WORK")
            print("   - The task_id attribute error may still be occurring")
            print("   - OllamaProcessingTool may still have issues")
            print("   - Additional debugging and fixes may be needed")
        
        print()
        
        # Specific recommendations
        print("üìã RECOMMENDATIONS:")
        if not task_id_error_resolved:
            print("   1. Check OllamaProcessingTool line 76 implementation")
            print("   2. Verify config.get('task_id', 'unknown') is working correctly")
            print("   3. Check if task_id is being passed properly in config")
            print("   4. Review backend logs for AttributeError exceptions")
        
        if not ollama_tool_working:
            print("   1. Verify Ollama service is running and accessible")
            print("   2. Check OllamaProcessingTool initialization")
            print("   3. Test tool execution independently")
        
        if not automatic_execution_working:
            print("   1. Check task execution pipeline for blocking issues")
            print("   2. Verify WebSocket events are being emitted")
            print("   3. Check for infinite loops or hanging processes")
        
        if task_id_error_resolved and ollama_tool_working:
            print("   1. OllamaProcessingTool fix is working correctly")
            print("   2. Monitor for any regression issues")
            print("   3. Test with different types of tasks")
        
        print()
        
        # Task ID errors summary
        if self.task_id_errors_detected:
            print("üö® TASK_ID ERRORS DETECTED:")
            for error in self.task_id_errors_detected:
                print(f"   - {error}")
        else:
            print("‚úÖ NO TASK_ID ERRORS DETECTED")
        
        print()
        print("üìä OLLAMA PROCESSING TOOL FIX TESTING COMPLETE")
        
        if self.created_task_id:
            print(f"üìù Test Task ID: {self.created_task_id}")
            print("   Use this ID to check logs and debug if needed")

if __name__ == "__main__":
    tester = MitosisOllamaProcessingTester()
    results = tester.run_ollama_processing_tests()
    
    # Exit with appropriate code
    failed_tests = sum(1 for result in results if not result['success'])
    sys.exit(failed_tests)