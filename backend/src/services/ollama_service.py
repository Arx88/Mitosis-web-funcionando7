"""
Servicio de integraci√≥n con Ollama - Versi√≥n Real
Conecta directamente con Ollama para generar respuestas
"""

import json
import time
import os
import logging
from typing import Dict, List, Optional, Any
import requests
from requests.exceptions import RequestException, Timeout

class OllamaService:
    def __init__(self, base_url: str = None):
        self.base_url = base_url or os.getenv('OLLAMA_BASE_URL', 'https://bef4a4bb93d1.ngrok-free.app')
        self.default_model = os.getenv("OLLAMA_DEFAULT_MODEL", "llama3.1:8b")  # Configurable por defecto
        self.current_model = None
        self.conversation_history = []
        self.request_timeout = 90  # Base timeout, ser√° sobrescrito por configuraci√≥n por modelo
        
        # üÜï PROBLEMA 3: Configuraci√≥n de par√°metros por modelo
        self.model_configs = self._load_model_configs()
        
    def _load_model_configs(self) -> dict:
        """Carga las configuraciones de los modelos desde un archivo o define valores por defecto."""
        config_path = os.getenv("OLLAMA_MODEL_CONFIGS_PATH", "/app/backend/config/ollama_model_configs.json")
        
        try:
            if os.path.exists(config_path):
                with open(config_path, "r", encoding="utf-8") as f:
                    configs = json.load(f)
                    
                    # Validar estructura b√°sica de las configuraciones cargadas
                    for model, params in configs.items():
                        if model.startswith('_'):  # Ignorar metadatos
                            continue
                        if not isinstance(params, dict) or "options" not in params:
                            logging.getLogger(__name__).warning(f"‚ö†Ô∏è Configuraci√≥n inv√°lida para el modelo {model}. Usando valores por defecto.")
                            configs[model] = self._get_default_model_config()
                    
                    logging.getLogger(__name__).info(f"‚úÖ Configuraciones de modelos Ollama cargadas desde {config_path}")
                    return configs
            else:
                logging.getLogger(__name__).info(f"‚ÑπÔ∏è Archivo de configuraci√≥n no encontrado en {config_path}. Usando valores por defecto.")
                return self._get_default_model_configs()
                
        except Exception as e:
            logging.getLogger(__name__).error(f"‚ùå Error al cargar configuraciones de modelos desde {config_path}: {e}. Usando valores por defecto.")
            return self._get_default_model_configs()
    
    def _get_default_model_configs(self) -> dict:
        """Define las configuraciones por defecto para los modelos conocidos."""
        return {
            "llama3.1:8b": {
                "options": {
                    "temperature": 0.15,  # M√°s creativo que 0.1 pero preciso
                    "top_p": 0.7,         # Reduce probabilidad de palabras menos probables
                    "top_k": 20,          # Limita muestreo a las 20 palabras m√°s probables
                    "repeat_penalty": 1.1,# Penaliza repetici√≥n de tokens
                    "stop": ["```", "---", "<|eot_id|>", "<|end_of_text|>"]  # Tokens de parada espec√≠ficos
                },
                "request_timeout": 180    # 3 minutos para Llama3.1:8b
            },
            "qwen3:32b": {
                "options": {
                    "temperature": 0.1,   # Muy bajo para m√°xima precisi√≥n
                    "top_p": 0.6,         # M√°s restrictivo para evitar divagaciones
                    "top_k": 15,          # Muestreo muy limitado
                    "repeat_penalty": 1.05,# Penalizaci√≥n ligera
                    "stop": ["```json", "</tool_code>", "<|im_end|>", "<|endoftext|>"]  # Tokens espec√≠ficos Qwen
                },
                "request_timeout": 480    # 8 minutos para Qwen3:32b (modelo grande)
            },
            "deepseek-r1:32b": {
                "options": {
                    "temperature": 0.12,
                    "top_p": 0.65,
                    "top_k": 18,
                    "repeat_penalty": 1.08,
                    "stop": ["```", "---", "<|endofthought|>", "<|end|>"]
                },
                "request_timeout": 420    # 7 minutos para DeepSeek R1
            },
            "default": {
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 40,
                    "repeat_penalty": 1.1,
                    "stop": []
                },
                "request_timeout": 120    # 2 minutos por defecto
            }
        }
    
    def _get_model_config(self, model_name: str) -> dict:
        """Obtiene la configuraci√≥n para un modelo dado, con fallback a la configuraci√≥n por defecto."""
        if model_name in self.model_configs:
            return self.model_configs[model_name]
        else:
            logging.getLogger(__name__).warning(f"‚ö†Ô∏è No hay configuraci√≥n espec√≠fica para el modelo '{model_name}', usando configuraci√≥n por defecto.")
            return self.model_configs.get("default", self.model_configs["default"])
    
    def get_model_info(self, model_name: str = None) -> dict:
        """
        Obtiene informaci√≥n detallada sobre la configuraci√≥n de un modelo.
        
        Args:
            model_name: Nombre del modelo (si no se especifica, usa el modelo actual)
        
        Returns:
            Dict con informaci√≥n de configuraci√≥n del modelo
        """
        target_model = model_name or self.get_current_model()
        config = self._get_model_config(target_model)
        
        return {
            'model_name': target_model,
            'config': config,
            'is_optimized': target_model in self.model_configs and target_model != 'default',
            'timeout': config.get('request_timeout', 120),
            'temperature': config.get('options', {}).get('temperature', 0.7),
            'description': config.get('description', 'Sin descripci√≥n disponible')
        }
        
        
    def is_healthy(self) -> bool:
        """Verificar si Ollama est√° disponible"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def check_connection(self) -> Dict[str, Any]:
        """Verificar conexi√≥n con Ollama y retornar informaci√≥n detallada"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                return {
                    'status': 'connected',
                    'url': self.base_url,
                    'models_available': len(models),
                    'current_model': self.current_model or self.default_model,
                    'healthy': True
                }
        except Exception as e:
            return {
                'status': 'error',
                'url': self.base_url,
                'error': str(e),
                'healthy': False
            }
        
        return {
            'status': 'disconnected',
            'url': self.base_url,
            'healthy': False
        }
    
    def get_available_models(self) -> List[str]:
        """Obtener lista de modelos disponibles desde Ollama"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                return models
        except:
            pass
        
        # Fallback a modelos conocidos si no se puede conectar
        return [
            "llama3.2",
            "llama3.1", 
            "mistral",
            "codellama",
            "phi3"
        ]
    
    def set_model(self, model_name: str) -> bool:
        """Establecer el modelo a usar"""
        available_models = self.get_available_models()
        if model_name in available_models:
            self.current_model = model_name
            return True
        return False
    
    def get_current_model(self) -> str:
        """Obtener el modelo actual"""
        return self.current_model or self.default_model
    
    def generate_casual_response(self, prompt: str, context: Dict = None) -> Dict[str, Any]:
        """
        Generar respuesta casual usando Ollama (sin planes ni herramientas)
        
        Args:
            prompt: Mensaje del usuario
            context: Contexto adicional (historial, etc.)
        
        Returns:
            Dict con respuesta casual y metadatos
        """
        if not self.is_healthy():
            return {
                'response': "‚ö†Ô∏è Ollama no est√° disponible en este momento. Verifica la configuraci√≥n del endpoint de Ollama.",
                'tool_calls': [],
                'raw_response': "",
                'model': self.get_current_model(),
                'timestamp': time.time(),
                'error': 'Ollama no disponible'
            }
        
        try:
            # Construir el prompt con system prompt para conversaci√≥n casual
            system_prompt = self._build_system_prompt(use_tools=False, conversation_mode=True)
            full_prompt = self._build_full_prompt(prompt, context, system_prompt)
            
            # Hacer la llamada a Ollama
            response = self._call_ollama_api(full_prompt)
            
            if response.get('error'):
                return {
                    'response': f"‚ùå Error al generar respuesta: {response['error']}",
                    'tool_calls': [],
                    'raw_response': "",
                    'model': self.get_current_model(),
                    'timestamp': time.time(),
                    'error': response['error']
                }
            
            # Para conversaci√≥n casual, no parseamos tool calls, solo devolvemos el texto
            response_text = response.get('response', '').strip()
            
            return {
                'response': response_text,
                'tool_calls': [],
                'raw_response': response.get('response', ''),
                'model': self.get_current_model(),
                'timestamp': time.time()
            }
            
        except Exception as e:
            return {
                'response': f"‚ùå Error interno: {str(e)}",
                'tool_calls': [],
                'raw_response': "",
                'model': self.get_current_model(),
                'timestamp': time.time(),
                'error': str(e)
            }

    def generate_response(self, prompt: str, context: Dict = None, use_tools: bool = True) -> Dict[str, Any]:
        """
        Generar respuesta usando Ollama real
        
        Args:
            prompt: Mensaje del usuario
            context: Contexto adicional (historial, herramientas, etc.)
            use_tools: Si debe considerar el uso de herramientas
        
        Returns:
            Dict con respuesta, tool_calls, y metadatos
        """
        if not self.is_healthy():
            return {
                'response': "‚ö†Ô∏è Ollama no est√° disponible en este momento. Aseg√∫rate de que est√© ejecut√°ndose en localhost:11434",
                'tool_calls': [],
                'raw_response': "",
                'model': self.get_current_model(),
                'timestamp': time.time(),
                'error': 'Ollama no disponible'
            }
        
        try:
            # Construir el prompt completo
            system_prompt = self._build_system_prompt(use_tools, conversation_mode=False)
            full_prompt = self._build_full_prompt(prompt, context, system_prompt)
            
            # Hacer la llamada a Ollama
            response = self._call_ollama_api(full_prompt)
            
            if response.get('error'):
                return {
                    'response': f"‚ùå Error al generar respuesta: {response['error']}",
                    'tool_calls': [],
                    'raw_response': "",
                    'model': self.get_current_model(),
                    'timestamp': time.time(),
                    'error': response['error']
                }
            
            # Parsear la respuesta
            parsed_response = self._parse_response(response.get('response', ''))
            
            return {
                'response': parsed_response['text'],
                'tool_calls': parsed_response['tool_calls'],
                'raw_response': response.get('response', ''),
                'model': self.get_current_model(),
                'timestamp': time.time()
            }
            
        except Exception as e:
            return {
                'response': f"‚ùå Error interno: {str(e)}",
                'tool_calls': [],
                'raw_response': "",
                'model': self.get_current_model(),
                'timestamp': time.time(),
                'error': str(e)
            }
    
    def _call_ollama_api(self, prompt: str, custom_options: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Hacer llamada real a la API de Ollama con par√°metros optimizados por modelo espec√≠fico
        PROBLEMA 3: Implementaci√≥n de configuraci√≥n din√°mica por modelo
        """
        try:
            current_model_name = self.get_current_model()
            model_config = self._get_model_config(current_model_name)
            
            # Obtener opciones base del modelo
            model_options = model_config.get("options", {}).copy()
            
            # Detectar si es una solicitud JSON y ajustar par√°metros espec√≠ficamente
            is_json_request = any(keyword in prompt.lower() for keyword in ['json', '"steps"', 'genera un plan', 'plan de acci√≥n'])
            
            if is_json_request:
                # Para solicitudes JSON, usar par√°metros m√°s estrictos
                model_options['temperature'] = min(model_options.get('temperature', 0.7) * 0.5, 0.1)  # Reducir temperatura para JSON
                model_options['top_p'] = min(model_options.get('top_p', 0.9) * 0.8, 0.7)  # M√°s restrictivo
                
                # Agregar stops espec√≠ficos para JSON si no est√°n
                current_stops = model_options.get('stop', [])
                json_stops = ['```', '---', '}```', '}\n```']
                model_options['stop'] = list(set(current_stops + json_stops))
            
            # Fusionar con opciones personalizadas si se proporcionan
            if custom_options:
                model_options.update(custom_options)
            
            # Determinar el timeout de la solicitud usando configuraci√≥n por modelo
            request_timeout = model_config.get("request_timeout", self.request_timeout)
            
            payload = {
                "model": current_model_name,
                "prompt": prompt,
                "stream": False,
                "options": model_options  # üÜï Usar las opciones espec√≠ficas del modelo
            }
            
            # Logging detallado para debug
            logger = logging.getLogger(__name__)
            logger.debug(f"ü§ñ Ollama Request - Model: {current_model_name}")
            logger.debug(f"‚öôÔ∏è Options: temp={model_options.get('temperature')}, top_p={model_options.get('top_p')}, timeout={request_timeout}s")
            if is_json_request:
                logger.debug(f"üìã JSON mode detected, using strict parameters")
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=request_timeout  # üÜï Usar el timeout espec√≠fico del modelo
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                logger.error(f"‚ùå Ollama API returned error for model {current_model_name}: HTTP {response.status_code}")
                return {
                    'error': f"HTTP {response.status_code}: {response.text}"
                }
                
        except Timeout:
            logger = logging.getLogger(__name__)
            logger.error(f"‚è±Ô∏è Ollama API request timed out after {request_timeout} seconds for model {current_model_name}.")
            return {
                'error': f"Timeout despu√©s de {request_timeout} segundos para el modelo {current_model_name}. El modelo puede necesitar m√°s tiempo para respuestas complejas."
            }
        except RequestException as e:
            logger = logging.getLogger(__name__)
            logger.error(f"üîå Connection error to Ollama API for model {current_model_name}: {str(e)}")
            return {
                'error': f"Error de conexi√≥n: {str(e)}"
            }
        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.error(f"üí• Unexpected error in Ollama API call for model {current_model_name}: {str(e)}")
            return {
                'error': f"Error inesperado: {str(e)}"
            }
    
    def _parse_response(self, response_text: str) -> Dict[str, Any]:
        """
        Parsear respuesta para extraer texto y tool calls con estrategias robustas
        Mejora implementada seg√∫n UPGRADE.md Secci√≥n 4: Servicio Ollama y Extracci√≥n de Query
        """
        import re
        import json
        import logging
        
        logger = logging.getLogger(__name__)
        
        if not response_text or not isinstance(response_text, str):
            return {'text': '', 'tool_calls': []}
        
        tool_calls = []
        clean_text = response_text
        
        # Estrategia 1: Buscar bloques JSON cl√°sicos con ``` 
        json_pattern_1 = r'```json\s*(\{.*?\})\s*```'
        matches_1 = re.findall(json_pattern_1, response_text, re.DOTALL)
        
        for match in matches_1:
            try:
                data = json.loads(match)
                if 'tool_call' in data:
                    tool_calls.append(data['tool_call'])
                    # Remover el bloque JSON del texto
                    clean_text = clean_text.replace(f'```json\n{match}\n```', '')
                    logger.debug(f"‚úÖ JSON parsing strategy 1 successful: {match[:50]}...")
            except json.JSONDecodeError as e:
                logger.debug(f"‚ö†Ô∏è JSON parsing strategy 1 failed for match: {str(e)}")
                continue
        
        # Estrategia 2: Buscar JSON sin marcadores de bloque
        if not tool_calls:
            json_pattern_2 = r'\{[^{}]*"tool_call"[^{}]*\{[^{}]*\}[^{}]*\}'
            matches_2 = re.findall(json_pattern_2, response_text)
            
            for match in matches_2:
                try:
                    data = json.loads(match)
                    if 'tool_call' in data:
                        tool_calls.append(data['tool_call'])
                        clean_text = clean_text.replace(match, '')
                        logger.debug(f"‚úÖ JSON parsing strategy 2 successful: {match[:50]}...")
                except json.JSONDecodeError as e:
                    logger.debug(f"‚ö†Ô∏è JSON parsing strategy 2 failed: {str(e)}")
                    continue
        
        # Estrategia 3: Buscar cualquier JSON v√°lido y verificar si contiene tool_call
        if not tool_calls:
            json_pattern_3 = r'\{[^}]*\}'
            potential_jsons = re.findall(json_pattern_3, response_text)
            
            for potential_json in potential_jsons:
                try:
                    # Intentar corregir JSON mal formateado
                    corrected_json = potential_json.replace("'", '"')  # Comillas simples por dobles
                    data = json.loads(corrected_json)
                    
                    if isinstance(data, dict) and 'tool_call' in data:
                        tool_calls.append(data['tool_call'])
                        clean_text = clean_text.replace(potential_json, '')
                        logger.debug(f"‚úÖ JSON parsing strategy 3 successful: {potential_json[:50]}...")
                        break
                except (json.JSONDecodeError, ValueError) as e:
                    logger.debug(f"‚ö†Ô∏è JSON parsing strategy 3 failed for '{potential_json[:30]}...': {str(e)}")
                    continue
        
        # Estrategia 4: Extracci√≥n por regex espec√≠fico de tool_call
        if not tool_calls:
            try:
                tool_pattern = r'"tool_call"\s*:\s*\{[^}]*"tool"\s*:\s*"([^"]+)"[^}]*"parameters"\s*:\s*\{[^}]*\}'
                tool_matches = re.finditer(tool_pattern, response_text)
                
                for tool_match in tool_matches:
                    try:
                        # Intentar construir tool_call b√°sico desde regex
                        full_match = tool_match.group()
                        tool_name_match = re.search(r'"tool"\s*:\s*"([^"]+)"', full_match)
                        params_match = re.search(r'"parameters"\s*:\s*(\{[^}]*\})', full_match)
                        
                        if tool_name_match:
                            tool_call = {
                                "tool": tool_name_match.group(1),
                                "parameters": json.loads(params_match.group(1)) if params_match else {}
                            }
                            tool_calls.append(tool_call)
                            clean_text = clean_text.replace(tool_match.group(), '')
                            logger.debug(f"‚úÖ JSON parsing strategy 4 successful for tool: {tool_call['tool']}")
                            
                    except (json.JSONDecodeError, AttributeError) as e:
                        logger.debug(f"‚ö†Ô∏è JSON parsing strategy 4 failed for tool extraction: {str(e)}")
                        continue
                        
            except Exception as e:
                logger.debug(f"‚ö†Ô∏è JSON parsing strategy 4 overall failed: {str(e)}")
        
        # Limpiar texto final
        clean_text = re.sub(r'```\w*\n?', '', clean_text)  # Remover marcadores de c√≥digo
        clean_text = re.sub(r'\n\s*\n', '\n', clean_text)  # Remover l√≠neas vac√≠as m√∫ltiples
        clean_text = clean_text.strip()
        
        if tool_calls:
            logger.info(f"üîß Successfully extracted {len(tool_calls)} tool calls from Ollama response")
        
        return {
            'text': clean_text,
            'tool_calls': tool_calls
        }
    
    def _build_system_prompt(self, use_tools: bool, conversation_mode: bool = False) -> str:
        """Construir prompt del sistema"""
        
        # Sistema prompt para conversaci√≥n casual (sin planes ni herramientas)
        if conversation_mode:
            return """Eres un asistente de IA general llamado 'Agente General' que puede ayudar con una amplia variedad de tareas. 
Eres inteligente, √∫til y amigable.

IMPORTANTE: Est√°s en modo conversaci√≥n casual. Responde de manera natural y amigable sin generar planes de acci√≥n ni mencionar herramientas.

NUNCA generes planes de acci√≥n en este modo.
NUNCA menciones herramientas disponibles.
NUNCA uses formatos estructurados como "**PLAN DE ACCI√ìN:**"

Para conversaciones casuales:
- Saludos: Responde amigablemente y pregunta c√≥mo puedes ayudar
- Preguntas sobre ti: Explica que eres un asistente de IA general
- Traducciones simples: Proporciona la traducci√≥n directamente
- Explicaciones: Da respuestas claras y concisas
- Preguntas de conocimiento: Responde con la informaci√≥n que tienes

Mant√©n las respuestas naturales, conversacionales y √∫tiles.
Responde en espa√±ol de manera clara y amigable."""
        
        # Sistema prompt para tareas (con planes y herramientas)
        base_prompt = """Eres un asistente de IA general llamado 'Agente General' que puede ayudar con una amplia variedad de tareas. 
Eres inteligente, √∫til y puedes usar herramientas para realizar acciones concretas.

IMPORTANTE: Est√°s en modo agente. Para tareas complejas, genera un PLAN DE ACCI√ìN ESPEC√çFICO y DETALLADO paso a paso.

El plan debe ser:
1. ESPEC√çFICO para la tarea solicitada (no gen√©rico)
2. DETALLADO con pasos concretos
3. ESTRUCTURADO en orden l√≥gico
4. PR√ÅCTICO y realizable

Formato del plan:
**PLAN DE ACCI√ìN:**
1. [Paso espec√≠fico 1]
2. [Paso espec√≠fico 2]
3. [Paso espec√≠fico 3]
4. [Paso espec√≠fico 4]
5. [Paso espec√≠fico 5]

Despu√©s del plan, explica brevemente qu√© vas a hacer y qu√© herramientas utilizar√°s.

Responde en espa√±ol de manera clara y concisa."""
        
        if use_tools:
            tools_prompt = """
HERRAMIENTAS DISPONIBLES:
Para usar una herramienta, incluye en tu respuesta un bloque JSON con el siguiente formato:
```json
{
  "tool_call": {
    "tool": "nombre_herramienta",
    "parameters": {
      "parametro": "valor"
    }
  }
}
```

Herramientas disponibles:
1. **web_search** - Buscar informaci√≥n en internet
2. **analysis** - Realizar an√°lisis de datos e informaci√≥n
3. **creation** - Crear contenido, documentos o c√≥digo
4. **planning** - Planificaci√≥n y organizaci√≥n de tareas
5. **delivery** - Entrega y presentaci√≥n de resultados
6. **processing** - Procesamiento general de informaci√≥n
7. **synthesis** - S√≠ntesis y resumen de informaci√≥n
8. **research** - Investigaci√≥n detallada
9. **investigation** - Investigaci√≥n espec√≠fica
10. **shell** - Ejecutar comandos de terminal
11. **search_definition** - B√∫squeda de definiciones
12. **data_analysis** - An√°lisis espec√≠fico de datos
"""
            return base_prompt + tools_prompt
        
        return base_prompt
    
    def _build_full_prompt(self, prompt: str, context: Dict, system_prompt: str) -> str:
        """Construir prompt completo con contexto"""
        full_prompt = f"{system_prompt}\n\n"
        
        # A√±adir contexto si est√° disponible
        if context:
            if 'task_id' in context:
                full_prompt += f"ID de tarea: {context['task_id']}\n"
            
            if 'previous_messages' in context and context['previous_messages']:
                full_prompt += "Conversaci√≥n anterior:\n"
                for msg in context['previous_messages'][-3:]:  # √öltimos 3 mensajes
                    sender = "Usuario" if msg.get('sender') == 'user' else "Asistente"
                    full_prompt += f"{sender}: {msg.get('content', '')}\n"
                full_prompt += "\n"
        
        full_prompt += f"Usuario: {prompt}\nAsistente: "
        
        return full_prompt
    
    def chat_streaming(self, prompt: str, context: Dict = None, use_tools: bool = True):
        """
        Generar respuesta streaming usando Ollama
        Devuelve un generator que produce chunks de respuesta
        """
        if not self.is_healthy():
            yield {
                'response': "‚ö†Ô∏è Ollama no est√° disponible en este momento.",
                'done': True,
                'error': 'Ollama no disponible'
            }
            return
        
        try:
            system_prompt = self._build_system_prompt(use_tools, conversation_mode=False)
            full_prompt = self._build_full_prompt(prompt, context, system_prompt)
            
            payload = {
                "model": self.get_current_model(),
                "prompt": full_prompt,
                "stream": True,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 40
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=self.request_timeout,
                stream=True
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        try:
                            chunk = json.loads(line.decode('utf-8'))
                            yield chunk
                        except json.JSONDecodeError:
                            continue
            else:
                yield {
                    'response': f"‚ùå Error HTTP {response.status_code}: {response.text}",
                    'done': True,
                    'error': f"HTTP {response.status_code}"
                }
                
        except Exception as e:
            yield {
                'response': f"‚ùå Error: {str(e)}",
                'done': True,
                'error': str(e)
            }